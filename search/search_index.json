{
    "docs": [
        {
            "location": "/", 
            "text": "LeoFS, A Storage System for a Data Lake and the Web\n\n\nWhy LeoFS?\n\n\nLeoFS is a highly available, distributed, eventually consistent object/blob store. If you are searching a storage system that is able to store huge amount and various kind of raw data in its native format, LeoFS is suitable for that.\n\n\nLeoFS supports the following features:\n\n\n\n\nMulti protocols support\n\n\nRESTful Interface\n\n\nAmazon S3-API\n1\n\n\nA S3 compatible storage system\n\n\nSwitch to LeoFS to decrease your cost from more expensive public-cloud solutions.\n\n\n\n\n\n\nNFS v3\n2\n\n\n\n\n\n\nBuilt-in object cache feature\n\n\nMulti data center/cluster replication\n\n\nUser and bucket management\n\n\nData lake solution\n\n\nCloud solution integration\n\n\n\n\nAbout\n\n\nLeoFS provides High Cost Performance Ratio. It allows you to build LeoFS clusters using commodity hardware on top of the Linux operating system. LeoFS will provide very good performance even on commodity hardware. LeoFS will require a smaller cluster than other storage to achieve the same performance. LeoFS is also very easy to setup and to operate.\n\n\nLeoFS provides High Reliability thanks to its great design on top of the Erlang/OTP capabilities. Erlang/OTP is known for being used in production systems for years with a solid nine nines (99.9999999%) availability, and LeoFS is no exception. A LeoFS system will stay up regardless of software errors or hardware failures happening inside the cluster.\n\n\nLeoFS provides High Scalability. Adding and removing nodes is simple and quick, allowing you to react swiftly when your needs change. A LeoFS cluster can be thought as elastic storage that you can stretch as much and as often as you need.\n\n\n\n\nLeoFS consists of three components - LeoStorage, LeoGateway and LeoManager which depend on Erlang.\n\n\nLeoGateway\n handles http-request and http-response from any clients when using REST-API OR S3-API. Also, it is already built in the object-cache mechanism, memory and disk cache.\n\n\nLeoStorage\n handles GET, PUT and DELETE objects as well as metadata. Also, it has replicator, recoverer and queueing mechanism in order to keep running a storage node and realize eventual consistency.\n\n\nLeoManager\n always monitors LeoGateway and LeoStorage nodes. The main monitoring status are Node status and RING\u2019s checksum in order to realize to keep high availability and keep data consistency.\n\n\nGetting Started\n\n\nTo try LeoFS, see our \nGetting Started guides\n, and to learn more about LeoFS, see our \nArchitecture section\n.\n\n\nGoals\n\n\nWe have been aiming to achieve three things:\n\n\n\n\nHigh reliability and availability\n\n\nOperating ratios is 99.9999999%, nine nines\n\n\nKeeps maintaining high data and system availability while some node downed in a LeoFS' cluster\n\n\n\n\n\n\nHigh scalability\n\n\nBuilds a huge capacity cluster at low cost\n\n\nSupports a multi data center/cluster replication\n\n\n\n\n\n\nHigh cost performance ratio\n\n\nHigh performance with minimum resources\n\n\nProvides easy management and easy operation to users\n\n\n\n\n\n\n\n\nResources\n\n\n\n\nLeoFS Website\n\n\nLeoFS Repository on GitHub \n\n\nLeoFS Download Page\n\n\nLeoFS Package for CentOS and Ubuntu\n\n\nLeoFS Package for FreeBSD \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmazon S3 API\n\n\n\n\n\n\nWikipedia: Network File System", 
            "title": "Introduction"
        }, 
        {
            "location": "/#leofs-a-storage-system-for-a-data-lake-and-the-web", 
            "text": "", 
            "title": "LeoFS, A Storage System for a Data Lake and the Web"
        }, 
        {
            "location": "/#why-leofs", 
            "text": "LeoFS is a highly available, distributed, eventually consistent object/blob store. If you are searching a storage system that is able to store huge amount and various kind of raw data in its native format, LeoFS is suitable for that.  LeoFS supports the following features:   Multi protocols support  RESTful Interface  Amazon S3-API 1  A S3 compatible storage system  Switch to LeoFS to decrease your cost from more expensive public-cloud solutions.    NFS v3 2    Built-in object cache feature  Multi data center/cluster replication  User and bucket management  Data lake solution  Cloud solution integration", 
            "title": "Why LeoFS?"
        }, 
        {
            "location": "/#about", 
            "text": "LeoFS provides High Cost Performance Ratio. It allows you to build LeoFS clusters using commodity hardware on top of the Linux operating system. LeoFS will provide very good performance even on commodity hardware. LeoFS will require a smaller cluster than other storage to achieve the same performance. LeoFS is also very easy to setup and to operate.  LeoFS provides High Reliability thanks to its great design on top of the Erlang/OTP capabilities. Erlang/OTP is known for being used in production systems for years with a solid nine nines (99.9999999%) availability, and LeoFS is no exception. A LeoFS system will stay up regardless of software errors or hardware failures happening inside the cluster.  LeoFS provides High Scalability. Adding and removing nodes is simple and quick, allowing you to react swiftly when your needs change. A LeoFS cluster can be thought as elastic storage that you can stretch as much and as often as you need.   LeoFS consists of three components - LeoStorage, LeoGateway and LeoManager which depend on Erlang.  LeoGateway  handles http-request and http-response from any clients when using REST-API OR S3-API. Also, it is already built in the object-cache mechanism, memory and disk cache.  LeoStorage  handles GET, PUT and DELETE objects as well as metadata. Also, it has replicator, recoverer and queueing mechanism in order to keep running a storage node and realize eventual consistency.  LeoManager  always monitors LeoGateway and LeoStorage nodes. The main monitoring status are Node status and RING\u2019s checksum in order to realize to keep high availability and keep data consistency.", 
            "title": "About"
        }, 
        {
            "location": "/#getting-started", 
            "text": "To try LeoFS, see our  Getting Started guides , and to learn more about LeoFS, see our  Architecture section .", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#goals", 
            "text": "We have been aiming to achieve three things:   High reliability and availability  Operating ratios is 99.9999999%, nine nines  Keeps maintaining high data and system availability while some node downed in a LeoFS' cluster    High scalability  Builds a huge capacity cluster at low cost  Supports a multi data center/cluster replication    High cost performance ratio  High performance with minimum resources  Provides easy management and easy operation to users", 
            "title": "Goals"
        }, 
        {
            "location": "/#resources", 
            "text": "LeoFS Website  LeoFS Repository on GitHub   LeoFS Download Page  LeoFS Package for CentOS and Ubuntu  LeoFS Package for FreeBSD          Amazon S3 API    Wikipedia: Network File System", 
            "title": "Resources"
        }, 
        {
            "location": "/whats_new/", 
            "text": "What's New?\n\n\nThis page lists the new features and enhancements in the v1.3 series of LeoFS.\n\n\nRelease v1.3.3\n\n\nUpdated Contents of LeoFS Package for CentOS and Ubuntu\n\n\n\n\nLeoFS package is generated by \nleofs_package\n which requires the system to run LeoFS as non-privileged user \nleofs\n.\n\n\n\n\nRelated Links\n\n\n\n\nv1.3.3's Change log\n\n\nFor Administrators / Settings / Environment Configuration\n\n\nFor Administrators / System Administration / System Migration\n\n\n\n\nRelease v1.3.2\n\n\nAWS SDK for Python, Boto3\n support\n\n\n\n\nAccording to a user's request, we have added Boto3 to LeoFS' client support.\n\n\n\n\nRelated Links\n\n\n\n\nv1.3.2's Change log\n\n\nBoto3 of LeoFS' client test\n\n\n\n\nRelease v1.3.1\n\n\nUser-defined metadata support\n\n\nIn order to provide optional information as a key-value pair into a metadata when you send a PUT request to store an object, you're able to set \nuse user-defined metadata\n of the object.\n\n\nRelated Links\n\n\n\n\nv1.3.1's Change log\n\n\nS3 / Developer Guide / Working with Amazon S3 Objects/Object Key and Metadata\n\n\n\n\nRelease v1.3.0\n\n\nAWS Signature v4 support\n\n\nIn order to cover latest AWS SDKs which includes \nGo\n, \nJava\n and others, we supported AWS Signature v4 with v1.3.0.\n\n\nRelated Links\n\n\n\n\nv1.3.0's Change log\n\n\nAWS General Reference / Signing AWS API Requests / Signature Version 4 Signing Process", 
            "title": "What's New?"
        }, 
        {
            "location": "/whats_new/#whats-new", 
            "text": "This page lists the new features and enhancements in the v1.3 series of LeoFS.", 
            "title": "What's New?"
        }, 
        {
            "location": "/whats_new/#release-v133", 
            "text": "", 
            "title": "Release v1.3.3"
        }, 
        {
            "location": "/whats_new/#updated-contents-of-leofs-package-for-centos-and-ubuntu", 
            "text": "LeoFS package is generated by  leofs_package  which requires the system to run LeoFS as non-privileged user  leofs .", 
            "title": "Updated Contents of LeoFS Package for CentOS and Ubuntu"
        }, 
        {
            "location": "/whats_new/#related-links", 
            "text": "v1.3.3's Change log  For Administrators / Settings / Environment Configuration  For Administrators / System Administration / System Migration", 
            "title": "Related Links"
        }, 
        {
            "location": "/whats_new/#release-v132", 
            "text": "", 
            "title": "Release v1.3.2"
        }, 
        {
            "location": "/whats_new/#aws-sdk-for-python-boto3-support", 
            "text": "According to a user's request, we have added Boto3 to LeoFS' client support.", 
            "title": "AWS SDK for Python, Boto3 support"
        }, 
        {
            "location": "/whats_new/#related-links_1", 
            "text": "v1.3.2's Change log  Boto3 of LeoFS' client test", 
            "title": "Related Links"
        }, 
        {
            "location": "/whats_new/#release-v131", 
            "text": "", 
            "title": "Release v1.3.1"
        }, 
        {
            "location": "/whats_new/#user-defined-metadata-support", 
            "text": "In order to provide optional information as a key-value pair into a metadata when you send a PUT request to store an object, you're able to set  use user-defined metadata  of the object.", 
            "title": "User-defined metadata support"
        }, 
        {
            "location": "/whats_new/#related-links_2", 
            "text": "v1.3.1's Change log  S3 / Developer Guide / Working with Amazon S3 Objects/Object Key and Metadata", 
            "title": "Related Links"
        }, 
        {
            "location": "/whats_new/#release-v130", 
            "text": "", 
            "title": "Release v1.3.0"
        }, 
        {
            "location": "/whats_new/#aws-signature-v4-support", 
            "text": "In order to cover latest AWS SDKs which includes  Go ,  Java  and others, we supported AWS Signature v4 with v1.3.0.", 
            "title": "AWS Signature v4 support"
        }, 
        {
            "location": "/whats_new/#related-links_3", 
            "text": "v1.3.0's Change log  AWS General Reference / Signing AWS API Requests / Signature Version 4 Signing Process", 
            "title": "Related Links"
        }, 
        {
            "location": "/installation/quick/", 
            "text": "Quick Installation and Setup\n\n\nPurpose\n\n\nThis section is a step by step guide to setting up LeoFS for the first time. By following this tutorial you're able to easily build a stand-alone LeoFS system.\n\n\nNote\n\n\nIn this section, you install LeoStorage, LeoGateway and LeoManager on a single system with no clustering to quickly understand LeoFS.\n\n\nInstallation v1.3.3 or Later\n\n\nUbuntu\n\n\nFor Ubuntu distributions, perform the following steps:\n\n\n\n\nDownloads a LeoFS' package from \nLeoFS' repository\n on GitHub\n\n\nInstalls a LeoFS using \ndpkg\n\n\n\n\nFor Ubuntu 16.04\n\n\n1\n2\n3\n4\n5\n6\n$ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs_{VERSION}-1_erl-18.3_ubuntu-16.04_amd64.deb\n$ sudo dpkg -i leofs_{VERSION}-1_erl-18.3_ubuntu-16.04_amd64.deb\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root root 4096 Sep  8 20:52 {VERSION}\nlrwxrwxrwx 1 root root    5 Sep  8 20:33 current -\n {VERSION}\n\n\n\n\n\n\nFor Ubuntu 14.04\n\n\n1\n2\n3\n4\n5\n6\n$ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs_{VERSION}-1_erl-18.3_ubuntu-14.04_amd64.deb\n$ sudo dpkg -i leofs_{VERSION}-1_erl-18.3_ubuntu-14.04_amd64.deb\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root root 4096 Sep  8 20:52 {VERSION}\nlrwxrwxrwx 1 root root    5 Sep  8 20:33 current -\n {VERSION}\n\n\n\n\n\n\nCentOS\n\n\nFor CentOS distributions, perform the following steps:\n\n\n\n\nDownloads a LeoFS' package from \nLeoFS' repository\n on GitHub\n\n\nInstalls a LeoFS using \nrpm\n\n\n\n\nFor CentOS 6.x\n\n\n1\n2\n3\n4\n5\n6\n$ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs-{VERSION}-1.erl-18.3.el6.x86_64.rpm\n$ sudo rpm -ivh leofs-{VERSION}-1.erl-18.3.el6.x86_64.rpm\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root root 4096 Sep  8 20:52 {VERSION}\nlrwxrwxrwx 1 root root    5 Sep  8 20:33 current -\n {VERSION}\n\n\n\n\n\n\nFor CentOS 7.x\n\n\n1\n2\n3\n4\n5\n6\n$ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs-{VERSION}-1.erl-18.3.el7.x86_64.rpm\n$ sudo rpm -ivh leofs-{VERSION}-1.erl-18.3.el7.x86_64.rpm\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root root 4096 Sep  8 20:52 {VERSION}\nlrwxrwxrwx 1 root root    5 Sep  8 20:33 current -\n {VERSION}\n\n\n\n\n\n\nConfiguration\n\n\nTo be able to access the LeoFS storage system, you need to edit \n/etc/hosts\n which adheres to \nrules for bucket naming of S3-API\n.\n\n\nModifies \u201c/etc/hosts\u201d\n\n\n\n\nAdds a domain for the LeoFS bucket in /etc/hosts\n\n\nBucket names must follow \nthese rules\n\n\n\n\n1\n2\n3\n## Replace {BUCKET_NAME} with the name of the bucket ##\n$ sudo vi /etc/hosts\n127.0.0.1 localhost {BUCKET_NAME}.localhost\n\n\n\n\n\n\nLaunches LeoManager, LeoStorage, and LeoGateway node(s)\n\n\nYou launch the LeoFS storage system by the following steps:\n\n\n\n\nThere is only single replica by \nthe default configuration of \nconsistency.num_of_replicas\n\n\nStarts both \nLeoManager master\n and \nLeoManager slave\n\n\nStarts a \nLeoStorage\n\n\nStarts a \nLeoGateway\n\n\n\n\n1\n2\n3\n4\n5\n$ cd /usr/local/leofs/{VERSION}\n$ leo_manager_0/bin/leo_manager start\n$ leo_manager_1/bin/leo_manager start\n$ leo_storage/bin/leo_storage start\n$ leo_gateway/bin/leo_gateway start\n\n\n\n\n\n\nFor systemd-based distributions running v1.4.0 or later, you can use systemd units to launch LeoFS instead:\n\n\n1\n# systemctl start leofs-manager-master leofs-manager-slave leofs-gateway leofs-storage\n\n\n\n\n\n\n\n\nNote: Do not mix systemctl and launch scripts\n\n\nPlease make sure not to mix launching/stopping through systemctl and directly through launch scripts. More information is available at \nFor Administrators / System Operations / Systemd Services\n.\n\n\n\n\nThen confirms whether the LeoManager nodes and the LeoStorage are running or not with the \nleofs-adm status\n command.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n$ leofs-adm status\n [System Confiuration]\n-----------------------------------+----------\n Item                              | Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version | 1.3.3\n                        cluster Id | leofs_1\n                             DC Id | dc_1\n                    Total replicas | 1\n          number of successes of R | 1\n          number of successes of W | 1\n          number of successes of D | 1\n number of rack-awareness replicas | 0\n                         ring size | 2^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n [mdcr] max number of joinable DCs | 2\n [mdcr] total replicas per a DC    | 1\n [mdcr] number of successes of R   | 1\n [mdcr] number of successes of W   | 1\n [mdcr] number of successes of D   | 1\n-----------------------------------+----------\n Manager RING hash\n-----------------------------------+----------\n                 current ring-hash | 433fe365\n                previous ring-hash | 433fe365\n-----------------------------------+----------\n\n [State of Node(s)]\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n type  |           node           |    state     |  current ring  |   prev ring    |          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    | storage_0@127.0.0.1      | attached     |                |                | 2017-05-01 00:43:06 +0000\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\nStarts the LeoFS storage system\n\n\nIf there is no issue, you're able to execute the \nleofs-adm start\n commmand to launch it, then confirm the current status with the \nleof-adm status\n command.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n$ ./leofs-adm start\nGenerating RING...\nGenerated RING\nOK 100% - storage_0@127.0.0.1\nOK\n\n$ leofs-adm status\n [System Confiuration]\n-----------------------------------+----------\n Item                              | Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version | 1.3.3\n                        cluster Id | leofs_1\n                             DC Id | dc_1\n                    Total replicas | 1\n          number of successes of R | 1\n          number of successes of W | 1\n          number of successes of D | 1\n number of rack-awareness replicas | 0\n                         ring size | 2^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n [mdcr] max number of joinable DCs | 2\n [mdcr] total replicas per a DC    | 1\n [mdcr] number of successes of R   | 1\n [mdcr] number of successes of W   | 1\n [mdcr] number of successes of D   | 1\n-----------------------------------+----------\n Manager RING hash\n-----------------------------------+----------\n                 current ring-hash | 433fe365\n                previous ring-hash | 433fe365\n-----------------------------------+----------\n\n [State of Node(s)]\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n type  |           node           |    state     |  current ring  |   prev ring    |          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    | storage_0@127.0.0.1      | running      | 433fe365       | 433fe365       | 2017-05-01 00:43:06 +0000\n  G    | gateway_0@127.0.0.1      | running      | 433fe365       | 433fe365       | 2017-05-01 00:43:08 +0000\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\nRetrieves both an access-key and a secret accesskey\n\n\nTo make a bucket for the test, you need to retrieve both an \naccess-key\n and a \nsecret access-key\n with the \nleofs-adm create-user\n command\n\n\n1\n2\n3\n$ leofs-adm create-user \nYOUR-ID\n\naccess-key-id: \nACCESS-KEY-ID\n\nsecret-access-key: \nSECRET-ACCESS-KEY-ID\n\n\n\n\n\n\n\nAfter that, you use S3-client(s) with those keys when you access the LeoFS storage system.\n\n\nUses\ns3cmd\n to access it\n\n\nIf you'd like to use \ns3cmd\n to access the LeoFS storage system, perform the following steps:\n\n\n\n\nInstalls \ns3cmd\n on your machine\n\n\nConfigures \ns3cmd\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n$ s3cmd --version\ns3cmd version \n1\n.6.1\n\n$ s3cmd --configure\n...\n\n## access_key = \naccess-key-id\n\n\n## secret_key = \nsecret-access-key\n\n\n## proxy_host = localhost\n\n\n## proxy_port = 8080\n\n\n\n\n\n\n\n\n\nCreates a bucket\n\n\n\n\n1\n2\n$ s3cmd mb \nBUCKET\n\nBucket \ns3://\nBUCKET\n/\n created\n\n\n\n\n\n\n\n\nPuts an object into the bucket\n\n\n\n\n1\n2\n3\n$ s3cmd put /path/to/\nOBJECT\n s3://\nBUCKET\n/\nupload: \nOBJET\n -\n \ns3://\nBUCKET\n/\nOBJECT\n  \n[\n1\n of \n1\n]\n\n \n1096\n of \n1096\n   \n100\n% in    0s   \n170\n.92 kB/s  \ndone\n\n\n\n\n\n\n\n\n\nGets an object\n\n\n\n\n1\n2\n3\n$ s3cmd get s3://\nBUCKET\n/\nOBJECT\n\ndownload: \ns3://\nBUCKET\n/\nOBJET\n -\n \n./\nOBJECT\n  \n[\n1\n of \n1\n]\n\n \n1096\n of \n1096\n   \n100\n% in    0s   \n307\n.38 kB/s  \ndone\n\n\n\n\n\n\n\n\n\nLists objects\n\n\n\n\n1\n2\n$ s3cmd ls s3://\nBUCKET\n/\n\n2017\n-01-30 \n02\n:24      \n1096\n   s3://\nBUCKET\n/\nOBJECT\n\n\n\n\n\n\n\n\n\nRemoves an object\n\n\n\n\n1\n2\n$ s3cmd del s3://\nBUCKET\n/\nOBJECT\n\ndelete: \ns3://\nBUCKET\n/\nOBJECT\n\n\n\n\n\n\n\nWrap up\n\n\nYou now know how to setup a stand-alone LeoFS storage system, and make sure to have a look at \nBuilding a LeoFS' cluster with Ansible\n to learn how to setup a LeoFS cluster.\n\n\nRelated Links\n\n\n\n\nGetting Started / Building a LeoFS' cluster with Ansible\n\n\nFor Administrators / Settings / Environment Configuration\n\n\nFor Administrators / Settings / LeoManager Settings\n\n\nFor Administrators / Settings / LeoStorage Settings\n\n\nFor Administrators / Settings / LeoGateway Settings\n\n\nFor Administrators / System Operations / Systemd Services", 
            "title": "Quick Installation and Setup"
        }, 
        {
            "location": "/installation/quick/#quick-installation-and-setup", 
            "text": "", 
            "title": "Quick Installation and Setup"
        }, 
        {
            "location": "/installation/quick/#purpose", 
            "text": "This section is a step by step guide to setting up LeoFS for the first time. By following this tutorial you're able to easily build a stand-alone LeoFS system.", 
            "title": "Purpose"
        }, 
        {
            "location": "/installation/quick/#note", 
            "text": "In this section, you install LeoStorage, LeoGateway and LeoManager on a single system with no clustering to quickly understand LeoFS.", 
            "title": "Note"
        }, 
        {
            "location": "/installation/quick/#installation-v133-or-later", 
            "text": "", 
            "title": "Installation v1.3.3 or Later"
        }, 
        {
            "location": "/installation/quick/#ubuntu", 
            "text": "For Ubuntu distributions, perform the following steps:   Downloads a LeoFS' package from  LeoFS' repository  on GitHub  Installs a LeoFS using  dpkg", 
            "title": "Ubuntu"
        }, 
        {
            "location": "/installation/quick/#for-ubuntu-1604", 
            "text": "1\n2\n3\n4\n5\n6 $ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs_{VERSION}-1_erl-18.3_ubuntu-16.04_amd64.deb\n$ sudo dpkg -i leofs_{VERSION}-1_erl-18.3_ubuntu-16.04_amd64.deb\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root root 4096 Sep  8 20:52 {VERSION}\nlrwxrwxrwx 1 root root    5 Sep  8 20:33 current -  {VERSION}", 
            "title": "For Ubuntu 16.04"
        }, 
        {
            "location": "/installation/quick/#for-ubuntu-1404", 
            "text": "1\n2\n3\n4\n5\n6 $ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs_{VERSION}-1_erl-18.3_ubuntu-14.04_amd64.deb\n$ sudo dpkg -i leofs_{VERSION}-1_erl-18.3_ubuntu-14.04_amd64.deb\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root root 4096 Sep  8 20:52 {VERSION}\nlrwxrwxrwx 1 root root    5 Sep  8 20:33 current -  {VERSION}", 
            "title": "For Ubuntu 14.04"
        }, 
        {
            "location": "/installation/quick/#centos", 
            "text": "For CentOS distributions, perform the following steps:   Downloads a LeoFS' package from  LeoFS' repository  on GitHub  Installs a LeoFS using  rpm", 
            "title": "CentOS"
        }, 
        {
            "location": "/installation/quick/#for-centos-6x", 
            "text": "1\n2\n3\n4\n5\n6 $ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs-{VERSION}-1.erl-18.3.el6.x86_64.rpm\n$ sudo rpm -ivh leofs-{VERSION}-1.erl-18.3.el6.x86_64.rpm\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root root 4096 Sep  8 20:52 {VERSION}\nlrwxrwxrwx 1 root root    5 Sep  8 20:33 current -  {VERSION}", 
            "title": "For CentOS 6.x"
        }, 
        {
            "location": "/installation/quick/#for-centos-7x", 
            "text": "1\n2\n3\n4\n5\n6 $ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs-{VERSION}-1.erl-18.3.el7.x86_64.rpm\n$ sudo rpm -ivh leofs-{VERSION}-1.erl-18.3.el7.x86_64.rpm\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root root 4096 Sep  8 20:52 {VERSION}\nlrwxrwxrwx 1 root root    5 Sep  8 20:33 current -  {VERSION}", 
            "title": "For CentOS 7.x"
        }, 
        {
            "location": "/installation/quick/#configuration", 
            "text": "To be able to access the LeoFS storage system, you need to edit  /etc/hosts  which adheres to  rules for bucket naming of S3-API .", 
            "title": "Configuration"
        }, 
        {
            "location": "/installation/quick/#modifies-etchosts", 
            "text": "Adds a domain for the LeoFS bucket in /etc/hosts  Bucket names must follow  these rules   1\n2\n3 ## Replace {BUCKET_NAME} with the name of the bucket ##\n$ sudo vi /etc/hosts\n127.0.0.1 localhost {BUCKET_NAME}.localhost", 
            "title": "Modifies \u201c/etc/hosts\u201d"
        }, 
        {
            "location": "/installation/quick/#launches-leomanager-leostorage-and-leogateway-nodes", 
            "text": "You launch the LeoFS storage system by the following steps:   There is only single replica by  the default configuration of  consistency.num_of_replicas  Starts both  LeoManager master  and  LeoManager slave  Starts a  LeoStorage  Starts a  LeoGateway   1\n2\n3\n4\n5 $ cd /usr/local/leofs/{VERSION}\n$ leo_manager_0/bin/leo_manager start\n$ leo_manager_1/bin/leo_manager start\n$ leo_storage/bin/leo_storage start\n$ leo_gateway/bin/leo_gateway start   For systemd-based distributions running v1.4.0 or later, you can use systemd units to launch LeoFS instead:  1 # systemctl start leofs-manager-master leofs-manager-slave leofs-gateway leofs-storage    Note: Do not mix systemctl and launch scripts  Please make sure not to mix launching/stopping through systemctl and directly through launch scripts. More information is available at  For Administrators / System Operations / Systemd Services .   Then confirms whether the LeoManager nodes and the LeoStorage are running or not with the  leofs-adm status  command.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37 $ leofs-adm status\n [System Confiuration]\n-----------------------------------+----------\n Item                              | Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version | 1.3.3\n                        cluster Id | leofs_1\n                             DC Id | dc_1\n                    Total replicas | 1\n          number of successes of R | 1\n          number of successes of W | 1\n          number of successes of D | 1\n number of rack-awareness replicas | 0\n                         ring size | 2^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n [mdcr] max number of joinable DCs | 2\n [mdcr] total replicas per a DC    | 1\n [mdcr] number of successes of R   | 1\n [mdcr] number of successes of W   | 1\n [mdcr] number of successes of D   | 1\n-----------------------------------+----------\n Manager RING hash\n-----------------------------------+----------\n                 current ring-hash | 433fe365\n                previous ring-hash | 433fe365\n-----------------------------------+----------\n\n [State of Node(s)]\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n type  |           node           |    state     |  current ring  |   prev ring    |          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    | storage_0@127.0.0.1      | attached     |                |                | 2017-05-01 00:43:06 +0000\n-------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Launches LeoManager, LeoStorage, and LeoGateway node(s)"
        }, 
        {
            "location": "/installation/quick/#starts-the-leofs-storage-system", 
            "text": "If there is no issue, you're able to execute the  leofs-adm start  commmand to launch it, then confirm the current status with the  leof-adm status  command.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44 $ ./leofs-adm start\nGenerating RING...\nGenerated RING\nOK 100% - storage_0@127.0.0.1\nOK\n\n$ leofs-adm status\n [System Confiuration]\n-----------------------------------+----------\n Item                              | Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version | 1.3.3\n                        cluster Id | leofs_1\n                             DC Id | dc_1\n                    Total replicas | 1\n          number of successes of R | 1\n          number of successes of W | 1\n          number of successes of D | 1\n number of rack-awareness replicas | 0\n                         ring size | 2^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n [mdcr] max number of joinable DCs | 2\n [mdcr] total replicas per a DC    | 1\n [mdcr] number of successes of R   | 1\n [mdcr] number of successes of W   | 1\n [mdcr] number of successes of D   | 1\n-----------------------------------+----------\n Manager RING hash\n-----------------------------------+----------\n                 current ring-hash | 433fe365\n                previous ring-hash | 433fe365\n-----------------------------------+----------\n\n [State of Node(s)]\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n type  |           node           |    state     |  current ring  |   prev ring    |          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    | storage_0@127.0.0.1      | running      | 433fe365       | 433fe365       | 2017-05-01 00:43:06 +0000\n  G    | gateway_0@127.0.0.1      | running      | 433fe365       | 433fe365       | 2017-05-01 00:43:08 +0000\n-------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Starts the LeoFS storage system"
        }, 
        {
            "location": "/installation/quick/#retrieves-both-an-access-key-and-a-secret-accesskey", 
            "text": "To make a bucket for the test, you need to retrieve both an  access-key  and a  secret access-key  with the  leofs-adm create-user  command  1\n2\n3 $ leofs-adm create-user  YOUR-ID \naccess-key-id:  ACCESS-KEY-ID \nsecret-access-key:  SECRET-ACCESS-KEY-ID    After that, you use S3-client(s) with those keys when you access the LeoFS storage system.", 
            "title": "Retrieves both an access-key and a secret accesskey"
        }, 
        {
            "location": "/installation/quick/#usess3cmd-to-access-it", 
            "text": "If you'd like to use  s3cmd  to access the LeoFS storage system, perform the following steps:   Installs  s3cmd  on your machine  Configures  s3cmd   1\n2\n3\n4\n5\n6\n7\n8\n9 $ s3cmd --version\ns3cmd version  1 .6.1\n\n$ s3cmd --configure\n... ## access_key =  access-key-id  ## secret_key =  secret-access-key  ## proxy_host = localhost  ## proxy_port = 8080     Creates a bucket   1\n2 $ s3cmd mb  BUCKET \nBucket  s3:// BUCKET /  created    Puts an object into the bucket   1\n2\n3 $ s3cmd put /path/to/ OBJECT  s3:// BUCKET /\nupload:  OBJET  -   s3:// BUCKET / OBJECT    [ 1  of  1 ] \n  1096  of  1096     100 % in    0s    170 .92 kB/s   done     Gets an object   1\n2\n3 $ s3cmd get s3:// BUCKET / OBJECT \ndownload:  s3:// BUCKET / OBJET  -   ./ OBJECT    [ 1  of  1 ] \n  1096  of  1096     100 % in    0s    307 .38 kB/s   done     Lists objects   1\n2 $ s3cmd ls s3:// BUCKET / 2017 -01-30  02 :24       1096    s3:// BUCKET / OBJECT     Removes an object   1\n2 $ s3cmd del s3:// BUCKET / OBJECT \ndelete:  s3:// BUCKET / OBJECT", 
            "title": "Usess3cmd to access it"
        }, 
        {
            "location": "/installation/quick/#wrap-up", 
            "text": "You now know how to setup a stand-alone LeoFS storage system, and make sure to have a look at  Building a LeoFS' cluster with Ansible  to learn how to setup a LeoFS cluster.", 
            "title": "Wrap up"
        }, 
        {
            "location": "/installation/quick/#related-links", 
            "text": "Getting Started / Building a LeoFS' cluster with Ansible  For Administrators / Settings / Environment Configuration  For Administrators / Settings / LeoManager Settings  For Administrators / Settings / LeoStorage Settings  For Administrators / Settings / LeoGateway Settings  For Administrators / System Operations / Systemd Services", 
            "title": "Related Links"
        }, 
        {
            "location": "/installation/cluster/", 
            "text": "Building a LeoFS' cluster with Ansible\n\n\nPurpose\n\n\nThis tutorial teaches you how to easily build a LeoFS cluster. All steps will not be explained in detail, it is assumed you already know \nhow to setup a stand-alone LeoFS system\n. This guide exists to help you get a cluster up and running quickly. We recommend that you read the LeoFS Configuration and Administration Guide to learn how to administer your LeoFS cluster. We hope that by reading this tutorial you will be able to get a cluster started as quickly as possible.\n\n\nInstalls and Launches LeoFS with \nleofs_ansible\n\n\nYou can easily install LeoFS into your servers with using \nleofs_ansible\n, perform following steps:\n\n\nUses \nleofs_ansible\n\n\n\n\nInstalls and sets up \nAnsible\n\n\nleofs_ansible's documentation\n is already published, you can follow it to install and launch a LeoFS storage system.\n\n\n\n\nAn example of \nhosts\n\n\nManager\n\n\n\n\nA number of nodes: 2\n\n\nIP: 10.0.0.101, 10.0.0.102\n\n\nName: M0@10.0.0.101, M1@10.0.0.102\n\n\n\n\nGateway\n\n\n\n\nA number of nodes: 1\n\n\nIP: 10.0.0.103\n\n\nName: G0@10.0.0.103\n\n\n\n\nStorage\n\n\n\n\nA number of nodes: 3\n\n\nIP: 10.0.0.104 .. 10.0.0.106\n\n\nName: S0@10.0.0.104 .. S2@10.0.0.106\n\n\n\n\nIn this case, we configure basic properties and nodes of LeoManager, LeoStorage an LeoGateway. You need to configure those properties to suit your environment.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n##\n## Please check roles/common/vars/leofs_releases for available versions\n##\n[all:vars]\nleofs_version=1.3.2\nbuild_temp_path=\n/tmp/leofs_builder\n\nbuild_install_path=\n/tmp/\n\nbuild_branch=\nmaster\n\nsource=\npackage\n\n\n[builder]\n10.0.0.100\n\n# nodename of leo_manager_0 and leo_manager_1 are set at group_vars/all\n[leo_manager_0]\n10.0.0.101\n\n# nodename of leo_manager_0 and leo_manager_1 are set at group_vars/all\n[leo_manager_1]\n10.0.0.102\n\n[leo_storage]\n10.0.0.104 leofs_module_nodename=S0@10.0.0.104\n10.0.0.105 leofs_module_nodename=S1@10.0.0.105\n10.0.0.106 leofs_module_nodename=S2@10.0.0.106\n\n[leo_gateway]\n10.0.0.103 leofs_module_nodename=G0@10.0.0.103\n\n[leofs_nodes:children]\nleo_manager_0\nleo_manager_1\nleo_gateway\nleo_storage\n\n\n\n\n\n\nConfirmation\n\n\nUses the \nleofs-adm status\n command to confirm current its status.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n$\n \nleofs\n-\nadm\n \nstatus\n\n \n[\nSystem\n \nConfiuration\n]\n\n\n-----------------------------------+----------\n\n \nItem\n                              \n|\n \nValue\n\n\n-----------------------------------+----------\n\n \nBasic\n/\nConsistency\n \nlevel\n\n\n-----------------------------------+----------\n\n                    \nsystem\n \nversion\n \n|\n \n1.3.2\n\n                        \ncluster\n \nId\n \n|\n \nleofs_1\n\n                             \nDC\n \nId\n \n|\n \ndc_1\n\n                    \nTotal\n \nreplicas\n \n|\n \n2\n\n          \nnumber\n \nof\n \nsuccesses\n \nof\n \nR\n \n|\n \n1\n\n          \nnumber\n \nof\n \nsuccesses\n \nof\n \nW\n \n|\n \n1\n\n          \nnumber\n \nof\n \nsuccesses\n \nof\n \nD\n \n|\n \n1\n\n \nnumber\n \nof\n \nrack\n-\nawareness\n \nreplicas\n \n|\n \n0\n\n                         \nring\n \nsize\n \n|\n \n2\n^\n128\n\n\n-----------------------------------+----------\n\n \nMulti\n \nDC\n \nreplication\n \nsettings\n\n\n-----------------------------------+----------\n\n        \nmax\n \nnumber\n \nof\n \njoinable\n \nDCs\n \n|\n \n2\n\n           \nnumber\n \nof\n \nreplicas\n \na\n \nDC\n \n|\n \n1\n\n\n-----------------------------------+----------\n\n \nManager\n \nRING\n \nhash\n\n\n-----------------------------------+----------\n\n                 \ncurrent\n \nring\n-\nhash\n \n|\n \n3923\nd007\n\n                \nprevious\n \nring\n-\nhash\n \n|\n \n3923\nd007\n\n\n-----------------------------------+----------\n\n\n \n[\nState\n \nof\n \nNode\n(\ns\n)]\n\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n \ntype\n  \n|\n           \nnode\n           \n|\n    \nstate\n     \n|\n  \ncurrent\n \nring\n  \n|\n   \nprev\n \nring\n    \n|\n          \nupdated\n \nat\n\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n  \nS\n    \n|\n \nS0\n@10.0.0.104\n            \n|\n \nrunning\n      \n|\n \n3923\nd007\n       \n|\n \n3923\nd007\n       \n|\n \n2017\n-\n01\n-\n30\n \n12\n:\n32\n:\n48\n \n+\n0900\n\n  \nS\n    \n|\n \nS1\n@10.0.0.105\n            \n|\n \nrunning\n      \n|\n \n3923\nd007\n       \n|\n \n3923\nd007\n       \n|\n \n2017\n-\n01\n-\n30\n \n12\n:\n32\n:\n48\n \n+\n0900\n\n  \nS\n    \n|\n \nS2\n@10.0.0.106\n            \n|\n \nrunning\n      \n|\n \n3923\nd007\n       \n|\n \n3923\nd007\n       \n|\n \n2017\n-\n01\n-\n30\n \n12\n:\n32\n:\n48\n \n+\n0900\n\n  \nG\n    \n|\n \nG0\n@10.0.0.103\n            \n|\n \nrunning\n      \n|\n \n3923\nd007\n       \n|\n \n3923\nd007\n       \n|\n \n2017\n-\n01\n-\n30\n \n12\n:\n32\n:\n52\n \n+\n0900\n\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\n\nRetrieves both an access-key and a secret accesskey\n\n\nTo make a bucket for the test, you need to retrieve both an \naccess-key\n and a \nsecret access-key\n with the \nleofs-adm create-user\n command\n\n\n1\n2\n3\n$ leofs-adm create-user \nYOUR-ID\n\naccess-key-id: \nACCESS-KEY-ID\n\nsecret-access-key: \nSECRET-ACCESS-KEY-ID\n\n\n\n\n\n\n\nAfter that, you use S3-client(s) with those keys when you access the LeoFS storage system.\n\n\nConfigure an endpoint\n\n\nTo make buckets and objects available from a host running S3-client(s), you need to add an endpoint also tweak settings for the name resolution.\n\n\nAssign a name to LeoGateway through DNS or /etc/hosts\n\n\nIn this example, we take the latter way.\n\n\n1\n2\n3\n$ sudo vi /etc/hosts\n\n# Add the below line on every host you are supposed to run S3-client(s)\n\n\n10\n.0.0.103 leo_gateway\n\n\n\n\n\n\nAdd an endpoint through leofs-adm\n\n\n1\n$ leofs-adm add-endpoint leo_gateway\n\n\n\n\n\n\nMake a specific bucket and objects under the bucket available\n\n\nYou need to resolve the domain name \ntest.leo_gateway\n into the IP address of LeoGateway if you are going to have a bucket named \ntest\n.\n\n\n1\n2\n3\n$ sudo vi /etc/hosts\n\n# Add the below line on every host you are supposed to run S3-client(s)\n\n\n10\n.0.0.103 test.leo_gateway\n\n\n\n\n\n\nUses\ns3cmd\n to access it\n\n\nIf you'd like to use \ns3cmd\n to access the LeoFS storage system, perform the following steps:\n\n\n\n\nInstalls \ns3cmd\n on your machine\n\n\nConfigures \ns3cmd\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n$ s3cmd --version\ns3cmd version \n1\n.6.1\n\n$ s3cmd --configure\n...\n\n## access_key = \naccess-key-id\n\n\n## secret_key = \nsecret-access-key\n\n\n## proxy_host = leo_gateway\n\n\n## proxy_port = 8080\n\n\n\n\n\n\n\n\n\nCreates a bucket\n\n\n\n\n1\n2\n$ s3cmd mb \nBUCKET\n\nBucket \ns3://\nBUCKET\n/\n created\n\n\n\n\n\n\n\n\nPuts an object into the bucket\n\n\n\n\n1\n2\n3\n$ s3cmd put /path/to/\nOBJECT\n s3://\nBUCKET\n/\nupload: \nOBJET\n -\n \ns3://\nBUCKET\n/\nOBJECT\n  \n[\n1\n of \n1\n]\n\n \n1096\n of \n1096\n   \n100\n% in    0s   \n170\n.92 kB/s  \ndone\n\n\n\n\n\n\n\n\n\nGets an object\n\n\n\n\n1\n2\n3\n$ s3cmd get s3://\nBUCKET\n/\nOBJECT\n\ndownload: \ns3://\nBUCKET\n/\nOBJET\n -\n \n./\nOBJECT\n  \n[\n1\n of \n1\n]\n\n \n1096\n of \n1096\n   \n100\n% in    0s   \n307\n.38 kB/s  \ndone\n\n\n\n\n\n\n\n\n\nLists objects\n\n\n\n\n1\n2\n$ s3cmd ls s3://\nBUCKET\n/\n\n2017\n-01-30 \n02\n:24      \n1096\n   s3://\nBUCKET\n/\nOBJECT\n\n\n\n\n\n\n\n\n\nRemoves an object\n\n\n\n\n1\n2\n$ s3cmd del s3://\nBUCKET\n/\nOBJECT\n\ndelete: \ns3://\nBUCKET\n/\nOBJECT\n\n\n\n\n\n\n\nWrap up\n\n\nYou now have a working LeoFS cluster. Make sure to have a look at \nAdministrators / Settings\n to learn more about administration and settings of a LeoFS storage system.\n\n\nRelated Links\n\n\n\n\nGetting Started / Quick Installation and Setup\n\n\nFor Administrators / Setup / Planning for Production\n\n\nFor Administrators / Settings / Environment Configuration\n\n\nFor Administrators / Settings / Cluster Settings\n\n\nFor Administrators / Settings / LeoManager Settings\n\n\nFor Administrators / Settings / LeoStorage Settings\n\n\nFor Administrators / Settings / LeoGateway Settings\n\n\nFor Administrators / System Operations / Cluster Operations\n\n\nFAQ / LeoFS Clients", 
            "title": "Building a LeoFS' cluster with Ansible"
        }, 
        {
            "location": "/installation/cluster/#building-a-leofs-cluster-with-ansible", 
            "text": "", 
            "title": "Building a LeoFS' cluster with Ansible"
        }, 
        {
            "location": "/installation/cluster/#purpose", 
            "text": "This tutorial teaches you how to easily build a LeoFS cluster. All steps will not be explained in detail, it is assumed you already know  how to setup a stand-alone LeoFS system . This guide exists to help you get a cluster up and running quickly. We recommend that you read the LeoFS Configuration and Administration Guide to learn how to administer your LeoFS cluster. We hope that by reading this tutorial you will be able to get a cluster started as quickly as possible.", 
            "title": "Purpose"
        }, 
        {
            "location": "/installation/cluster/#installs-and-launches-leofs-with-leofs_ansible", 
            "text": "You can easily install LeoFS into your servers with using  leofs_ansible , perform following steps:", 
            "title": "Installs and Launches LeoFS with leofs_ansible"
        }, 
        {
            "location": "/installation/cluster/#uses-leofs_ansible", 
            "text": "Installs and sets up  Ansible  leofs_ansible's documentation  is already published, you can follow it to install and launch a LeoFS storage system.", 
            "title": "Uses leofs_ansible"
        }, 
        {
            "location": "/installation/cluster/#an-example-of-hosts", 
            "text": "", 
            "title": "An example of hosts"
        }, 
        {
            "location": "/installation/cluster/#manager", 
            "text": "A number of nodes: 2  IP: 10.0.0.101, 10.0.0.102  Name: M0@10.0.0.101, M1@10.0.0.102", 
            "title": "Manager"
        }, 
        {
            "location": "/installation/cluster/#gateway", 
            "text": "A number of nodes: 1  IP: 10.0.0.103  Name: G0@10.0.0.103", 
            "title": "Gateway"
        }, 
        {
            "location": "/installation/cluster/#storage", 
            "text": "A number of nodes: 3  IP: 10.0.0.104 .. 10.0.0.106  Name: S0@10.0.0.104 .. S2@10.0.0.106   In this case, we configure basic properties and nodes of LeoManager, LeoStorage an LeoGateway. You need to configure those properties to suit your environment.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34 ##\n## Please check roles/common/vars/leofs_releases for available versions\n##\n[all:vars]\nleofs_version=1.3.2\nbuild_temp_path= /tmp/leofs_builder \nbuild_install_path= /tmp/ \nbuild_branch= master \nsource= package \n\n[builder]\n10.0.0.100\n\n# nodename of leo_manager_0 and leo_manager_1 are set at group_vars/all\n[leo_manager_0]\n10.0.0.101\n\n# nodename of leo_manager_0 and leo_manager_1 are set at group_vars/all\n[leo_manager_1]\n10.0.0.102\n\n[leo_storage]\n10.0.0.104 leofs_module_nodename=S0@10.0.0.104\n10.0.0.105 leofs_module_nodename=S1@10.0.0.105\n10.0.0.106 leofs_module_nodename=S2@10.0.0.106\n\n[leo_gateway]\n10.0.0.103 leofs_module_nodename=G0@10.0.0.103\n\n[leofs_nodes:children]\nleo_manager_0\nleo_manager_1\nleo_gateway\nleo_storage", 
            "title": "Storage"
        }, 
        {
            "location": "/installation/cluster/#confirmation", 
            "text": "Uses the  leofs-adm status  command to confirm current its status.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37 $   leofs - adm   status \n  [ System   Confiuration ]  -----------------------------------+---------- \n  Item                                |   Value  -----------------------------------+---------- \n  Basic / Consistency   level  -----------------------------------+---------- \n                     system   version   |   1.3.2 \n                         cluster   Id   |   leofs_1 \n                              DC   Id   |   dc_1 \n                     Total   replicas   |   2 \n           number   of   successes   of   R   |   1 \n           number   of   successes   of   W   |   1 \n           number   of   successes   of   D   |   1 \n  number   of   rack - awareness   replicas   |   0 \n                          ring   size   |   2 ^ 128  -----------------------------------+---------- \n  Multi   DC   replication   settings  -----------------------------------+---------- \n         max   number   of   joinable   DCs   |   2 \n            number   of   replicas   a   DC   |   1  -----------------------------------+---------- \n  Manager   RING   hash  -----------------------------------+---------- \n                  current   ring - hash   |   3923 d007 \n                 previous   ring - hash   |   3923 d007  -----------------------------------+---------- \n\n  [ State   of   Node ( s )]  -------+--------------------------+--------------+----------------+----------------+---------------------------- \n  type    |             node             |      state       |    current   ring    |     prev   ring      |            updated   at  -------+--------------------------+--------------+----------------+----------------+---------------------------- \n   S      |   S0 @10.0.0.104              |   running        |   3923 d007         |   3923 d007         |   2017 - 01 - 30   12 : 32 : 48   + 0900 \n   S      |   S1 @10.0.0.105              |   running        |   3923 d007         |   3923 d007         |   2017 - 01 - 30   12 : 32 : 48   + 0900 \n   S      |   S2 @10.0.0.106              |   running        |   3923 d007         |   3923 d007         |   2017 - 01 - 30   12 : 32 : 48   + 0900 \n   G      |   G0 @10.0.0.103              |   running        |   3923 d007         |   3923 d007         |   2017 - 01 - 30   12 : 32 : 52   + 0900  -------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Confirmation"
        }, 
        {
            "location": "/installation/cluster/#retrieves-both-an-access-key-and-a-secret-accesskey", 
            "text": "To make a bucket for the test, you need to retrieve both an  access-key  and a  secret access-key  with the  leofs-adm create-user  command  1\n2\n3 $ leofs-adm create-user  YOUR-ID \naccess-key-id:  ACCESS-KEY-ID \nsecret-access-key:  SECRET-ACCESS-KEY-ID    After that, you use S3-client(s) with those keys when you access the LeoFS storage system.", 
            "title": "Retrieves both an access-key and a secret accesskey"
        }, 
        {
            "location": "/installation/cluster/#configure-an-endpoint", 
            "text": "To make buckets and objects available from a host running S3-client(s), you need to add an endpoint also tweak settings for the name resolution.", 
            "title": "Configure an endpoint"
        }, 
        {
            "location": "/installation/cluster/#assign-a-name-to-leogateway-through-dns-or-etchosts", 
            "text": "In this example, we take the latter way.  1\n2\n3 $ sudo vi /etc/hosts # Add the below line on every host you are supposed to run S3-client(s)  10 .0.0.103 leo_gateway", 
            "title": "Assign a name to LeoGateway through DNS or /etc/hosts"
        }, 
        {
            "location": "/installation/cluster/#add-an-endpoint-through-leofs-adm", 
            "text": "1 $ leofs-adm add-endpoint leo_gateway", 
            "title": "Add an endpoint through leofs-adm"
        }, 
        {
            "location": "/installation/cluster/#make-a-specific-bucket-and-objects-under-the-bucket-available", 
            "text": "You need to resolve the domain name  test.leo_gateway  into the IP address of LeoGateway if you are going to have a bucket named  test .  1\n2\n3 $ sudo vi /etc/hosts # Add the below line on every host you are supposed to run S3-client(s)  10 .0.0.103 test.leo_gateway", 
            "title": "Make a specific bucket and objects under the bucket available"
        }, 
        {
            "location": "/installation/cluster/#usess3cmd-to-access-it", 
            "text": "If you'd like to use  s3cmd  to access the LeoFS storage system, perform the following steps:   Installs  s3cmd  on your machine  Configures  s3cmd   1\n2\n3\n4\n5\n6\n7\n8\n9 $ s3cmd --version\ns3cmd version  1 .6.1\n\n$ s3cmd --configure\n... ## access_key =  access-key-id  ## secret_key =  secret-access-key  ## proxy_host = leo_gateway  ## proxy_port = 8080     Creates a bucket   1\n2 $ s3cmd mb  BUCKET \nBucket  s3:// BUCKET /  created    Puts an object into the bucket   1\n2\n3 $ s3cmd put /path/to/ OBJECT  s3:// BUCKET /\nupload:  OBJET  -   s3:// BUCKET / OBJECT    [ 1  of  1 ] \n  1096  of  1096     100 % in    0s    170 .92 kB/s   done     Gets an object   1\n2\n3 $ s3cmd get s3:// BUCKET / OBJECT \ndownload:  s3:// BUCKET / OBJET  -   ./ OBJECT    [ 1  of  1 ] \n  1096  of  1096     100 % in    0s    307 .38 kB/s   done     Lists objects   1\n2 $ s3cmd ls s3:// BUCKET / 2017 -01-30  02 :24       1096    s3:// BUCKET / OBJECT     Removes an object   1\n2 $ s3cmd del s3:// BUCKET / OBJECT \ndelete:  s3:// BUCKET / OBJECT", 
            "title": "Usess3cmd to access it"
        }, 
        {
            "location": "/installation/cluster/#wrap-up", 
            "text": "You now have a working LeoFS cluster. Make sure to have a look at  Administrators / Settings  to learn more about administration and settings of a LeoFS storage system.", 
            "title": "Wrap up"
        }, 
        {
            "location": "/installation/cluster/#related-links", 
            "text": "Getting Started / Quick Installation and Setup  For Administrators / Setup / Planning for Production  For Administrators / Settings / Environment Configuration  For Administrators / Settings / Cluster Settings  For Administrators / Settings / LeoManager Settings  For Administrators / Settings / LeoStorage Settings  For Administrators / Settings / LeoGateway Settings  For Administrators / System Operations / Cluster Operations  FAQ / LeoFS Clients", 
            "title": "Related Links"
        }, 
        {
            "location": "/architecture/README/", 
            "text": "Architecture\n\n\nConcept\n\n\nWhat we\u2019re focused on is \nhigh availability\n, \nhigh scalability\n, and \nhigh-cost performance ratio\n because unstructured data which have been exponentially increasing day by day, and we needed to more efficiently manage objects to find values from tons of raw data.\n\n\nArchitecture Overview\n\n\nWe succeeded in designing and implementing LeoFS as simple as possible. LeoFS consists of three components, \nLeoManager\n, \nLeoStorage\n, and \nLeoGateway\n. The role of each component is clearly defined.\n\n\n\n\nWhat we also carefully desined LeoFS is three things:\n\n\n\n\nTo keep running LeoFS without SPOF\n\n\nTo keep maintaining high performance regardless of the kind of data and amount data\n\n\nTo provide easy administration by the LeoFS' CLI, \nleofs-adm\n\n\n\n\nRelated Links\n\n\n\n\nConcept and Architecture / LeoGateway's Architecture\n\n\nConcept and Architecture / LeoStorage's Architecture\n\n\nConcept and Architecture / LeoManager's Architecture", 
            "title": "LeoFS' Architecture"
        }, 
        {
            "location": "/architecture/README/#architecture", 
            "text": "", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/README/#concept", 
            "text": "What we\u2019re focused on is  high availability ,  high scalability , and  high-cost performance ratio  because unstructured data which have been exponentially increasing day by day, and we needed to more efficiently manage objects to find values from tons of raw data.", 
            "title": "Concept"
        }, 
        {
            "location": "/architecture/README/#architecture-overview", 
            "text": "We succeeded in designing and implementing LeoFS as simple as possible. LeoFS consists of three components,  LeoManager ,  LeoStorage , and  LeoGateway . The role of each component is clearly defined.   What we also carefully desined LeoFS is three things:   To keep running LeoFS without SPOF  To keep maintaining high performance regardless of the kind of data and amount data  To provide easy administration by the LeoFS' CLI,  leofs-adm", 
            "title": "Architecture Overview"
        }, 
        {
            "location": "/architecture/README/#related-links", 
            "text": "Concept and Architecture / LeoGateway's Architecture  Concept and Architecture / LeoStorage's Architecture  Concept and Architecture / LeoManager's Architecture", 
            "title": "Related Links"
        }, 
        {
            "location": "/architecture/leo_gateway/", 
            "text": "LeoGateway's Architecture\n\n\nLeoGateway consists of the fast HTTP server which is \nCowboy\n, the multi-protocols handler, and the object cache. It provides multi-protocols which are the RESTful API, \nAmazon S3-API\n, and NFS v3. If you adopt using Amazon S3-API, you can easily access LeoFS with S3 clients which include \ns3cmd\n, \nDragonDisk\n and AWS SDKs - \nJava\n, \nRuby\n, \nGo\n, \nPython (Boto3)\n and others.\n\n\n\n\nA client requests an object or a bucket operation to a LeoGateway node, then it requests the message of an operation to a LeoStorage node.\n\n\nA destination LeoStorage node is decided by RING \n(distributed hash table)\n, which is generated and distributed at LeoManager nodes.\n\n\nLeoGateway also provides built-in support for the object cache to realize keeping high performance and reduction of traffic between LeoGateway nodes and LeoStorage nodes.\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoGateway Settings\n\n\nFor Administrators / Interface / S3-API\n\n\nFor Administrators / Interface / REST-API\n\n\nFor Administrators / Interface / NFS v3\n\n\nFor Administrators / System Operations / S3-API related Operations", 
            "title": "LeoGateway's Architecture"
        }, 
        {
            "location": "/architecture/leo_gateway/#leogateways-architecture", 
            "text": "LeoGateway consists of the fast HTTP server which is  Cowboy , the multi-protocols handler, and the object cache. It provides multi-protocols which are the RESTful API,  Amazon S3-API , and NFS v3. If you adopt using Amazon S3-API, you can easily access LeoFS with S3 clients which include  s3cmd ,  DragonDisk  and AWS SDKs -  Java ,  Ruby ,  Go ,  Python (Boto3)  and others.   A client requests an object or a bucket operation to a LeoGateway node, then it requests the message of an operation to a LeoStorage node.  A destination LeoStorage node is decided by RING  (distributed hash table) , which is generated and distributed at LeoManager nodes.  LeoGateway also provides built-in support for the object cache to realize keeping high performance and reduction of traffic between LeoGateway nodes and LeoStorage nodes.", 
            "title": "LeoGateway's Architecture"
        }, 
        {
            "location": "/architecture/leo_gateway/#related-links", 
            "text": "For Administrators / Settings / LeoGateway Settings  For Administrators / Interface / S3-API  For Administrators / Interface / REST-API  For Administrators / Interface / NFS v3  For Administrators / System Operations / S3-API related Operations", 
            "title": "Related Links"
        }, 
        {
            "location": "/architecture/leo_storage/", 
            "text": "LeoStorage's Architecture\n\n\nFundamentals\n\n\nLeoStorage consists of \nthe object storage\n and \nthe metadata storage\n, and it includes replicator and repairer realize eventual consistency.\n\n\nWRITE-Request Handling\n\n\nA LeoStorage node accepts a request from a LeoGateway node then automatically replicate an object into the LeoStorage cluster. Finally, its LeoStorage node confirms whether a stored object satisfies the consistency rule.\n\n\nREAD-Request Handling\n\n\nA LeoGateway node requests a LeoStorage node; then its LeoStorage node retrieves an object from the local object-storage or a remote LeoStorage node. Finally, its LeoStorage node responds an object to its LeoGateway node. Also, its LeoStorage node checks the consistency with the asynchronous processing. Please note that \nLeoGateway cache settings\n can affect requests handling.\n\n\nIf its LeoStorage node finds inconsistency of an object, its node fixes the inconsistent object with the backend process. Its object eventually keeps consistency with the functions.\n\n\n\n\nData Structure\n\n\nLeoFS\u2019 object consists of three layers which are \nmetadata\n, \nneedle\n and \nobject-container\n.\n\n\n\n\nLeoObjectStorage\n manages and stores both an object and metadata which stores as a needle.\n\n\nLeoObjectStorage's metadata-storage handles and stores attributes of an object which includes filename, size, checksum, and others, and it depends on \nLeveldb\n.\n\n\nLeoObjectStorage's object-container adopts a log structured file format, which is robust and high performance because an effect of the local file system is just a little part, and LeoStorage is necessary to remove unnecessary objects from the object containers, which is realized by the data compaction feature.\n\n\n\n\n\n\nLarge Object Support\n\n\nLeoFS supports handling a large size object since v0.12. The purpose of this feature is two things:\n\n\n\n\nTo equalize disk usage of each LeoStorage node.\n\n\nTo realize high I/O efficiency and high availability.\n\n\n\n\nWRITE-Request Handling\n\n\nA LeoGateway node divides a large size object into plural objects, then those chunks are replicated into a LeoStorage cluster which is similar to handling small size objects, and the default chunk size is 5MB, the configuration of which can change a custom chunked object size.\n\n\nREAD-Request Handling\n\n\nA LeoGateway node retrieves a metadata of a requested object, then if it's a large size object, its LeoGateway node retrieves the chunked objects in order of the fragment object number from the LeoStorage cluster. Finally, its LeoGateway node responds the objects to the client.\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoStorage Settings\n\n\nFor Administrators / System Operations / Cluster Operations", 
            "title": "LeoStorage's Architecture"
        }, 
        {
            "location": "/architecture/leo_storage/#leostorages-architecture", 
            "text": "", 
            "title": "LeoStorage's Architecture"
        }, 
        {
            "location": "/architecture/leo_storage/#fundamentals", 
            "text": "LeoStorage consists of  the object storage  and  the metadata storage , and it includes replicator and repairer realize eventual consistency.", 
            "title": "Fundamentals"
        }, 
        {
            "location": "/architecture/leo_storage/#write-request-handling", 
            "text": "A LeoStorage node accepts a request from a LeoGateway node then automatically replicate an object into the LeoStorage cluster. Finally, its LeoStorage node confirms whether a stored object satisfies the consistency rule.", 
            "title": "WRITE-Request Handling"
        }, 
        {
            "location": "/architecture/leo_storage/#read-request-handling", 
            "text": "A LeoGateway node requests a LeoStorage node; then its LeoStorage node retrieves an object from the local object-storage or a remote LeoStorage node. Finally, its LeoStorage node responds an object to its LeoGateway node. Also, its LeoStorage node checks the consistency with the asynchronous processing. Please note that  LeoGateway cache settings  can affect requests handling.  If its LeoStorage node finds inconsistency of an object, its node fixes the inconsistent object with the backend process. Its object eventually keeps consistency with the functions.", 
            "title": "READ-Request Handling"
        }, 
        {
            "location": "/architecture/leo_storage/#data-structure", 
            "text": "LeoFS\u2019 object consists of three layers which are  metadata ,  needle  and  object-container .   LeoObjectStorage  manages and stores both an object and metadata which stores as a needle.  LeoObjectStorage's metadata-storage handles and stores attributes of an object which includes filename, size, checksum, and others, and it depends on  Leveldb .  LeoObjectStorage's object-container adopts a log structured file format, which is robust and high performance because an effect of the local file system is just a little part, and LeoStorage is necessary to remove unnecessary objects from the object containers, which is realized by the data compaction feature.", 
            "title": "Data Structure"
        }, 
        {
            "location": "/architecture/leo_storage/#large-object-support", 
            "text": "LeoFS supports handling a large size object since v0.12. The purpose of this feature is two things:   To equalize disk usage of each LeoStorage node.  To realize high I/O efficiency and high availability.", 
            "title": "Large Object Support"
        }, 
        {
            "location": "/architecture/leo_storage/#write-request-handling_1", 
            "text": "A LeoGateway node divides a large size object into plural objects, then those chunks are replicated into a LeoStorage cluster which is similar to handling small size objects, and the default chunk size is 5MB, the configuration of which can change a custom chunked object size.", 
            "title": "WRITE-Request Handling"
        }, 
        {
            "location": "/architecture/leo_storage/#read-request-handling_1", 
            "text": "A LeoGateway node retrieves a metadata of a requested object, then if it's a large size object, its LeoGateway node retrieves the chunked objects in order of the fragment object number from the LeoStorage cluster. Finally, its LeoGateway node responds the objects to the client.", 
            "title": "READ-Request Handling"
        }, 
        {
            "location": "/architecture/leo_storage/#related-links", 
            "text": "For Administrators / Settings / LeoStorage Settings  For Administrators / System Operations / Cluster Operations", 
            "title": "Related Links"
        }, 
        {
            "location": "/architecture/leo_manager/", 
            "text": "LeoManager's Architecture\n\n\nLeoManager monitors the state of LeoGateway and LeoStorage nodes to keep the high availability of a LeoFS system. A consistency of RING (\ndistributed hash table\n) of LeoGateway and LeoStorage nodes are always monitored by LeoManager to prevent \nsplit-brain\n.\n\n\n\n\nBoth LeoManager nodes manage configurations of a system and information of every node to be able to recover a system reliability, and the data are replicated by \nErlang Mnesia\n to avoid data loss.\n\n\nLeoManager provides \nleofs-adm as a LeoFS administration commands\n to be able to operate LeoFS quickly. The administration commands already cover entire LeoFS features.\n\n\nRelated Links\n\n\n\n\nFor Administrators / Index of LeoFS' Commands\n\n\nFor Administrators / Settings / LeoManager Settings", 
            "title": "LeoManager's Architecture"
        }, 
        {
            "location": "/architecture/leo_manager/#leomanagers-architecture", 
            "text": "LeoManager monitors the state of LeoGateway and LeoStorage nodes to keep the high availability of a LeoFS system. A consistency of RING ( distributed hash table ) of LeoGateway and LeoStorage nodes are always monitored by LeoManager to prevent  split-brain .   Both LeoManager nodes manage configurations of a system and information of every node to be able to recover a system reliability, and the data are replicated by  Erlang Mnesia  to avoid data loss.  LeoManager provides  leofs-adm as a LeoFS administration commands  to be able to operate LeoFS quickly. The administration commands already cover entire LeoFS features.", 
            "title": "LeoManager's Architecture"
        }, 
        {
            "location": "/architecture/leo_manager/#related-links", 
            "text": "For Administrators / Index of LeoFS' Commands  For Administrators / Settings / LeoManager Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/setup/planning_for_production/", 
            "text": "Planning for Production\n\n\nThis section provides information to set up LeoFS on the production environment. Planning for production involves ensuring that the hardware and software requirements are met, and the deployment and security consideration are taken into account before installing LeoFS. You can choose to install LeoFS on-premises on \nsupported platforms\n or virtualization platforms.\n\n\nBefore you begin the actual installation, ensure that you go through the section.\nFor installation instructions on different platforms:\n\n\n\n\nOn-premises\n\n\nVirtualization platforms:\n\n\nAmazon Web Services\n and \nDocker\n \n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Setup / Supported Platforms\n\n\nFor Administrators / Setup / Hardware Requirements\n\n\nFor Administrators / Setup / Network Configuration", 
            "title": "Planning for Production"
        }, 
        {
            "location": "/admin/setup/planning_for_production/#planning-for-production", 
            "text": "This section provides information to set up LeoFS on the production environment. Planning for production involves ensuring that the hardware and software requirements are met, and the deployment and security consideration are taken into account before installing LeoFS. You can choose to install LeoFS on-premises on  supported platforms  or virtualization platforms.  Before you begin the actual installation, ensure that you go through the section.\nFor installation instructions on different platforms:   On-premises  Virtualization platforms:  Amazon Web Services  and  Docker", 
            "title": "Planning for Production"
        }, 
        {
            "location": "/admin/setup/planning_for_production/#related-links", 
            "text": "For Administrators / Setup / Supported Platforms  For Administrators / Setup / Hardware Requirements  For Administrators / Setup / Network Configuration", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/setup/supported_platforms/", 
            "text": "Supported Platforms\n\n\nLeoFS supports popular operating systems and virtual environments.\n\n\n\n\n\n\n\n\nPlatform\n\n\nVersion\n\n\n32/64 bit\n\n\nSupported\n\n\n\n\n\n\n\n\n\n\nCentOS\n\n\n7\n\n\n64 bit\n\n\nDevelopment, testing and production\n\n\n\n\n\n\nCentOS\n\n\n6\n\n\n64 bit\n\n\nDevelopment, testing and production\n\n\n\n\n\n\nUbuntu Linux\n\n\n16.04\n\n\n64 bit\n\n\nDevelopment, testing and production\n\n\n\n\n\n\nUbuntu Linux\n\n\n14.04\n\n\n64 bit\n\n\nDevelopment, testing and production\n\n\n\n\n\n\nAWS.EC2 Red Hat Enterprise Linux\n\n\n7\n\n\n64 bit\n\n\nDevelopment, testing\n\n\n\n\n\n\nAWS.EC2 Red Hat Enterprise Linux\n\n\n6\n\n\n64 bit\n\n\nDevelopment, testing\n\n\n\n\n\n\nAWS.EC2 Ubuntu\n\n\n16.04\n\n\n64 bit\n\n\nDevelopment, testing\n\n\n\n\n\n\nAWS.EC2 Ubuntu\n\n\n14.04\n\n\n64 bit\n\n\nDevelopment, testing\n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Setup / Planning for Production", 
            "title": "Supported Platforms"
        }, 
        {
            "location": "/admin/setup/supported_platforms/#supported-platforms", 
            "text": "LeoFS supports popular operating systems and virtual environments.     Platform  Version  32/64 bit  Supported      CentOS  7  64 bit  Development, testing and production    CentOS  6  64 bit  Development, testing and production    Ubuntu Linux  16.04  64 bit  Development, testing and production    Ubuntu Linux  14.04  64 bit  Development, testing and production    AWS.EC2 Red Hat Enterprise Linux  7  64 bit  Development, testing    AWS.EC2 Red Hat Enterprise Linux  6  64 bit  Development, testing    AWS.EC2 Ubuntu  16.04  64 bit  Development, testing    AWS.EC2 Ubuntu  14.04  64 bit  Development, testing", 
            "title": "Supported Platforms"
        }, 
        {
            "location": "/admin/setup/supported_platforms/#related-links", 
            "text": "For Administrators / Setup / Planning for Production", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/setup/hardware_requirements/", 
            "text": "Hardware Requirements\n\n\nMinimum Requirements\n\n\n\n\n\n\n\n\n\n\nCPU\n\n\nMemory\n\n\nDisk\n\n\n\n\n\n\n\n\n\n\nManager\n\n\n1\n\n\n512 MB\n\n\n20 GB\n\n\n\n\n\n\nGateway\n\n\n2\n\n\n1 GB\n\n\n20 GB\n\n\n\n\n\n\nStorage\n\n\n2\n\n\n1 GB\n\n\n100 GB\n\n\n\n\n\n\n\n\nHardware Recommendation\n\n\n\n\nWorkload on Manager is low\n, it does not consume many resources\n\n\nBetter CPU allows LeoGateway\n to process more operations (OPS, \nOperation Per Second\n)\n\n\nLeoGateway utilizes \nmemory and disk as cache\n, adding those resources can reduce the workload to LeoStorage\n\n\nSSD\n on a LeoStoage node significantly improves small object read performance\n\n\n10Gbps network\n is recommended\n\n\n\n\nReference Platform\n\n\n\n\n\n\n\n\nHardware\n\n\nDetail\n\n\n\n\n\n\n\n\n\n\nCPU\n\n\n16C32T (Intel E5-2630 v3)\n\n\n\n\n\n\nMemory\n\n\n32 GB\n\n\n\n\n\n\nNetwork\n\n\n10 GbE\n\n\n\n\n\n\nDisk\n\n\nSSD (Crucial BX100)\n\n\n\n\n\n\n\n\nPerformance\n\n\n\n\n\n\n\n\nData Set\n\n\nRead\n\n\nWrite\n\n\nResource Usage\n\n\n\n\n\n\n\n\n\n\nImage (32KB)\n\n\n20,000 OPS\n\n\n20,000 OPS\n\n\nHigh CPU Usage\n\n\n\n\n\n\nSmall Mixed (\n2MB)\n\n\n1,200 OPS\n\n\n1,500 OPS\n\n\nHigh Disk I/O\n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Setup / Planning for Production\n\n\nleo-project/notes - LeoFS Benchmark Report", 
            "title": "Hardware Requirements"
        }, 
        {
            "location": "/admin/setup/hardware_requirements/#hardware-requirements", 
            "text": "", 
            "title": "Hardware Requirements"
        }, 
        {
            "location": "/admin/setup/hardware_requirements/#minimum-requirements", 
            "text": "CPU  Memory  Disk      Manager  1  512 MB  20 GB    Gateway  2  1 GB  20 GB    Storage  2  1 GB  100 GB", 
            "title": "Minimum Requirements"
        }, 
        {
            "location": "/admin/setup/hardware_requirements/#hardware-recommendation", 
            "text": "Workload on Manager is low , it does not consume many resources  Better CPU allows LeoGateway  to process more operations (OPS,  Operation Per Second )  LeoGateway utilizes  memory and disk as cache , adding those resources can reduce the workload to LeoStorage  SSD  on a LeoStoage node significantly improves small object read performance  10Gbps network  is recommended", 
            "title": "Hardware Recommendation"
        }, 
        {
            "location": "/admin/setup/hardware_requirements/#reference-platform", 
            "text": "Hardware  Detail      CPU  16C32T (Intel E5-2630 v3)    Memory  32 GB    Network  10 GbE    Disk  SSD (Crucial BX100)", 
            "title": "Reference Platform"
        }, 
        {
            "location": "/admin/setup/hardware_requirements/#performance", 
            "text": "Data Set  Read  Write  Resource Usage      Image (32KB)  20,000 OPS  20,000 OPS  High CPU Usage    Small Mixed ( 2MB)  1,200 OPS  1,500 OPS  High Disk I/O", 
            "title": "Performance"
        }, 
        {
            "location": "/admin/setup/hardware_requirements/#related-links", 
            "text": "For Administrators / Setup / Planning for Production  leo-project/notes - LeoFS Benchmark Report", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/setup/network_config/", 
            "text": "Network Configurations\n\n\nFirewall Rules\n\n\nIn order for a LeoFS system to operate correctly, it is necessary to set and check the firewall rules in your environment. LeoFS depends on \nErlang/OTP\n's RPC, which uses specified ports and provides LeoFS' SNMP agent which also uses a port per a LeoFS' component node - LeoStorage, LeoGateway, and LeoManager.\n\n\n\n\n\n\n\n\nSubsystem\n\n\nDirection\n\n\nPorts\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLeoManager (master)\n\n\n\n\n\n\n\n\n\n\n\n\nLeoManager (master)\n\n\nIncoming\n\n\n10010/*\n\n\nLeoManager console (text)\n\n\n\n\n\n\nLeoManager (master)\n\n\nIncoming\n\n\n10020/*\n\n\nLeoManager console (json)\n\n\n\n\n\n\nLeoManager (master)\n\n\nIncoming\n\n\n4369/*\n\n\nErlang Port Mapper\n\n\n\n\n\n\nLeoManager (master)\n\n\nIncoming\n\n\n4020/*\n\n\nSNMP Listen Port\n\n\n\n\n\n\nLeoManager (master)\n\n\nOutgoing\n\n\n*/4369\n\n\nErlang Port Mapper\n\n\n\n\n\n\nLeoManager (slave)\n\n\n\n\n\n\n\n\n\n\n\n\nLeoManager (slave)\n\n\nIncoming\n\n\n10011/*\n\n\nLeoManager console (text)\n\n\n\n\n\n\nLeoManager (slave)\n\n\nIncoming\n\n\n10021/*\n\n\nLeoManager console (json)\n\n\n\n\n\n\nLeoManager (slave)\n\n\nIncoming\n\n\n4369/*\n\n\nErlang Port Mapper\n\n\n\n\n\n\nLeoManager (slave)\n\n\nIncoming\n\n\n4021/*\n\n\nSNMP Listen Port\n\n\n\n\n\n\nLeoManager (slave)\n\n\nOutgoing\n\n\n*/4369\n\n\nErlang Port Mapper\n\n\n\n\n\n\nLeoStorage\n\n\n\n\n\n\n\n\n\n\n\n\nLeoStorage\n\n\nIncoming\n\n\n4369/*\n\n\nErlang Port Mapper\n\n\n\n\n\n\nLeoStorage\n\n\nIncoming\n\n\n4010/*\n\n\nSNMP Listen Port\n\n\n\n\n\n\nLeoStorage\n\n\nOutgoing\n\n\n*/4369\n\n\nErlang Port Mapper\n\n\n\n\n\n\nLeoGateway\n\n\n\n\n\n\n\n\n\n\n\n\nLeoGateway\n\n\nIncoming\n\n\n8080/*\n\n\nHTTP listen port\n\n\n\n\n\n\nLeoGateway\n\n\nIncoming\n\n\n8443/*\n\n\nHTTPS listen port\n\n\n\n\n\n\nLeoGateway\n\n\nIncoming\n\n\n4369/*\n\n\nErlang Port Mapper\n\n\n\n\n\n\nLeoGateway\n\n\nIncoming\n\n\n4000/*\n\n\nSNMP Listen Port\n\n\n\n\n\n\nLeoGateway\n\n\nOutgoing\n\n\n*/4369\n\n\nErlang Port Mapper\n\n\n\n\n\n\n\n\nHow to Change Erlang's Port Range\n\n\nPort range can be specified by setting Erlang's kernel variables \ninet_dist_listen_min\n and \ninet_dist_listen_max\n. If you need to change those configuration items, you need to enter the Erlang console of a target node.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## Example:\n\n\n##   - This forces Erlang to use only ports 9100--9105 for distributed Erlang traffic.\n\n$ bin/leo_storage remote_console\n\n\n(\nstorage_0@127.0.0.1\n)\n1\n application:set_env\n(\nkernel, inet_dist_listen_min, \n9100\n)\n.\n\n(\nstorage_0@127.0.0.1\n)\n2\n application:set_env\n(\nkernel, inet_dist_listen_max, \n9105\n)\n.\n\n\n## Press [CTRL+c] to exit the Erlang console\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoManager Settings\n\n\nFor Administrators / Settings / LeoStorage Settings\n\n\nFor Administrators / Settings / LeoGateway Settings", 
            "title": "Network Configurations"
        }, 
        {
            "location": "/admin/setup/network_config/#network-configurations", 
            "text": "", 
            "title": "Network Configurations"
        }, 
        {
            "location": "/admin/setup/network_config/#firewall-rules", 
            "text": "In order for a LeoFS system to operate correctly, it is necessary to set and check the firewall rules in your environment. LeoFS depends on  Erlang/OTP 's RPC, which uses specified ports and provides LeoFS' SNMP agent which also uses a port per a LeoFS' component node - LeoStorage, LeoGateway, and LeoManager.     Subsystem  Direction  Ports  Description      LeoManager (master)       LeoManager (master)  Incoming  10010/*  LeoManager console (text)    LeoManager (master)  Incoming  10020/*  LeoManager console (json)    LeoManager (master)  Incoming  4369/*  Erlang Port Mapper    LeoManager (master)  Incoming  4020/*  SNMP Listen Port    LeoManager (master)  Outgoing  */4369  Erlang Port Mapper    LeoManager (slave)       LeoManager (slave)  Incoming  10011/*  LeoManager console (text)    LeoManager (slave)  Incoming  10021/*  LeoManager console (json)    LeoManager (slave)  Incoming  4369/*  Erlang Port Mapper    LeoManager (slave)  Incoming  4021/*  SNMP Listen Port    LeoManager (slave)  Outgoing  */4369  Erlang Port Mapper    LeoStorage       LeoStorage  Incoming  4369/*  Erlang Port Mapper    LeoStorage  Incoming  4010/*  SNMP Listen Port    LeoStorage  Outgoing  */4369  Erlang Port Mapper    LeoGateway       LeoGateway  Incoming  8080/*  HTTP listen port    LeoGateway  Incoming  8443/*  HTTPS listen port    LeoGateway  Incoming  4369/*  Erlang Port Mapper    LeoGateway  Incoming  4000/*  SNMP Listen Port    LeoGateway  Outgoing  */4369  Erlang Port Mapper", 
            "title": "Firewall Rules"
        }, 
        {
            "location": "/admin/setup/network_config/#how-to-change-erlangs-port-range", 
            "text": "Port range can be specified by setting Erlang's kernel variables  inet_dist_listen_min  and  inet_dist_listen_max . If you need to change those configuration items, you need to enter the Erlang console of a target node.  1\n2\n3\n4\n5\n6\n7\n8 ## Example:  ##   - This forces Erlang to use only ports 9100--9105 for distributed Erlang traffic. \n$ bin/leo_storage remote_console ( storage_0@127.0.0.1 ) 1  application:set_env ( kernel, inet_dist_listen_min,  9100 ) . ( storage_0@127.0.0.1 ) 2  application:set_env ( kernel, inet_dist_listen_max,  9105 ) . ## Press [CTRL+c] to exit the Erlang console", 
            "title": "How to Change Erlang's Port Range"
        }, 
        {
            "location": "/admin/setup/network_config/#related-links", 
            "text": "For Administrators / Settings / LeoManager Settings  For Administrators / Settings / LeoStorage Settings  For Administrators / Settings / LeoGateway Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_admin/load_balancing/", 
            "text": "Running LeoFS gateways behind load balancer\n\n\nExample configuration\n\n\nUsing load balancer (LB) in front of at least two LeoGateways that are operating in S3 or REST mode is always a good idea for production and staging systems; besides distributing the load between multiple gateways, it provides ability to restart and upgrade gateway servers without affecting service. It's likely that any LB that works in HTTP mode can be used in front of LeoGateways (TCP and IP-level LBs are a bad choice for this). Here is example configuration for HAProxy\n1\n (tested on version 1.6) in front of 4 backends (LeoGateways).\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\nglobal\n    log 127.0.0.1     local0\n    log 127.0.0.1     local1 notice\n    user haproxy\n    group haproxy\n    maxconn 90000\n    spread-checks 5\n\ndefaults\n    log     global\n    option  dontlognull\n    option  redispatch\n    option  allbackups\n    no option  httpclose\n    retries 3\n    maxconn 90000\n    timeout connect 5000\n    timeout check   3000\n    timeout client  30000\n    timeout server  35000\n\nfrontend leofs\n    bind    192.168.130.66:8080\n    mode    http\n    option  httplog\n    capture request header Host len 64\n    use_backend leofs_backend\n\nbackend leofs_backend\n    mode    http\n    balance roundrobin\n    option  httpchk GET /_leofs_adm/ping HTTP/1.1\\r\\nHost:\\ s3.amazonaws.com\n    timeout http-keep-alive 4500\n\n    server    gw0 gw0.lan:8081 weight 1 maxconn 10000 check\n    server    gw1 gw1.lan:8081 weight 1 maxconn 10000 check\n    server    gw2 gw2.lan:8081 weight 1 maxconn 10000 check\n    server    gw3 gw3.lan:8081 weight 1 maxconn 10000 check\n\n\n\n\n\n\nHealth Check URL\n\n\nOne of the important options here is \noption httpchk\n, which makes LB check if gateway is working and can connect to storage servers. Problematic gateways (which refuse connection, reply with an error or don't provide reply fast enough, which is controlled by \ntimeout check\n parameter here) can be automatically disabled / re-enabled this way. The URL for health check is always \n/_leofs_adm/ping\n, regardless of gateway mode (S3 or REST). For S3 mode, \"Host:\" header is required as well (any S3 endpoint can be used there). Similar feature should be activated in configuration of any LB.\n\n\n\n\nNote: Health Check in REST mode\n\n\nYou can't get an object stored at \n/_leofs_adm/ping\n as it's shadowed by the Health Check URL so be careful not to put any object at \n/_leofs_adm/ping\n in REST mode.\n\n\n\n\nThings to check for production systems\n\n\nGenerally, it's best to use latest version of HAProxy (or, at very least, 1.6). This example is suited (and probably doesn't require much tweaking) for people who just need LB for extra reliability. However, please note that running a high load production system would require tweaking of various timeouts, LB / backend and LB / client connection parameters. Depending on clients and type of load, there is no single perfect solution; it requires tweaking, experimentation and performance measurements (most LBs have internal statistics which can help here), besides, often OS-level tweaking is required as well. Here are some important points both for tweaking this configuration to handle high load and for configuring other load balancers.\n\n\nOpen files limit\n: each connection uses an open file (socket), and operating system limits amount of open files for processes. Generally, for users of official Linux packages that are running LeoFS v1.3.8 and higher these limits should be increased automatically. For other cases, please consult documentation for your OS / distro on how to properly increase file limits for processes; it's a good idea to set this limit to high value like 65535 for LeoGateway (usually, default is 1024). Open files limit should be raised for LB as well, it should be bigger than maximum allowed amount of connections to all backend servers plus maximum amount of client connections; in case of HAProxy, it must be set higher than number in global \nmaxconn\n option (e.g. 31000 for this example with \nmaxconn 30000\n).\n\n\nRequest timeouts\n: LB can impose a time limit on how fast backend server must give a reply (time before it starts sending HTTP headers). In case of HAProxy, it's \ntimeout server\n option: when backend takes more time to reply than the time specified there, HAProxy will send \"504 Gateway Timeout\" to client instead. However, since LeoGateway has its own timeout logic, aborting request like that is a bad idea, since gateway was still busy working on that request. Either LB timeouts or gateway timeouts (or both) should be tweaked and LB server-side timeout should be set \nbigger\n than timeouts in leo_gateway.conf (\ntimeout.get\n and \ntimeout.ls\n, which are 30 seconds by default). This way, gateway will give its own (503) reply to client when it can't process request due to high load and LB will generate \"504 Gateway Timeout\" only in special cases like network problem between LB and backend. Since timeouts in leo_gateway.conf are set for internal operation between LeoGateway and LeoStorage, the overall delay between request to gateway and reply might be bigger than that, so timeout in LB configuration should be set appropriately. Also, when requesting a large object, gateway can insert delays between chunks of data up to \ntimeout.get\n value. Some LBs need to be tweaked to handle these delays without closing the connection (in case of HAProxy, it's \ntimeout tunnel\n option), though generally it shouldn't be a problem.\n\n\nExample:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# timeouts in leo_gateway.conf set to 30 seconds\ntimeout.get = 30000\ntimeout.ls = 30000\n\n# timeout for headers in haproxy.cfg set to higher value - 35 seconds\n# mid-stream inactivity timeout (both directions) set to 10 minutes\nbackend\n    timeout server 35000\n    timeout tunnel 10m\n\n\n\n\n\n\nHTTP connection reuse\n: many LBs offer a feature of reusing the same connections to backends to handle requests from various clients. Generally this mode isn't default because it doesn't work with all types of backends. However, it works with LeoGateway since it operates in stateless (REST) mode (this includes operating in S3 protocol mode which follows REST as well). Under some conditions, like requests from very high amount of clients, this option can significantly decrease latency and reduce load on LB and gateway, since it allows to use much smaller amount of connections between LB and backend compared to amount of connections between LB and clients, and save time / CPU on re-establishing TCP connections to backend. In HAProxy, this is controlled by \nhttp-reuse\n directive, e.g. \nhttp-reuse always\n.\n\n\nLong-lived keep-alive tunnels\n: alternative to the above, LBs can use one-on-one connection mappings between clients and backends. Each connection from client creates new connection to backend, which is closed when client closes the connection, so they behave like tunnels from client to backend. Under some conditions (clients use HTTP keep-alive, low amount of clients, long-lived connections with high rate of requests) this might be a better option compared to connection reuse. Modern versions of HAProxy use this mode by default, but older versions (1.5 or earlier) and other LBs might behave differently. With this style of connections, number of connections from clients that LB can handle is limited by maximum allowed amount of connections to all active backends. This mode shouldn't be confused with keep-alive connections only between clients and LB.\n\n\nActivating either of these features (reusing connection to backends or tunnels with keep-alive to both client and backend) can significantly reduce load and increase performance compared to establishing connection between LB and backend for every request. However, the downside is that memory requirements for LB are increased and some extra tweaking might be needed. The most important tweak is keep-alive timeout between LB and backend. LeoGateway has timeout parameters (\nhttp.timeout_for_header\n and \nhttp.timeout_for_body\n), first of which is set to 5s by default. It means that keep-alive connection without any traffic in it for 5 seconds will be closed by LeoGateway. It's a good idea to increase this parameter when operating in the above modes. Regardless whether it's increased or not, the similar parameter on LB side should be set to \nlower\n value than timeout on LeoGateway. In HAProxy, this is controlled by \ntimeout http-keep-alive\n in the backend section. This is especially important when HTTP connections are reused, because it avoids the problem when LB tries to send data into existing connection at the same time as LeoGateway closes that connection. Ensuring that LeoGateway keeps the inactive connection open a bit longer than the time after which LB will close connection from its side solves this problem.\n\n\nExample:\n\n\n1\n2\n3\n4\n5\n6\n# keep-alive timeout between requests in leo_gateway.conf set to 30 seconds\nhttp.timeout_for_header = 30000\n\n# keep-alive timeout in haproxy.cfg set to smaller value - 29 seconds\nbackend\n    timeout http-keep-alive 29000\n\n\n\n\n\n\nSSL/TLS termination\n: obviously, for LB to work with LeoGateway in HTTP protocol mode, SSL has to be terminated on LB itself. Usually (e.g. in case of HAProxy in multi-process mode) it will provide much higher SSL performance compared to serving SSL from LeoGateway directly, so LeoGateway should always be configured to use HTTP when operating behind LB. In cases where secure traffic between LB and LeoGateway is a must, either LB should be configured to decrypt/re-encrypt SSL traffic (so it can still operate in HTTP and not in TCP tunneling mode), or some other layer of encryption, like VPN should be used.\n\n\nHigh availability\n: a single load balancer becomes a point of failure (PoF) itself. For highly available configurations, at least two LBs are needed, configured for failover (e.g. using floating IP with Corosync / Pacemaker\n2\n). Alternatively, IP-level LB (e.g. Linux Virtual Server\n3\n) can be set up in front of HTTP-level LBs. IP-level balancing solutions provide higher availability compared to failover and can distribute load between multiple HTTP-level LBs, but require dedicated hardware.\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoGateway Settings\n\n\n\n\n\n\n\n\n\n\n\n\nHAProxy version 1.6 - Configuration Manual\n\n\n\n\n\n\nHigh Availability Cluster Stack\n\n\n\n\n\n\nThe Linux Virtual Server Project", 
            "title": "Load Balancing Configuration"
        }, 
        {
            "location": "/admin/system_admin/load_balancing/#running-leofs-gateways-behind-load-balancer", 
            "text": "", 
            "title": "Running LeoFS gateways behind load balancer"
        }, 
        {
            "location": "/admin/system_admin/load_balancing/#example-configuration", 
            "text": "Using load balancer (LB) in front of at least two LeoGateways that are operating in S3 or REST mode is always a good idea for production and staging systems; besides distributing the load between multiple gateways, it provides ability to restart and upgrade gateway servers without affecting service. It's likely that any LB that works in HTTP mode can be used in front of LeoGateways (TCP and IP-level LBs are a bad choice for this). Here is example configuration for HAProxy 1  (tested on version 1.6) in front of 4 backends (LeoGateways).   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38 global\n    log 127.0.0.1     local0\n    log 127.0.0.1     local1 notice\n    user haproxy\n    group haproxy\n    maxconn 90000\n    spread-checks 5\n\ndefaults\n    log     global\n    option  dontlognull\n    option  redispatch\n    option  allbackups\n    no option  httpclose\n    retries 3\n    maxconn 90000\n    timeout connect 5000\n    timeout check   3000\n    timeout client  30000\n    timeout server  35000\n\nfrontend leofs\n    bind    192.168.130.66:8080\n    mode    http\n    option  httplog\n    capture request header Host len 64\n    use_backend leofs_backend\n\nbackend leofs_backend\n    mode    http\n    balance roundrobin\n    option  httpchk GET /_leofs_adm/ping HTTP/1.1\\r\\nHost:\\ s3.amazonaws.com\n    timeout http-keep-alive 4500\n\n    server    gw0 gw0.lan:8081 weight 1 maxconn 10000 check\n    server    gw1 gw1.lan:8081 weight 1 maxconn 10000 check\n    server    gw2 gw2.lan:8081 weight 1 maxconn 10000 check\n    server    gw3 gw3.lan:8081 weight 1 maxconn 10000 check", 
            "title": "Example configuration"
        }, 
        {
            "location": "/admin/system_admin/load_balancing/#health-check-url", 
            "text": "One of the important options here is  option httpchk , which makes LB check if gateway is working and can connect to storage servers. Problematic gateways (which refuse connection, reply with an error or don't provide reply fast enough, which is controlled by  timeout check  parameter here) can be automatically disabled / re-enabled this way. The URL for health check is always  /_leofs_adm/ping , regardless of gateway mode (S3 or REST). For S3 mode, \"Host:\" header is required as well (any S3 endpoint can be used there). Similar feature should be activated in configuration of any LB.   Note: Health Check in REST mode  You can't get an object stored at  /_leofs_adm/ping  as it's shadowed by the Health Check URL so be careful not to put any object at  /_leofs_adm/ping  in REST mode.", 
            "title": "Health Check URL"
        }, 
        {
            "location": "/admin/system_admin/load_balancing/#things-to-check-for-production-systems", 
            "text": "Generally, it's best to use latest version of HAProxy (or, at very least, 1.6). This example is suited (and probably doesn't require much tweaking) for people who just need LB for extra reliability. However, please note that running a high load production system would require tweaking of various timeouts, LB / backend and LB / client connection parameters. Depending on clients and type of load, there is no single perfect solution; it requires tweaking, experimentation and performance measurements (most LBs have internal statistics which can help here), besides, often OS-level tweaking is required as well. Here are some important points both for tweaking this configuration to handle high load and for configuring other load balancers.  Open files limit : each connection uses an open file (socket), and operating system limits amount of open files for processes. Generally, for users of official Linux packages that are running LeoFS v1.3.8 and higher these limits should be increased automatically. For other cases, please consult documentation for your OS / distro on how to properly increase file limits for processes; it's a good idea to set this limit to high value like 65535 for LeoGateway (usually, default is 1024). Open files limit should be raised for LB as well, it should be bigger than maximum allowed amount of connections to all backend servers plus maximum amount of client connections; in case of HAProxy, it must be set higher than number in global  maxconn  option (e.g. 31000 for this example with  maxconn 30000 ).  Request timeouts : LB can impose a time limit on how fast backend server must give a reply (time before it starts sending HTTP headers). In case of HAProxy, it's  timeout server  option: when backend takes more time to reply than the time specified there, HAProxy will send \"504 Gateway Timeout\" to client instead. However, since LeoGateway has its own timeout logic, aborting request like that is a bad idea, since gateway was still busy working on that request. Either LB timeouts or gateway timeouts (or both) should be tweaked and LB server-side timeout should be set  bigger  than timeouts in leo_gateway.conf ( timeout.get  and  timeout.ls , which are 30 seconds by default). This way, gateway will give its own (503) reply to client when it can't process request due to high load and LB will generate \"504 Gateway Timeout\" only in special cases like network problem between LB and backend. Since timeouts in leo_gateway.conf are set for internal operation between LeoGateway and LeoStorage, the overall delay between request to gateway and reply might be bigger than that, so timeout in LB configuration should be set appropriately. Also, when requesting a large object, gateway can insert delays between chunks of data up to  timeout.get  value. Some LBs need to be tweaked to handle these delays without closing the connection (in case of HAProxy, it's  timeout tunnel  option), though generally it shouldn't be a problem.  Example:  1\n2\n3\n4\n5\n6\n7\n8\n9 # timeouts in leo_gateway.conf set to 30 seconds\ntimeout.get = 30000\ntimeout.ls = 30000\n\n# timeout for headers in haproxy.cfg set to higher value - 35 seconds\n# mid-stream inactivity timeout (both directions) set to 10 minutes\nbackend\n    timeout server 35000\n    timeout tunnel 10m   HTTP connection reuse : many LBs offer a feature of reusing the same connections to backends to handle requests from various clients. Generally this mode isn't default because it doesn't work with all types of backends. However, it works with LeoGateway since it operates in stateless (REST) mode (this includes operating in S3 protocol mode which follows REST as well). Under some conditions, like requests from very high amount of clients, this option can significantly decrease latency and reduce load on LB and gateway, since it allows to use much smaller amount of connections between LB and backend compared to amount of connections between LB and clients, and save time / CPU on re-establishing TCP connections to backend. In HAProxy, this is controlled by  http-reuse  directive, e.g.  http-reuse always .  Long-lived keep-alive tunnels : alternative to the above, LBs can use one-on-one connection mappings between clients and backends. Each connection from client creates new connection to backend, which is closed when client closes the connection, so they behave like tunnels from client to backend. Under some conditions (clients use HTTP keep-alive, low amount of clients, long-lived connections with high rate of requests) this might be a better option compared to connection reuse. Modern versions of HAProxy use this mode by default, but older versions (1.5 or earlier) and other LBs might behave differently. With this style of connections, number of connections from clients that LB can handle is limited by maximum allowed amount of connections to all active backends. This mode shouldn't be confused with keep-alive connections only between clients and LB.  Activating either of these features (reusing connection to backends or tunnels with keep-alive to both client and backend) can significantly reduce load and increase performance compared to establishing connection between LB and backend for every request. However, the downside is that memory requirements for LB are increased and some extra tweaking might be needed. The most important tweak is keep-alive timeout between LB and backend. LeoGateway has timeout parameters ( http.timeout_for_header  and  http.timeout_for_body ), first of which is set to 5s by default. It means that keep-alive connection without any traffic in it for 5 seconds will be closed by LeoGateway. It's a good idea to increase this parameter when operating in the above modes. Regardless whether it's increased or not, the similar parameter on LB side should be set to  lower  value than timeout on LeoGateway. In HAProxy, this is controlled by  timeout http-keep-alive  in the backend section. This is especially important when HTTP connections are reused, because it avoids the problem when LB tries to send data into existing connection at the same time as LeoGateway closes that connection. Ensuring that LeoGateway keeps the inactive connection open a bit longer than the time after which LB will close connection from its side solves this problem.  Example:  1\n2\n3\n4\n5\n6 # keep-alive timeout between requests in leo_gateway.conf set to 30 seconds\nhttp.timeout_for_header = 30000\n\n# keep-alive timeout in haproxy.cfg set to smaller value - 29 seconds\nbackend\n    timeout http-keep-alive 29000   SSL/TLS termination : obviously, for LB to work with LeoGateway in HTTP protocol mode, SSL has to be terminated on LB itself. Usually (e.g. in case of HAProxy in multi-process mode) it will provide much higher SSL performance compared to serving SSL from LeoGateway directly, so LeoGateway should always be configured to use HTTP when operating behind LB. In cases where secure traffic between LB and LeoGateway is a must, either LB should be configured to decrypt/re-encrypt SSL traffic (so it can still operate in HTTP and not in TCP tunneling mode), or some other layer of encryption, like VPN should be used.  High availability : a single load balancer becomes a point of failure (PoF) itself. For highly available configurations, at least two LBs are needed, configured for failover (e.g. using floating IP with Corosync / Pacemaker 2 ). Alternatively, IP-level LB (e.g. Linux Virtual Server 3 ) can be set up in front of HTTP-level LBs. IP-level balancing solutions provide higher availability compared to failover and can distribute load between multiple HTTP-level LBs, but require dedicated hardware.", 
            "title": "Things to check for production systems"
        }, 
        {
            "location": "/admin/system_admin/load_balancing/#related-links", 
            "text": "For Administrators / Settings / LeoGateway Settings       HAProxy version 1.6 - Configuration Manual    High Availability Cluster Stack    The Linux Virtual Server Project", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/settings/cluster/", 
            "text": "Cluster Settings\n\n\nThis document outlines the various configuration items to keep in mind when planning a LeoFS system's cluster, and this documentation leads you to be able to configure its cluster when planning and launching it correctly.\n\n\nPrior Knowledge\n\n\nLeoFS adopts \neventual consistency\n of the consistency model; it takes priority over AP \n(Availability and Partition tolerance)\n over C \n(consistency)\n which depends on \nCAP theorem\n.\n\n\nTo keep the consistency of objects eventually, LeoFS delivers the replication and recovery feature to automatically fix consistency of objects. You can configure the consistency level of a LeoFS system, and it is affected by the configuration.\n\n\nHow to Keep RING's Consistency\n\n\nCase 1: Both LeoManager nodes are unavailable\n\n\nIf both LeoManager nodes are unavailable, LeoStorage and LeoGateway nodes don't update the RING to keep its consistency into the LeoFS system.\n\n\nCase 2: One LeoManager node is unavailable\n\n\nIf a LeoManager node is unavailable, LeoFS can update the RING, and synchronize it with the LeoFS' system eventually. After restarting another LeoManager node, LeoManager automatically synchronizes the RING between the manager nodes.\n\n\nConsistency Level\n\n\nConfigure the consistency level of a LeoFS system at \nLeoManager's configuration file - leo_manager_0.conf\n. You need to carefully configure the consistency level because it is not able to change some items after starting the system.\n\n\nThere are four configuration items at \nleo_manager_0.conf\n, items of which have a great impact on \ndata availability\n and \nstorage performance\n.\n\n\n\n\n\n\n\n\nItem\n\n\nAbbr\n\n\nModifiable\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nconsistency.num_of_replicas\n\n\nn\n\n\nNo\n\n\n1\n\n\nA number of replicas\n\n\n\n\n\n\nconsistency.write\n\n\nw\n\n\nYes\n\n\n1\n\n\nA number of replicas needed for a successful WRITE operation\n\n\n\n\n\n\nconsistency.read\n\n\nr\n\n\nYes\n\n\n1\n\n\nA number of replicas needed for a successful READ operation\n\n\n\n\n\n\nconsistency.delete\n\n\nd\n\n\nYes\n\n\n1\n\n\nA number of replicas needed for a successful DELETE operation\n\n\n\n\n\n\nconsistency.rack_aware_replicas\n\n\n\n\nNo\n\n\n0\n\n\nA number of rack-aware replicas\n\n\n\n\n\n\n\n\nData Availability of Consistency Level\n\n\nThis document delivers the relationship of \ndata availability\n and \nconfiguration level\n as below:\n\n\n\n\n\n\n\n\nData Availability\n\n\nConfiguration Level\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nExtremely Low\n\n\nn=2, r=1\nw=1, d=1\n\n\nData can not be acquired even if two nodes goes down \n(for personal use)\n\n\n\n\n\n\nLow\n\n\nn=3, r=1\nw=1, d=1\n\n\nLow data consistency\n\n\n\n\n\n\nMiddle(1)\n\n\nn=3, r=1\nw=2, d=2\n\n\nTypical settings\n\n\n\n\n\n\nMiddle(2)\n\n\nn=3, r=2\nw=2, d=2\n\n\nHigh data consistency than \nMiddle(1)\n\n\n\n\n\n\nHigh\n\n\nn=3, r=2\nw=3, d=3\n\n\nData can not be input and removed even if one node goes down\n\n\n\n\n\n\nExtremely High\n\n\nn=3, r=3\nw=3, d=3\n\n\nData can not be acquired even if one node goes down \n(can not be recommended)\n\n\n\n\n\n\n\n\nHow To Change Consistency Level\n\n\nYou can change \nconsistency.write\n, \nconsistency.read\n and \nconsistency.delete\n of the consistency level that you use the \nleofs-adm update-consistency-level\n command, but you cannot update \nnum_of_replicas\n and \nrack_aware_replicas\n.\n\n\n1\n2\n## Changes the consistency level to [w:2, d:2, r:1]\n\n$ leofs-adm update-consistency-level \n2\n \n2\n \n1\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoManager Settings", 
            "title": "Cluster Settings"
        }, 
        {
            "location": "/admin/settings/cluster/#cluster-settings", 
            "text": "This document outlines the various configuration items to keep in mind when planning a LeoFS system's cluster, and this documentation leads you to be able to configure its cluster when planning and launching it correctly.", 
            "title": "Cluster Settings"
        }, 
        {
            "location": "/admin/settings/cluster/#prior-knowledge", 
            "text": "LeoFS adopts  eventual consistency  of the consistency model; it takes priority over AP  (Availability and Partition tolerance)  over C  (consistency)  which depends on  CAP theorem .  To keep the consistency of objects eventually, LeoFS delivers the replication and recovery feature to automatically fix consistency of objects. You can configure the consistency level of a LeoFS system, and it is affected by the configuration.", 
            "title": "Prior Knowledge"
        }, 
        {
            "location": "/admin/settings/cluster/#how-to-keep-rings-consistency", 
            "text": "", 
            "title": "How to Keep RING's Consistency"
        }, 
        {
            "location": "/admin/settings/cluster/#case-1-both-leomanager-nodes-are-unavailable", 
            "text": "If both LeoManager nodes are unavailable, LeoStorage and LeoGateway nodes don't update the RING to keep its consistency into the LeoFS system.", 
            "title": "Case 1: Both LeoManager nodes are unavailable"
        }, 
        {
            "location": "/admin/settings/cluster/#case-2-one-leomanager-node-is-unavailable", 
            "text": "If a LeoManager node is unavailable, LeoFS can update the RING, and synchronize it with the LeoFS' system eventually. After restarting another LeoManager node, LeoManager automatically synchronizes the RING between the manager nodes.", 
            "title": "Case 2: One LeoManager node is unavailable"
        }, 
        {
            "location": "/admin/settings/cluster/#consistency-level", 
            "text": "Configure the consistency level of a LeoFS system at  LeoManager's configuration file - leo_manager_0.conf . You need to carefully configure the consistency level because it is not able to change some items after starting the system.  There are four configuration items at  leo_manager_0.conf , items of which have a great impact on  data availability  and  storage performance .     Item  Abbr  Modifiable  Default  Description      consistency.num_of_replicas  n  No  1  A number of replicas    consistency.write  w  Yes  1  A number of replicas needed for a successful WRITE operation    consistency.read  r  Yes  1  A number of replicas needed for a successful READ operation    consistency.delete  d  Yes  1  A number of replicas needed for a successful DELETE operation    consistency.rack_aware_replicas   No  0  A number of rack-aware replicas", 
            "title": "Consistency Level"
        }, 
        {
            "location": "/admin/settings/cluster/#data-availability-of-consistency-level", 
            "text": "This document delivers the relationship of  data availability  and  configuration level  as below:     Data Availability  Configuration Level  Description      Extremely Low  n=2, r=1 w=1, d=1  Data can not be acquired even if two nodes goes down  (for personal use)    Low  n=3, r=1 w=1, d=1  Low data consistency    Middle(1)  n=3, r=1 w=2, d=2  Typical settings    Middle(2)  n=3, r=2 w=2, d=2  High data consistency than  Middle(1)    High  n=3, r=2 w=3, d=3  Data can not be input and removed even if one node goes down    Extremely High  n=3, r=3 w=3, d=3  Data can not be acquired even if one node goes down  (can not be recommended)", 
            "title": "Data Availability of Consistency Level"
        }, 
        {
            "location": "/admin/settings/cluster/#how-to-change-consistency-level", 
            "text": "You can change  consistency.write ,  consistency.read  and  consistency.delete  of the consistency level that you use the  leofs-adm update-consistency-level  command, but you cannot update  num_of_replicas  and  rack_aware_replicas .  1\n2 ## Changes the consistency level to [w:2, d:2, r:1] \n$ leofs-adm update-consistency-level  2   2   1", 
            "title": "How To Change Consistency Level"
        }, 
        {
            "location": "/admin/settings/cluster/#related-links", 
            "text": "For Administrators / Settings / LeoManager Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/settings/environment_config/", 
            "text": "Environment Config Files\n\n\nOverview\n\n\nStarting from v1.3.3, some environment variables used by launch scripts can be redefined in\n\n\"environment config files\"\n. They have shell syntax and are read by launch scripts.\n\n\n[Environment Files]\n\n\n1\n2\n3\n4\nleo_manager_0/etc/leo_manager.environment\nleo_manager_1/etc/leo_manager.environment\nleo_gateway/etc/leo_gateway.environment\nleo_storage/etc/leo_storage.environment\n\n\n\n\n\n\nChanging settings in these files is completely optional, but can be used to better organize directories used by LeoFS nodes and simplify upgrades. Here is highly customized example of \n.environment\n and \n.config\n files that allow LeoFS to store all work information and logs outside of default installation tree (\n/usr/local/leofs/\nversion\n).\n\n\nAs a result, upgrade process to newer version becomes as simple as placing \nleo_*.environment\n files in \netc\n directory of new version, for example (for \nleo_manager_0\n):\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## Stop the process of LeoManager\n\n$ /usr/local/leofs/\nold_version\n/leo_manager_0/bin/leo_manager stop\n\n\n## Overwrite the environment file\n\n$ cp /usr/local/leofs/\nold_version\n/leo_manager_0/etc/leo_manager.environment \n\\\n\n     /usr/local/leofs/\nnew_version\n/leo_manager_0/etc/\n\n\n## Restart the process of LeoManager\n\n$ /usr/local/leofs/\nnew_version\n/leo_manager_0/bin/leo_manager start\n\n\n\n\n\n\nWith this, users can place actual config files (like \nleo_manager.conf\n) to the directory of their choice and change them independently of version upgrades, and the \n.environment\n files that need to be placed into installation tree don't need to be changed between versions. With the correct setup, since no work/temporary files will be kept in the installation tree, old version can be removed cleanly.\n\n\n\n\nNote: When managing nodes through systemd\n\n\nPlease do not use example commands above (\nleo_manager stop/start\n) when running nodes as systemd services for LeoFS v1.3.8 or later, use \nsystemctl stop/start leofs-manager-master\n instead.\n\n\n\n\nExample Configuration\n\n\nContents of \n/usr/local/leofs/\nversion\n/leo_manager_0/etc/leo_manager.environment\n:\n\n\n1\n2\n3\n4\n# pick config file from fixed place\n\n\nRUNNER_ETC_DIR\n=\n/etc/leofs/leo_manager_0\n\n\n# store erlang.log.* and run_erl.log in this directory\n\n\nRUNNER_LOG_DIR\n=\n/var/log/leofs/leo_manager_0\n\n\n\n\n\n\n\nDirectories defined in \nRUNNER_ETC_DIR\n and \nRUNNER_LOG_DIR\n \n(in this example, \n/etc/leofs/leo_manager_0\n and \n/var/log/leofs/leo_manager_0\n)\n must be writable by \nleofs\n user, also \n$RUNNER_LOG_DIR/sasl\n \n(here \n/var/log/leofs/leo_manager_0/sasl\n)\n must exist:\n\n\n1\n2\n3\ndrwxr-xr-x. 2 leofs leofs 4096 Apr  4 20:40 /etc/leofs/leo_manager_0/\ndrwxr-xr-x. 4 leofs leofs 4096 Apr  5 20:00 /var/log/leofs/leo_manager_0/\ndrwxr-xr-x. 2 leofs leofs 4096 Apr  4 20:40 /var/log/leofs/leo_manager_0/sasl/\n\n\n\n\n\n\nPaths containing whitespace characters (spaces, tabs, etc) aren't allowed in environment files due to technical reasons.\n\n\nIn \nleo_manager.conf\n, all options related to directories should point to external paths:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nsasl.sasl_error_log\n \n=\n \n/var/log/leofs/leo_manager_0/sasl/sasl-error.log\n\n\nsasl.error_logger_mf_dir\n \n=\n \n/var/log/leofs/leo_manager_0/sasl\n\n\nmnesia.dir\n \n=\n \n/var/local/leofs/leo_manager_0/work/mnesia/127.0.0.1\n\n\nqueue_dir\n \n=\n \n/var/local/leofs/leo_manager_0/work/queue\n\n\nlog.erlang\n \n=\n \n/var/log/leofs/leo_manager_0/erlang\n\n\nlog.app\n \n=\n \n/var/log/leofs/leo_manager_0/app\n\n\nlog.member_dir\n \n=\n \n/var/log/leofs/leo_manager_0/ring\n\n\nlog.ring_dir\n \n=\n \n/var/log/leofs/leo_manager_0/ring\n\n\nerlang.crash_dump\n \n=\n \n/var/log/leofs/leo_manager_0/erl_crash.dump\n\n\n\n\n\n\n\nFor \nleo_storage.conf\n it will be:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nsasl.sasl_error_log\n \n=\n \n/var/log/leofs/leo_storage/sasl/sasl-error.log\n\n\nsasl.error_logger_mf_dir\n \n=\n \n/var/log/leofs/leo_storage/sasl\n\n\nobj_containers.path\n \n=\n \n[/mnt/avs]\n\n\nlog.erlang\n \n=\n \n/var/log/leofs/leo_storage/erlang\n\n\nlog.app\n \n=\n \n/var/log/leofs/leo_storage/app\n\n\nlog.member_dir\n \n=\n \n/var/log/leofs/leo_storage/ring\n\n\nlog.ring_dir\n \n=\n \n/var/log/leofs/leo_storage/ring\n\n\nqueue_dir\n  \n=\n \n/var/local/leofs/leo_storage/work/queue\n\n\nleo_ordning_reda.temp_stacked_dir\n \n=\n \n/var/local/leofs/leo_storage/work/ord_reda/\n\n\nerlang.crash_dump\n \n=\n \n/var/log/leofs/leo_storage/erl_crash.dump\n\n\n\n\n\n\n\nFor \nleo_gateway.conf\n:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nsasl.sasl_error_log\n \n=\n \n/var/log/leofs/leo_gateway/sasl/sasl-error.log\n\n\nsasl.error_logger_mf_dir\n \n=\n \n/var/log/leofs/leo_gateway/sasl\n\n\nlog.erlang\n \n=\n \n/var/log/leofs/leo_gateway/erlang\n\n\nlog.app\n \n=\n \n/var/log/leofs/leo_gateway/app\n\n\nlog.member_dir\n \n=\n \n/var/log/leofs/leo_gateway/ring\n\n\nlog.ring_dir\n \n=\n \n/var/log/leofs/leo_gateway/ring\n\n\ncache.cache_disc_dir_data\n \n=\n \n/var/local/leofs/leo_gateway/cache/data\n\n\ncache.cache_disc_dir_journal\n \n=\n \n/var/local/leofs/leo_gateway/cache/journal\n\n\nqueue_dir\n \n=\n \n/var/local/leofs/leo_gateway/work/queue\n\n\nerlang.crash_dump\n \n=\n \n/var/log/leofs/leo_gateway/erl_crash.dump\n\n\n\n\n\n\n\nAll these directories must exist and have correct \nownership/permissions\n \n(writable by \nleofs\n user, unless set up otherwise)\n\n\nAdditional settings - SNMP config\n\n\nWhen pursuing \"pure\" system which keeps all the data out of installation tree, one might also decide to move SNMP agent config and \nSNMP db directories\n to external paths, by setting it in \nleo_manager.config\n:\n\n\n1\n2\n## leo_manager_0.conf\n\n\nsnmp_conf\n \n=\n \n/etc/leofs/leo_manager_0/leo_manager_snmp\n\n\n\n\n\n\n\nthen copying \n/usr/local/leofs/\nversion\n/leo_manager_0/snmp/snmpa_manager_0/leo_manager_snmp.config\n to \n/etc/leofs/leo_manager_0/leo_manager_snmp\n and setting\n\n\n1\n{\ndb_dir\n,\n \n/var/local/leofs/leo_manager_0/snmp_db\n},\n\n\n\n\n\n\n\nin \n/etc/leofs/leo_manager_0/leo_manager_snmp.config\n to make sure that absolutely no temporary files are created in \n/usr/local/leofs\n tree. It shouldn't matter otherwise since there is no need to keep contents of \nSNMP db directory\n between upgrades.\n\n\n(Here, copy of leo_manager_snmp.config was made so that original config would be untouched; while it is possible to change \ndb_dir\n in original \n/usr/local/leofs/\nversion\n/leo_manager_0/snmp/snmpa_manager_0/leo_manager_snmp.config\n as well, doing so would mean that this file needs to be replaced after each upgrade, reducing benefit of only changing \n.environment\n file after upgrade)\n\n\nNotice\n\n\nNote that this configuration is just an example of how to use \n.environment\n config features to move all the log files and config files out of the tree so they reside at fixed paths, to simplify configuration changes and upgrades as much as possible.\n\n\nThe resulting upgrade process can be less safe than original one suggested at \nFor Administrators / System Administration / System Migration\n, because the new version changes working \nmnesia\n and \nqueue\n directories upon launch and going back to the older version might be not always possible.\n\n\nUsers should consider making backups of work directories (\n/var/local/leofs\n in this example) before launching the newer version of a node.\n\n\nRelated Links\n\n\n\n\nFor Administrators / System Administration / System Migration", 
            "title": "Environment Configuration"
        }, 
        {
            "location": "/admin/settings/environment_config/#environment-config-files", 
            "text": "", 
            "title": "Environment Config Files"
        }, 
        {
            "location": "/admin/settings/environment_config/#overview", 
            "text": "Starting from v1.3.3, some environment variables used by launch scripts can be redefined in \"environment config files\" . They have shell syntax and are read by launch scripts.  [Environment Files]  1\n2\n3\n4 leo_manager_0/etc/leo_manager.environment\nleo_manager_1/etc/leo_manager.environment\nleo_gateway/etc/leo_gateway.environment\nleo_storage/etc/leo_storage.environment   Changing settings in these files is completely optional, but can be used to better organize directories used by LeoFS nodes and simplify upgrades. Here is highly customized example of  .environment  and  .config  files that allow LeoFS to store all work information and logs outside of default installation tree ( /usr/local/leofs/ version ).  As a result, upgrade process to newer version becomes as simple as placing  leo_*.environment  files in  etc  directory of new version, for example (for  leo_manager_0 ):  1\n2\n3\n4\n5\n6\n7\n8\n9 ## Stop the process of LeoManager \n$ /usr/local/leofs/ old_version /leo_manager_0/bin/leo_manager stop ## Overwrite the environment file \n$ cp /usr/local/leofs/ old_version /leo_manager_0/etc/leo_manager.environment  \\ \n     /usr/local/leofs/ new_version /leo_manager_0/etc/ ## Restart the process of LeoManager \n$ /usr/local/leofs/ new_version /leo_manager_0/bin/leo_manager start   With this, users can place actual config files (like  leo_manager.conf ) to the directory of their choice and change them independently of version upgrades, and the  .environment  files that need to be placed into installation tree don't need to be changed between versions. With the correct setup, since no work/temporary files will be kept in the installation tree, old version can be removed cleanly.   Note: When managing nodes through systemd  Please do not use example commands above ( leo_manager stop/start ) when running nodes as systemd services for LeoFS v1.3.8 or later, use  systemctl stop/start leofs-manager-master  instead.", 
            "title": "Overview"
        }, 
        {
            "location": "/admin/settings/environment_config/#example-configuration", 
            "text": "Contents of  /usr/local/leofs/ version /leo_manager_0/etc/leo_manager.environment :  1\n2\n3\n4 # pick config file from fixed place  RUNNER_ETC_DIR = /etc/leofs/leo_manager_0  # store erlang.log.* and run_erl.log in this directory  RUNNER_LOG_DIR = /var/log/leofs/leo_manager_0    Directories defined in  RUNNER_ETC_DIR  and  RUNNER_LOG_DIR   (in this example,  /etc/leofs/leo_manager_0  and  /var/log/leofs/leo_manager_0 )  must be writable by  leofs  user, also  $RUNNER_LOG_DIR/sasl   (here  /var/log/leofs/leo_manager_0/sasl )  must exist:  1\n2\n3 drwxr-xr-x. 2 leofs leofs 4096 Apr  4 20:40 /etc/leofs/leo_manager_0/\ndrwxr-xr-x. 4 leofs leofs 4096 Apr  5 20:00 /var/log/leofs/leo_manager_0/\ndrwxr-xr-x. 2 leofs leofs 4096 Apr  4 20:40 /var/log/leofs/leo_manager_0/sasl/   Paths containing whitespace characters (spaces, tabs, etc) aren't allowed in environment files due to technical reasons.  In  leo_manager.conf , all options related to directories should point to external paths:  1\n2\n3\n4\n5\n6\n7\n8\n9 sasl.sasl_error_log   =   /var/log/leofs/leo_manager_0/sasl/sasl-error.log  sasl.error_logger_mf_dir   =   /var/log/leofs/leo_manager_0/sasl  mnesia.dir   =   /var/local/leofs/leo_manager_0/work/mnesia/127.0.0.1  queue_dir   =   /var/local/leofs/leo_manager_0/work/queue  log.erlang   =   /var/log/leofs/leo_manager_0/erlang  log.app   =   /var/log/leofs/leo_manager_0/app  log.member_dir   =   /var/log/leofs/leo_manager_0/ring  log.ring_dir   =   /var/log/leofs/leo_manager_0/ring  erlang.crash_dump   =   /var/log/leofs/leo_manager_0/erl_crash.dump    For  leo_storage.conf  it will be:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 sasl.sasl_error_log   =   /var/log/leofs/leo_storage/sasl/sasl-error.log  sasl.error_logger_mf_dir   =   /var/log/leofs/leo_storage/sasl  obj_containers.path   =   [/mnt/avs]  log.erlang   =   /var/log/leofs/leo_storage/erlang  log.app   =   /var/log/leofs/leo_storage/app  log.member_dir   =   /var/log/leofs/leo_storage/ring  log.ring_dir   =   /var/log/leofs/leo_storage/ring  queue_dir    =   /var/local/leofs/leo_storage/work/queue  leo_ordning_reda.temp_stacked_dir   =   /var/local/leofs/leo_storage/work/ord_reda/  erlang.crash_dump   =   /var/log/leofs/leo_storage/erl_crash.dump    For  leo_gateway.conf :   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 sasl.sasl_error_log   =   /var/log/leofs/leo_gateway/sasl/sasl-error.log  sasl.error_logger_mf_dir   =   /var/log/leofs/leo_gateway/sasl  log.erlang   =   /var/log/leofs/leo_gateway/erlang  log.app   =   /var/log/leofs/leo_gateway/app  log.member_dir   =   /var/log/leofs/leo_gateway/ring  log.ring_dir   =   /var/log/leofs/leo_gateway/ring  cache.cache_disc_dir_data   =   /var/local/leofs/leo_gateway/cache/data  cache.cache_disc_dir_journal   =   /var/local/leofs/leo_gateway/cache/journal  queue_dir   =   /var/local/leofs/leo_gateway/work/queue  erlang.crash_dump   =   /var/log/leofs/leo_gateway/erl_crash.dump    All these directories must exist and have correct  ownership/permissions   (writable by  leofs  user, unless set up otherwise)", 
            "title": "Example Configuration"
        }, 
        {
            "location": "/admin/settings/environment_config/#additional-settings-snmp-config", 
            "text": "When pursuing \"pure\" system which keeps all the data out of installation tree, one might also decide to move SNMP agent config and  SNMP db directories  to external paths, by setting it in  leo_manager.config :  1\n2 ## leo_manager_0.conf  snmp_conf   =   /etc/leofs/leo_manager_0/leo_manager_snmp    then copying  /usr/local/leofs/ version /leo_manager_0/snmp/snmpa_manager_0/leo_manager_snmp.config  to  /etc/leofs/leo_manager_0/leo_manager_snmp  and setting  1 { db_dir ,   /var/local/leofs/leo_manager_0/snmp_db },    in  /etc/leofs/leo_manager_0/leo_manager_snmp.config  to make sure that absolutely no temporary files are created in  /usr/local/leofs  tree. It shouldn't matter otherwise since there is no need to keep contents of  SNMP db directory  between upgrades.  (Here, copy of leo_manager_snmp.config was made so that original config would be untouched; while it is possible to change  db_dir  in original  /usr/local/leofs/ version /leo_manager_0/snmp/snmpa_manager_0/leo_manager_snmp.config  as well, doing so would mean that this file needs to be replaced after each upgrade, reducing benefit of only changing  .environment  file after upgrade)", 
            "title": "Additional settings - SNMP config"
        }, 
        {
            "location": "/admin/settings/environment_config/#notice", 
            "text": "Note that this configuration is just an example of how to use  .environment  config features to move all the log files and config files out of the tree so they reside at fixed paths, to simplify configuration changes and upgrades as much as possible.  The resulting upgrade process can be less safe than original one suggested at  For Administrators / System Administration / System Migration , because the new version changes working  mnesia  and  queue  directories upon launch and going back to the older version might be not always possible.  Users should consider making backups of work directories ( /var/local/leofs  in this example) before launching the newer version of a node.", 
            "title": "Notice"
        }, 
        {
            "location": "/admin/settings/environment_config/#related-links", 
            "text": "For Administrators / System Administration / System Migration", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/settings/leo_manager/", 
            "text": "LeoManager Settings\n\n\nPrior Knowledge\n\n\nThe current version, v1.3 of LeoManager depends on \nErlang Mnesia, A distributed telecommunications DBMS\n to manage configurations of a LeoFS system and information of all nodes. LeoManager nodes must keep running to replicate the data for preventing data loss. You need to configure both LeoManager master and the slave.\n\n\nOther Configurations\n\n\nIf you want to modify settings like where to place \nleo_manager.conf\n, what user is starting a LeoManager process and so on, refer \nFor Administrators / Settings / Environment Configuration\n for more information.\n\n\nConfiguration\n\n\nThere are some configuration differences between LeoManager-master and LeoManager-slave. LeoManager-master only has \nthe consistency level\n and \nthe multi datacenter replication\n.\n\n\nThe default setting is to launch a LeoFS system on one node, whose setting cannot replicate data because the total number of the replica is one, and data loss could happen with high probability. You need to modify the configuration suitably before launching the LeoFS system on your production or other environments.\n\n\nLeoManager Configurations\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBasic\n\n\n\n\n\n\n\n\nmanager.partner\n\n\nThe partner of manager's alias. This configuration is necessary for communicationg between \nLeoManager\ns master\n and \nLeoManager\ns slave\n. \n( Default: manager_1@127.0.0.1 )\n\n\n\n\n\n\nconsole.port.cui\n\n\nThe port number of LeoManager's console for text format\n( Default: 10010 )\n\n\n\n\n\n\nconsole.port.json\n\n\nThe port number of LeoManager's console for JSON format\n( Default: 10020 )\n\n\n\n\n\n\nconsole.acceptors.cui\n\n\nThe maximum number of acceptors of LeoManager's console for text format\n( Default: 3 )\n\n\n\n\n\n\nconsole.acceptors.json\n\n\nThe maximum number of acceptors of LeoManager's console for JSON format\n( Default:16 )\n\n\n\n\n\n\nSystem\n\n\n\n\n\n\n\n\nsystem.dc_id\n\n\nDatacenter ID\n is necessary for using the data center replication\n( Default: dc_1 )\n\n\n\n\n\n\nsystem.cluster_id\n\n\nCluster ID\n is also necessary for using the data center replication\n( Default: leofs_1 )\n\n\n\n\n\n\nConsistency Level\n\n\n\n\n\n\n\n\nconsistency.num_of_replicas\n\n\nonly LeoManager\ns master\n The total number of object copies\n( Default: 1 )\n\n\n\n\n\n\nconsistency.write\n\n\nonly LeoManager\ns master\n The total number of object copies needed for a successful WRITE operation\n( Default: 1 )\n\n\n\n\n\n\nconsistency.read\n\n\nonly LeoManager\ns master\n The total number of object copies needed for a successful READ operation\n( Default: 1 )\n\n\n\n\n\n\nconsistency.delete\n\n\nonly LeoManager\ns master\n The total number of object copies needed for a successful DELETE operation\n( Default: 1 )\n\n\n\n\n\n\nconsistency.rack_aware_replicas\n\n\nonly LeoManager\ns master\n The total number of object copies of rack-aware\n( Default: 0 )\n\n\n\n\n\n\nMulti Data Center Replication\n\n\n\n\n\n\n\n\nmdc_replication.max_targets\n\n\nonly LeoManager\ns master\n The maximum number of replication targets of clusters OR data centers\n( Default: 2 )\n\n\n\n\n\n\nmdc_replication.num_of_replicas_a_dc\n\n\nonly LeoManager\ns master\n A remote cluster of a LeoFS system which receives this cluster's objects, and then replicates them, which adhere to a replication method of each object\n( Default: 1 )\n\n\n\n\n\n\nmdc_replication.consistency.write\n\n\nonly LeoManager\ns master\n \n[since 1.3.3]\n \n A number of replicas needed for a successful WRITE-operation\n( Default: 1 )\n\n\n\n\n\n\nmdc_replication.consistency.read\n\n\nonly LeoManager\ns master\n \n[since 1.3.3]\n \n A number of replicas needed for a successful READ-operation\n( Default: 1 )\n\n\n\n\n\n\nmdc_replication.consistency.delete\n\n\nonly LeoManager\ns master\n \n[since 1.3.3]\n \n A number of replicas needed for a successful DELETE-operation\n( Default: 1 )\n\n\n\n\n\n\nRPC for Multi Datacenter Replication\n\n\n\n\n\n\n\n\nrpc.server.acceptors\n\n\nThe total number of acceptor of the RPC server\n( Default: 16 )\n\n\n\n\n\n\nrpc.server.listen_port\n\n\nThe listening port of the RPC server\n( Default: 13075 )\n\n\n\n\n\n\nrpc.server.listen_timeout\n\n\nThe listening timeout\n( Default: 5000 )\n\n\n\n\n\n\nrpc.client.connection_pool_size\n\n\nA client is able to keep connections of a remote LeoFS up to the pool size\n( Default: 16 )\n\n\n\n\n\n\nrpc.client.connection_buffer_size\n\n\nA client is able to increase connections of a remote LeoFS up to the buffer size\n( Default: 16 )\n\n\n\n\n\n\nMnesia\n\n\n\n\n\n\n\n\nmnesia.dir\n\n\nThe directory of the database file of Mnesia*(Erlang distributed DB)\n( Default: ./work/mnesia/127.0.0.1 )*\n\n\n\n\n\n\nmnesia.dump_log_write_threshold\n\n\nThe maximum number of writes allowed to the transaction log before a new dump of the log is performed. Default is 100 log writes.\n- See also: \nErlang Mnesia dump_log_write_threshold\n( Default: 50000 )\n\n\n\n\n\n\nmnesia.dc_dump_limit\n\n\nMnesia's tables are dumped when \nfilesize(Log) \n (filesize(Tab)/Dc_dump_limit)\n. Lower values reduce CPU overhead but increase disk space and startup times. Default is 4.\n- See also: \nErlang Mnesia\n( Default: 40 )\n\n\n\n\n\n\nLog\n\n\n\n\n\n\n\n\nlog.log_level\n\n\nLeoManager's logger controls outputting logs by the log level:\n1: Info\n2: Warn\n3: Error\n( Default: 1 )\n\n\n\n\n\n\nlog.erlang\n\n\nThe output destination of Erlang's logs\n( Default: ./log/erlang )\n\n\n\n\n\n\nlog.app\n\n\nThe output destination of LeoManager's logs\n( Default: ./log/app )\n\n\n\n\n\n\nlog.member_dir\n\n\nThe output destination of the member's dump file\n( Default: ./log/ring )\n\n\n\n\n\n\nlog.ring_dir\n\n\nThe output destination of the RING's dump file\n( Default: ./log/ring )\n\n\n\n\n\n\nOther Directories\n\n\n\n\n\n\n\n\nqueue_dir\n\n\nThe directory of the data file of LeoFS' MQ\n( Default: ./work/queue )\n\n\n\n\n\n\nsnmp_agent\n\n\nThe directory of the snmp agent file of LeoFS\n( Default: ./snmp/snmpa_manager_0/LEO-MANAGER )\n\n\n\n\n\n\n\n\nErlang VM's Related Configurations\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nnodename\n\n\nThe format of the node name is \nNAME\n@\nIP-ADDRESS\n, which must be unique always in a LeoFS system\n( Default: manager_0@127.0.0.1 )\n\n\n\n\n\n\ndistributed_cookie\n\n\nSets the magic cookie of the node to \nCookie\n. \n- See also: \nDistributed Erlang\n( Default: 401321b4 )\n\n\n\n\n\n\nerlang.kernel_poll\n\n\nKernel poll reduces LeoFS' CPU usage when it has hundreds (or more) network connections.\n( Default: true )\n\n\n\n\n\n\nerlang.asyc_threads\n\n\nThe total number of Erlang aynch threads\n( Default: 32 )\n\n\n\n\n\n\nerlang.max_ports\n\n\nThe max_ports sets the default value of maximum number of ports.\n- See also: \nErlang erlang:open_port/2\n( Default: 64000 )\n\n\n\n\n\n\nerlang.crash_dump\n\n\nThe output destination of an Erlang crash dump\n( Default: ./log/erl_crash.dump )\n\n\n\n\n\n\nerlang.max_ets_tables\n\n\nThe maxinum number of \nErlagn ETS\n tables\n( Default: 256000 )\n\n\n\n\n\n\nerlang.smp\n\n\n-smp\n enable and \n-smp\n start the Erlang runtime system with \nSMP\n support enabled.\n( Default: enable )\n\n\n\n\n\n\nerlang.schedulers.compaction_of_load\n\n\nEnables or disables scheduler compaction of load. If it's enabled, the Erlang VM will attempt to fully load as many scheduler threads as mush as possible.\n( Default: true )\n\n\n\n\n\n\nerlang.schedulers.utilization_balancing\n\n\nEnables or disables scheduler utilization balancing of load. By default scheduler utilization balancing is disabled and instead scheduler compaction of load is enabled, which strives for a load distribution that causes as many scheduler threads as possible to be fully loaded (that is, not run out of work).\n( Default: false )\n\n\n\n\n\n\nerlang.distribution_buffer_size\n\n\nSender-side network distribution buffer size \n(unit: KB)\n( Default: 32768 )\n\n\n\n\n\n\nerlang.fullsweep_after\n\n\nOption fullsweep_after makes it possible to specify the maximum number of generational collections before forcing a fullsweep, even if there is room on the old heap. Setting the number to zero disables the general collection algorithm, that is, all live data is copied at every garbage collection.\n( Default: 0 )\n\n\n\n\n\n\nerlang.secio\n\n\nEnables or disables eager check I/O scheduling. The flag effects when schedulers will check for I/O operations possible to execute, and when such I/O operations will execute.\n( Default: true )\n\n\n\n\n\n\nprocess_limit\n\n\nThe maxinum number of Erlang processes. Sets the maximum number of simultaneously existing processes for this system if a Number is passed as value. Valid range for Number is [1024-134217727]\n( Default: 1048576 )\n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nConcept and Architecture / LeoManager's Architecture\n\n\nFor Administrators / Settings / Cluster Settings\n\n\nFor Administrators / Settings / Environment Configuration\n\n\nFor Administrators / System Administration / System Monitoring\n\n\nFor Administrators / System Operations / Multi Data Center Replication", 
            "title": "LeoManager Settings"
        }, 
        {
            "location": "/admin/settings/leo_manager/#leomanager-settings", 
            "text": "", 
            "title": "LeoManager Settings"
        }, 
        {
            "location": "/admin/settings/leo_manager/#prior-knowledge", 
            "text": "The current version, v1.3 of LeoManager depends on  Erlang Mnesia, A distributed telecommunications DBMS  to manage configurations of a LeoFS system and information of all nodes. LeoManager nodes must keep running to replicate the data for preventing data loss. You need to configure both LeoManager master and the slave.", 
            "title": "Prior Knowledge"
        }, 
        {
            "location": "/admin/settings/leo_manager/#other-configurations", 
            "text": "If you want to modify settings like where to place  leo_manager.conf , what user is starting a LeoManager process and so on, refer  For Administrators / Settings / Environment Configuration  for more information.", 
            "title": "Other Configurations"
        }, 
        {
            "location": "/admin/settings/leo_manager/#configuration", 
            "text": "There are some configuration differences between LeoManager-master and LeoManager-slave. LeoManager-master only has  the consistency level  and  the multi datacenter replication .  The default setting is to launch a LeoFS system on one node, whose setting cannot replicate data because the total number of the replica is one, and data loss could happen with high probability. You need to modify the configuration suitably before launching the LeoFS system on your production or other environments.", 
            "title": "Configuration"
        }, 
        {
            "location": "/admin/settings/leo_manager/#leomanager-configurations", 
            "text": "Item  Description      Basic     manager.partner  The partner of manager's alias. This configuration is necessary for communicationg between  LeoManager s master  and  LeoManager s slave .  ( Default: manager_1@127.0.0.1 )    console.port.cui  The port number of LeoManager's console for text format ( Default: 10010 )    console.port.json  The port number of LeoManager's console for JSON format ( Default: 10020 )    console.acceptors.cui  The maximum number of acceptors of LeoManager's console for text format ( Default: 3 )    console.acceptors.json  The maximum number of acceptors of LeoManager's console for JSON format ( Default:16 )    System     system.dc_id  Datacenter ID  is necessary for using the data center replication ( Default: dc_1 )    system.cluster_id  Cluster ID  is also necessary for using the data center replication ( Default: leofs_1 )    Consistency Level     consistency.num_of_replicas  only LeoManager s master  The total number of object copies ( Default: 1 )    consistency.write  only LeoManager s master  The total number of object copies needed for a successful WRITE operation ( Default: 1 )    consistency.read  only LeoManager s master  The total number of object copies needed for a successful READ operation ( Default: 1 )    consistency.delete  only LeoManager s master  The total number of object copies needed for a successful DELETE operation ( Default: 1 )    consistency.rack_aware_replicas  only LeoManager s master  The total number of object copies of rack-aware ( Default: 0 )    Multi Data Center Replication     mdc_replication.max_targets  only LeoManager s master  The maximum number of replication targets of clusters OR data centers ( Default: 2 )    mdc_replication.num_of_replicas_a_dc  only LeoManager s master  A remote cluster of a LeoFS system which receives this cluster's objects, and then replicates them, which adhere to a replication method of each object ( Default: 1 )    mdc_replication.consistency.write  only LeoManager s master   [since 1.3.3]    A number of replicas needed for a successful WRITE-operation ( Default: 1 )    mdc_replication.consistency.read  only LeoManager s master   [since 1.3.3]    A number of replicas needed for a successful READ-operation ( Default: 1 )    mdc_replication.consistency.delete  only LeoManager s master   [since 1.3.3]    A number of replicas needed for a successful DELETE-operation ( Default: 1 )    RPC for Multi Datacenter Replication     rpc.server.acceptors  The total number of acceptor of the RPC server ( Default: 16 )    rpc.server.listen_port  The listening port of the RPC server ( Default: 13075 )    rpc.server.listen_timeout  The listening timeout ( Default: 5000 )    rpc.client.connection_pool_size  A client is able to keep connections of a remote LeoFS up to the pool size ( Default: 16 )    rpc.client.connection_buffer_size  A client is able to increase connections of a remote LeoFS up to the buffer size ( Default: 16 )    Mnesia     mnesia.dir  The directory of the database file of Mnesia*(Erlang distributed DB) ( Default: ./work/mnesia/127.0.0.1 )*    mnesia.dump_log_write_threshold  The maximum number of writes allowed to the transaction log before a new dump of the log is performed. Default is 100 log writes. - See also:  Erlang Mnesia dump_log_write_threshold ( Default: 50000 )    mnesia.dc_dump_limit  Mnesia's tables are dumped when  filesize(Log)   (filesize(Tab)/Dc_dump_limit) . Lower values reduce CPU overhead but increase disk space and startup times. Default is 4. - See also:  Erlang Mnesia ( Default: 40 )    Log     log.log_level  LeoManager's logger controls outputting logs by the log level: 1: Info 2: Warn 3: Error ( Default: 1 )    log.erlang  The output destination of Erlang's logs ( Default: ./log/erlang )    log.app  The output destination of LeoManager's logs ( Default: ./log/app )    log.member_dir  The output destination of the member's dump file ( Default: ./log/ring )    log.ring_dir  The output destination of the RING's dump file ( Default: ./log/ring )    Other Directories     queue_dir  The directory of the data file of LeoFS' MQ ( Default: ./work/queue )    snmp_agent  The directory of the snmp agent file of LeoFS ( Default: ./snmp/snmpa_manager_0/LEO-MANAGER )", 
            "title": "LeoManager Configurations"
        }, 
        {
            "location": "/admin/settings/leo_manager/#erlang-vms-related-configurations", 
            "text": "Item  Description      nodename  The format of the node name is  NAME @ IP-ADDRESS , which must be unique always in a LeoFS system ( Default: manager_0@127.0.0.1 )    distributed_cookie  Sets the magic cookie of the node to  Cookie .  - See also:  Distributed Erlang ( Default: 401321b4 )    erlang.kernel_poll  Kernel poll reduces LeoFS' CPU usage when it has hundreds (or more) network connections. ( Default: true )    erlang.asyc_threads  The total number of Erlang aynch threads ( Default: 32 )    erlang.max_ports  The max_ports sets the default value of maximum number of ports. - See also:  Erlang erlang:open_port/2 ( Default: 64000 )    erlang.crash_dump  The output destination of an Erlang crash dump ( Default: ./log/erl_crash.dump )    erlang.max_ets_tables  The maxinum number of  Erlagn ETS  tables ( Default: 256000 )    erlang.smp  -smp  enable and  -smp  start the Erlang runtime system with  SMP  support enabled. ( Default: enable )    erlang.schedulers.compaction_of_load  Enables or disables scheduler compaction of load. If it's enabled, the Erlang VM will attempt to fully load as many scheduler threads as mush as possible. ( Default: true )    erlang.schedulers.utilization_balancing  Enables or disables scheduler utilization balancing of load. By default scheduler utilization balancing is disabled and instead scheduler compaction of load is enabled, which strives for a load distribution that causes as many scheduler threads as possible to be fully loaded (that is, not run out of work). ( Default: false )    erlang.distribution_buffer_size  Sender-side network distribution buffer size  (unit: KB) ( Default: 32768 )    erlang.fullsweep_after  Option fullsweep_after makes it possible to specify the maximum number of generational collections before forcing a fullsweep, even if there is room on the old heap. Setting the number to zero disables the general collection algorithm, that is, all live data is copied at every garbage collection. ( Default: 0 )    erlang.secio  Enables or disables eager check I/O scheduling. The flag effects when schedulers will check for I/O operations possible to execute, and when such I/O operations will execute. ( Default: true )    process_limit  The maxinum number of Erlang processes. Sets the maximum number of simultaneously existing processes for this system if a Number is passed as value. Valid range for Number is [1024-134217727] ( Default: 1048576 )", 
            "title": "Erlang VM's Related Configurations"
        }, 
        {
            "location": "/admin/settings/leo_manager/#related-links", 
            "text": "Concept and Architecture / LeoManager's Architecture  For Administrators / Settings / Cluster Settings  For Administrators / Settings / Environment Configuration  For Administrators / System Administration / System Monitoring  For Administrators / System Operations / Multi Data Center Replication", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/settings/leo_storage/", 
            "text": "LeoStorage Settings\n\n\nPrior Knowledge\n\n\n\n\nNote: Configuration\n\n\nLeoStorage's features depend on its configuration. If once a LeoFS system is launched, you cannot modify the following LeoStorage's configurations because the algorithm of the data operation strictly adheres to the settings.\n\n\n\n\nIrrevocable and Attention Required Items:\n\n\n\n\n\n\n\n\nItem\n\n\nIrrevocable?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLeoStorage Basic\n\n\n\n\n\n\n\n\n\n\nobj_containers.path\n\n\nModifiable with condition\n\n\nAble to change the directory of the container(s) but not able to add or remove the directory(s). You need to move the data files which are \nobj_containers.path\n/avs/object\n and \nobj_containers.path\n/avs/metadata\n, which adhere to this configuration.\n\n\n\n\n\n\nobj_containers.num_of_containers\n\n\nYes\n\n\nNot able to change the configuration because LeoStorage cannot retrieve objects or metadatas.\n\n\n\n\n\n\nobj_containers.metadata_storage\n\n\nYes\n\n\nAs above\n\n\n\n\n\n\nnum_of_vnodes\n\n\nYes\n\n\nAs above\n\n\n\n\n\n\nMQ\n\n\n\n\n\n\n\n\n\n\nmq.backend_db\n\n\nModifiable with condition\n\n\nLose all the MQ's data after changing\n\n\n\n\n\n\nmq.num_of_mq_procs\n\n\nModifiable with condition\n\n\nAs above\n\n\n\n\n\n\nReplication and Recovery object(s)\n\n\n\n\n\n\n\n\n\n\nreplication.rack_awareness.rack_id\n\n\nYes\n\n\nNot able to change the configuration because LeoFS cannot retrieve objects or metadatas.\n\n\n\n\n\n\nOther Directories Settings\n\n\n\n\n\n\n\n\n\n\nqueue_dir\n\n\nModifiable with condition\n\n\nAble to change the MQ's directory but you need to move the MQ's data, which adhere to this configuration.\n\n\n\n\n\n\n\n\nOther Configurations\n\n\nIf you want to modify settings like where to place \nleo_storage.conf\n, what user is starting a LeoStorage process and so on, refer \nFor Administrators / Settings / Environment Configuration\n for more information.\n\n\nConfiguration\n\n\nLeoStorage Configurations\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLeoManager Nodes\n\n\n\n\n\n\n\n\nmanagers\n\n\nName of LeoManager nodes. This configuration is necessary for communicating with \nLeoManager\ns master\n and \nLeoManager\ns slave\n.\n( Default: [manager_0@127.0.0.1, manager_1@127.0.0.1] )\n\n\n\n\n\n\nLeoStorage Basic\n\n\n\n\n\n\n\n\nobj_containers.path\n\n\nDirectories of object-containers\n( Default: [./avs] )\n\n\n\n\n\n\nobj_containers.num_of_containers\n\n\nA number of object-containers of each directory. As \nbackend_db.eleveldb.write_buf_size\n * \nobj_containers.num_of_containers\n memory can be consumed in total, take both into account to meet with your memory footprint requirements on LeoStorage. \n( Default: [8] )\n\n\n\n\n\n\nobj_containers.sync_mode\n\n\nMode of the data synchronization. There're three modes:\nnone\n: Not synchronization every time \n(default)\nperiodic\n: Periodic synchronization which depends on \nobj_containers.sync_interval_in_ms\nwritethrough\n: Ensures that any buffers kept by the OS are written to disk every time\n( Default: none )\n\n\n\n\n\n\nobj_containers.sync_interval_in_ms\n\n\nInterval in ms of the data synchronization\n( Default: 1000, Unit: \nmsec\n )\n\n\n\n\n\n\nobj_containers.metadata_storage\n\n\nThe metadata storage feature is pluggable which depends on \nbitcask\n and \nleveldb\n.\n( Default: leveldb )\n\n\n\n\n\n\nnum_of_vnodes\n\n\nThe total number of virtual-nodes of a LeoStorage node for generating the distributed hashtable, RING\n( Default: 168 )\n\n\n\n\n\n\nobject_storage.is_strict_check\n\n\nEnable strict check between checksum of a metadata and checksum of an object.\n( Default: false )\n\n\n\n\n\n\nobject_storage.threshold_of_slow_processing\n\n\nThreshold of slow processing\n( Default: 1000, Unit: \nmsec\n )\n\n\n\n\n\n\nseeking_timeout_per_metadata\n\n\nTimeout of seeking metadatas per a metadata\n( Default: 10, Unit: \nmsec\n )\n\n\n\n\n\n\nmax_num_of_procs\n\n\nMaximum number of processes for both write and read operation\n( Default: 3000 )\n\n\n\n\n\n\nnum_of_obj_storage_read_procs\n\n\nTotal number of obj-storage-read processes per object-container, AVS\nRange: [1..100]\n( Default: 3 )\n\n\n\n\n\n\nWatchdog\n\n\n\n\n\n\n\n\nwatchdog.common.loosen_control_at_safe_count\n\n\nWhen reach a number of safe \n(clear watchdog)\n, a watchdog loosen the control\n( Default: 1 )\n\n\n\n\n\n\nWatchdog / REX\n\n\n\n\n\n\n\n\nwatchdog.rex.is_enabled\n\n\nEnables or disables the rex-watchdog which monitors the memory usage of \nErlang's RPC component\n.\n( Default: true )\n\n\n\n\n\n\nwatchdog.rex.interval\n\n\nAn interval of executing the watchdog processing\n( Default: 10, Unit: \nsec\n )\n\n\n\n\n\n\nwatchdog.rex.threshold_mem_capacity\n\n\nThreshold of memory capacity of binary for Erlang rex\n( Default: 33554432, Unit: \nbyte\n )\n\n\n\n\n\n\nWatchdog / CPU\n\n\n\n\n\n\n\n\nwatchdog.cpu.is_enabled\n\n\nEnables or disables the CPU-watchdog which monitors both \nCPU load average\n and \nCPU utilization\n( Default: false )\n\n\n\n\n\n\nwatchdog.cpu.raised_error_times\n\n\nTimes of raising error to a client\n( Default: 5 )\n\n\n\n\n\n\nwatchdog.cpu.interval\n\n\nAn interval of executing the watchdog processing\n( Default: 10, Unit: \nsec\n )\n\n\n\n\n\n\nwatchdog.cpu.threshold_cpu_load_avg\n\n\nThreshold of CPU load average\n( Default: 5.0 )\n\n\n\n\n\n\nwatchdog.cpu.threshold_cpu_util\n\n\nThreshold of CPU utilization\n( Default: 100 )\n\n\n\n\n\n\nWatchdog / DISK\n\n\n\n\n\n\n\n\nwatchdog.disk.is_enabled\n\n\nEnables or disables the \n( Default: false )\n\n\n\n\n\n\nwatchdog.disk.raised_error_times\n\n\nTimes of raising error to a client\n( Default: 5 )\n\n\n\n\n\n\nwatchdog.disk.interval\n\n\nAn interval of executing the watchdog processing\n( Default: 10, Unit: \nsec\n )\n\n\n\n\n\n\nwatchdog.disk.threshold_disk_use\n\n\nThreshold of Disk use(%) of a target disk's capacity\n( Default: 85, Unit: \npercent\n )\n\n\n\n\n\n\nwatchdog.disk.threshold_disk_util\n\n\nThreshold of Disk utilization\n( Default: 90, Unit: \npercent\n )\n\n\n\n\n\n\nwatchdog.disk.threshold_disk_rkb\n\n\nThreshold of disk read KB/sec\n( Default: 98304, Unit: \nKB\n )\n\n\n\n\n\n\nwatchdog.disk.threshold_disk_wkb\n\n\nThreshold of disk write KB/sec\n( Default: 98304, Unit: \nKB\n )\n\n\n\n\n\n\nwatchdog.disk.target_devices\n\n\nTarget devices for checking disk utilization\n( Default: [] )\n\n\n\n\n\n\nWatchdog / CLUSTER\n\n\n\n\n\n\n\n\nwatchdog.cluster.is_enabled\n\n\nEnables or disables the \n( Default: false )\n\n\n\n\n\n\nwatchdog.cluster.interval\n\n\nAn interval of executing the watchdog processing\n( Default: 10 )\n\n\n\n\n\n\nWatchdog / ERRORS\n\n\n\n\n\n\n\n\nwatchdog.error.is_enabled\n\n\nEnables or disables the \n( Default: false )\n\n\n\n\n\n\nwatchdog.error.interval\n\n\nAn interval of executing the watchdog processing\n( Default: 60 )\n\n\n\n\n\n\nwatchdog.error.threshold_count\n\n\nTotal counts of raising error to a client\n( Default: 100 )\n\n\n\n\n\n\nData Compaction\n\n\n\n\n\n\n\n\nData Compaction / Basic\n\n\n\n\n\n\n\n\ncompaction.limit_num_of_compaction_procs\n\n\nLimit of a number of procs to execute data-compaction in parallel\n( Default: 4 )\n\n\n\n\n\n\ncompaction.skip_prefetch_size\n\n\nPerfetch size when skipping garbage\n( Default: 512 )\n\n\n\n\n\n\ncompaction.waiting_time_regular\n\n\nRegular value of compaction-proc waiting time/batch-proc\n( Default: 500, Unit: \nmsec\n )\n\n\n\n\n\n\ncompaction.waiting_time_max\n\n\nMaximum value of compaction-proc waiting time/batch-proc\n( Default: 3000, Unit: \nmsec\n )\n\n\n\n\n\n\ncompaction.batch_procs_regular\n\n\nTotal number of regular compaction batch processes\n( Default: 1000 )\n\n\n\n\n\n\ncompaction.batch_procs_max\n\n\nMaximum number of compaction batch processes\n( Default: 1500 )\n\n\n\n\n\n\nData Compaction / Automated Data Compaction\n\n\n\n\n\n\n\n\nautonomic_op.compaction.is_enabled\n\n\nEnables or disables the auto-compaction\n( Default: false )\n\n\n\n\n\n\nautonomic_op.compaction.parallel_procs\n\n\nTotal number of parallel processes\n( Default: 1 )\n\n\n\n\n\n\nautonomic_op.compaction.interval\n\n\nAn interval time of between auto-comcations\n( Default: 3600, Unit: \nsec\n )\n\n\n\n\n\n\nautonomic_op.compaction.warn_active_size_ratio\n\n\nWarning ratio of active size\n( Default: 70, Unit: \npercent\n )\n\n\n\n\n\n\nautonomic_op.compaction.threshold_active_size_ratio\n\n\nThreshold ratio of active size. LeoStorage start data-comaction after reaching it\n( Default: 60, \npercent\n )\n\n\n\n\n\n\nMQ\n\n\n\n\n\n\n\n\nmq.backend_db\n\n\nThe MQ storage feature is pluggable which depends on \nbitcask\n and \nleveldb\n.\n( Default: leveldb )\n\n\n\n\n\n\nmq.num_of_mq_procs\n\n\nA number of mq-server's processes\n( Default: 8 )\n\n\n\n\n\n\nmq.num_of_batch_process_max\n\n\nMaximum number of bach processes of message\n( Default: 3000 )\n\n\n\n\n\n\nmq.num_of_batch_process_regular\n\n\nRegular value of bach processes of message\n( Default: 1600 )\n\n\n\n\n\n\nmq.interval_between_batch_procs_max\n\n\nMaximum value of interval between batch-procs\n( Default: 3000, Unit: \nmsec\n )\n\n\n\n\n\n\nmq.interval_between_batch_procs_regular\n\n\nRegular value of interval between batch-procs\n( Default: 500, Unit: \nmsec\n )\n\n\n\n\n\n\nBackend DB / eleveldb\n\n\n\n\n\n\n\n\nbackend_db.eleveldb.write_buf_size\n\n\nWrite Buffer Size. Larger values increase performance, especially during bulk loads.\nUp to two write buffers may be held in memory at the same time, so you may wish to adjust this parameter to control memory usage.Also, a larger write buffer will result in a longer recovery time the next time the database is opened. As \nbackend_db.eleveldb.write_buf_size\n * \nobj_containers.num_of_containers\n memory can be consumed in total, take both into account to meet with your memory footprint requirements on LeoStorage.\n( Default: 62914560 )\n\n\n\n\n\n\nbackend_db.eleveldb.max_open_files\n\n\nMax Open Files. Number of open files that can be used by the DB. You may need to increase this if your database has a large working set \n(budget one open file per 2MB of working set)\n.\n( Default: 1000 )\n\n\n\n\n\n\nbackend_db.eleveldb.sst_block_size\n\n\nThe size of a data block is controlled by the SST block size. The size represents a threshold, not a fixed count. Whenever a newly created block reaches this uncompressed size, leveldb considers it full and writes the block with its metadata to disk. The number of keys contained in the block depends upon the size of the values and keys.\n( Default: 4096 )\n\n\n\n\n\n\nReplication and Recovery object(s)\n\n\n\n\n\n\n\n\nreplication.rack_awareness.rack_id\n\n\nRack-Id\n for the rack-awareness replica placement feature\n\n\n\n\n\n\nreplication.recovery.size_of_stacked_objs\n\n\nSize of stacked objects. Objects are stacked to send as a bulked object to remote nodes.\n( Default: 5242880, Unit: \nbyte\n )\n\n\n\n\n\n\nreplication.recovery.stacking_timeout\n\n\nStacking timeout. A bulked object are sent to a remote node after reaching the timeout.\n( Default: 1, Unit: \nsec\n )\n\n\n\n\n\n\nMulti Data Center Replication / Basic\n\n\n\n\n\n\n\n\nmdc_replication.size_of_stacked_objs\n\n\nSize of stacked objects. Objects are stacked to send as a bulked object to a remote cluster.\n( Default: 33554432, Unit: \nbyte\n )\n\n\n\n\n\n\nmdc_replication.stacking_timeout\n\n\nStacking timeout. A bulked object are sent to a remote cluster after reaching the timeout.\n( Default: 30, Unit: \nsec\n )\n\n\n\n\n\n\nmdc_replication.req_timeout\n\n\nRequest timeout between clusters\n( Default: 30000, Unit: \nmsec\n )\n\n\n\n\n\n\nLog\n\n\n\n\n\n\n\n\nlog.log_level\n\n\nLog level:\n0:debug\n1:info\n2:warn\n3:error\n( Default: 1 )\n\n\n\n\n\n\nlog.is_enable_access_log\n\n\nEnables or disables the access-log feature\n( Default: false )\n\n\n\n\n\n\nlog.access_log_level\n\n\nAccess log's level:\n0: only regular case\n1: includes error cases\n( Default: 0 )\n\n\n\n\n\n\nlog.erlang\n\n\nDestination of log file(s) of Erlang's log\n( Default: ./log/erlang )\n\n\n\n\n\n\nlog.app\n\n\nDestination of log file(s) of LeoStorage\n( Default: ./log/app )\n\n\n\n\n\n\nlog.member_dir\n\n\nDestination of log file(s) of members of storage-cluster\n( Default: ./log/ring )\n\n\n\n\n\n\nlog.ring_dir\n\n\nDestination of log file(s) of RING\n( Default: ./log/ring )\n\n\n\n\n\n\nlog.is_enable_diagnosis_log\n\n\nDestination of data-diagnosis log(s)\n( Default: true )\n\n\n\n\n\n\nOther Directories Settings\n\n\n\n\n\n\n\n\nqueue_dir\n\n\nDirectory of queue for monitoring \"RING\"\n( Default: ./work/queue )\n\n\n\n\n\n\nsnmp_agent\n\n\nDirectory of SNMP agent configuration\n( Default: ./snmp/snmpa_storage_0/LEO-STORAGE )\n\n\n\n\n\n\n\n\nErlang VM's Related Configurations\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nnodename\n\n\nThe format of the node name is \nNAME\n@\nIP-ADDRESS\n, which must be unique always in a LeoFS system\n( Default: storage_0@127.0.0.1 )\n\n\n\n\n\n\ndistributed_cookie\n\n\nSets the magic cookie of the node to \nCookie\n. \n- See also: \nDistributed Erlang\n( Default: 401321b4 )\n\n\n\n\n\n\nerlang.kernel_poll\n\n\nKernel poll reduces LeoFS' CPU usage when it has hundreds (or more) network connections.\n( Default: true )\n\n\n\n\n\n\nerlang.asyc_threads\n\n\nThe total number of Erlang aynch threads\n( Default: 32 )\n\n\n\n\n\n\nerlang.max_ports\n\n\nThe max_ports sets the default value of maximum number of ports.\n- See also: \nErlang erlang:open_port/2\n( Default: 64000 )\n\n\n\n\n\n\nerlang.crash_dump\n\n\nThe output destination of an Erlang crash dump\n( Default: ./log/erl_crash.dump )\n\n\n\n\n\n\nerlang.max_ets_tables\n\n\nThe maxinum number of \nErlagn ETS\n tables\n( Default: 256000 )\n\n\n\n\n\n\nerlang.smp\n\n\n-smp\n enable and \n-smp\n start the Erlang runtime system with \nSMP\n support enabled.\n( Default: enable )\n\n\n\n\n\n\nerlang.schedulers.compaction_of_load\n\n\nEnables or disables scheduler compaction of load. If it's enabled, the Erlang VM will attempt to fully load as many scheduler threads as mush as possible.\n( Default: true )\n\n\n\n\n\n\nerlang.schedulers.utilization_balancing\n\n\nEnables or disables scheduler utilization balancing of load. By default scheduler utilization balancing is disabled and instead scheduler compaction of load is enabled, which strives for a load distribution that causes as many scheduler threads as possible to be fully loaded (that is, not run out of work).\n( Default: false )\n\n\n\n\n\n\nerlang.distribution_buffer_size\n\n\nSender-side network distribution buffer size \n(unit: KB)\n( Default: 32768 )\n\n\n\n\n\n\nerlang.fullsweep_after\n\n\nOption fullsweep_after makes it possible to specify the maximum number of generational collections before forcing a fullsweep, even if there is room on the old heap. Setting the number to zero disables the general collection algorithm, that is, all live data is copied at every garbage collection.\n( Default: 0 )\n\n\n\n\n\n\nerlang.secio\n\n\nEnables or disables eager check I/O scheduling. The flag effects when schedulers will check for I/O operations possible to execute, and when such I/O operations will execute.\n( Default: true )\n\n\n\n\n\n\nprocess_limit\n\n\nThe maxinum number of Erlang processes. Sets the maximum number of simultaneously existing processes for this system if a Number is passed as value. Valid range for Number is [1024-134217727]\n( Default: 1048576 )\n\n\n\n\n\n\n\n\nNotes and Tips of the Configuration\n\n\nobj_containers.path, obj_containers.num_of_containers\n\n\nYou can configure plural object containers with comma separated value of \nobj_containers.path\n and \nobj_containers.num_of_containers\n.\n\n\n1\n2\nobj_containers.path\n \n=\n \n[/var/leofs/avs/1, /var/leofs/avs/2]\n\n\nobj_containers.num_of_containers\n \n=\n \n[32, 64]\n\n\n\n\n\n\n\nobject_storage.is_strict_check\n\n\nWithout setting \nobject_storage.is_strict_check\n to true, there is a little possibility your data could be broken without any caution even if a LeoFS system is running on a filesystem like ZFS\n1\n that protect both the metadata and the data blocks through the checksum when bugs of any unexpected or unknown software got AVS files broken.\n\n\nConfiguration which can affect Load and CPU usage\n\n\nmq.num_of_mq_procs\n can affect not only the performance/load during recover/rebalance operations but the load while there is at least one node suspended/downed in the cluster. so that setting \nmq.num_of_mq_procs\n to an appropriate value based on the amount of expected traffic and hardware specs is really important. This section would give you the brief understanding on \nmq.num_of_mq_procs\n and how to choose the optimal value for your requirements.\n\n\n\n\n\n\nHow the \nmq.num_of_mq_procs\n setting affect the system operations\n\n\n\n\nHigh\n\n\nFast recover/rebalance time\n\n\nHigh CPU/Load on storage during recover/rebalance and also existing suspended/stopped nodes in the cluster\n\n\n\n\n\n\nLow\n\n\nSlow recover/rebalance time\n\n\nLow CPU/Load on storage during recover/rebalance and also existing suspended/stopped nodes in the cluster\n\n\n\n\n\n\n\n\n\n\n\n\nRecommend settings\n\n\n\n\nIf you have enough CPU resources on storage nodes then set it to a higher one as long as it doesn't affect the operations coming from LeoGateway\n\n\nIf you don't then set it to somewhat a lower one unless the recover take too much time\n\n\n\n\n\n\n\n\nFor more details, Please see Issue #987\n2\n.\n\n\nConfiguration related to MQ\n\n\nLeoStorage's MQ mechanism depends on the watchdog mechanism to reduce costs of a message consumption. The MQ dynamically updates \na number of batch processes\n and \nan interval of a message consumption\n.\n\n\nFigure: Number-of-batch-processes and interval:\n\n\n\n\nAs of Figure: Relationship of Watchdog and MQ, the watchdog can automatically adjust a value of \na number of batch processes\n between \nmq.num_of_batch_process_min\n and \nmq.num_of_batch_process_max\n, which is increased or decreased with \nmq.num_of_batch_process_step\n.\n\n\nOn the other hands, a value of an interval is adjusted between \nmq.interval_between_batch_procs_min\n and \nmq.interval_between_batch_procs_max\n, which is increased or decreased with \nmq.interval_between_batch_procs_step\n.\n\n\nWhen the each value reached the min value, the MQ changes the status to \nsuspending\n, after that the node\u2019s processing costs is changed to low, the MQ updates the status to \nrunning\n, again.\n\n\n\n\nConfiguration related to the auto-compaction\n\n\nLeoStorage's auto-compaction mechanism also depends on the watchdog mechanism to reduce costs of processing. The Auto-compaction can dynamically update \na number of batch processes\n and \nan interval of a processing of seeking an object\n. The basic design of the relationship with the watchdog is similar to the MQ.\n\n\nFigure: Number-of-batch-processes and interval\n\n\n\n\nAs of \nFigure: Relationship of the watchdog and the auto-compaction\n, the watchdog automatically adjusts the value of \na number of batch processes\n between \ncompaction.batch_procs_min\n and \ncompaction.batch_procs_max\n, which is increased or decreased with \ncompaction.batch_procs_step\n.\n\n\nOn the other hand, the value of an interval is adjusted between \ncompaction.waiting_time_min\n and \ncompaction.waiting_time_max\n, which is increased or decreased with \ncompaction.waiting_time_step\n.\n\n\nWhen the each value reached the min value, the auto-compaction changes the status to \nsuspending\n, after that the node\u2019s processing costs is changed to low, the auto-compaction updates the status to \nrunning\n, again.\n\n\nFigure: Relationship of the watchdog and the auto-compaction\n\n\n\n\nRelated Links\n\n\n\n\nConcept and Architecture / LeoStorage's Architecture\n\n\nFor Administrators / System Administration / System Monitoring\n\n\nFor Administrators / System Operations / Cluster Operations\n\n\nFor Administrators / System Operations / Data Operations\n\n\nFor Administrators / Settings / Environment Configuration\n\n\n\n\n\n\n\n\n\n\n\n\nZFS\n\n\n\n\n\n\nLeoFS' Issue #987, Measure rebalance/recover-node performance according to mq.num_of_mq_procs", 
            "title": "LeoStorage Settings"
        }, 
        {
            "location": "/admin/settings/leo_storage/#leostorage-settings", 
            "text": "", 
            "title": "LeoStorage Settings"
        }, 
        {
            "location": "/admin/settings/leo_storage/#prior-knowledge", 
            "text": "Note: Configuration  LeoStorage's features depend on its configuration. If once a LeoFS system is launched, you cannot modify the following LeoStorage's configurations because the algorithm of the data operation strictly adheres to the settings.", 
            "title": "Prior Knowledge"
        }, 
        {
            "location": "/admin/settings/leo_storage/#irrevocable-and-attention-required-items", 
            "text": "Item  Irrevocable?  Description      LeoStorage Basic      obj_containers.path  Modifiable with condition  Able to change the directory of the container(s) but not able to add or remove the directory(s). You need to move the data files which are  obj_containers.path /avs/object  and  obj_containers.path /avs/metadata , which adhere to this configuration.    obj_containers.num_of_containers  Yes  Not able to change the configuration because LeoStorage cannot retrieve objects or metadatas.    obj_containers.metadata_storage  Yes  As above    num_of_vnodes  Yes  As above    MQ      mq.backend_db  Modifiable with condition  Lose all the MQ's data after changing    mq.num_of_mq_procs  Modifiable with condition  As above    Replication and Recovery object(s)      replication.rack_awareness.rack_id  Yes  Not able to change the configuration because LeoFS cannot retrieve objects or metadatas.    Other Directories Settings      queue_dir  Modifiable with condition  Able to change the MQ's directory but you need to move the MQ's data, which adhere to this configuration.", 
            "title": "Irrevocable and Attention Required Items:"
        }, 
        {
            "location": "/admin/settings/leo_storage/#other-configurations", 
            "text": "If you want to modify settings like where to place  leo_storage.conf , what user is starting a LeoStorage process and so on, refer  For Administrators / Settings / Environment Configuration  for more information.", 
            "title": "Other Configurations"
        }, 
        {
            "location": "/admin/settings/leo_storage/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/admin/settings/leo_storage/#leostorage-configurations", 
            "text": "Item  Description      LeoManager Nodes     managers  Name of LeoManager nodes. This configuration is necessary for communicating with  LeoManager s master  and  LeoManager s slave . ( Default: [manager_0@127.0.0.1, manager_1@127.0.0.1] )    LeoStorage Basic     obj_containers.path  Directories of object-containers ( Default: [./avs] )    obj_containers.num_of_containers  A number of object-containers of each directory. As  backend_db.eleveldb.write_buf_size  *  obj_containers.num_of_containers  memory can be consumed in total, take both into account to meet with your memory footprint requirements on LeoStorage.  ( Default: [8] )    obj_containers.sync_mode  Mode of the data synchronization. There're three modes: none : Not synchronization every time  (default) periodic : Periodic synchronization which depends on  obj_containers.sync_interval_in_ms writethrough : Ensures that any buffers kept by the OS are written to disk every time ( Default: none )    obj_containers.sync_interval_in_ms  Interval in ms of the data synchronization ( Default: 1000, Unit:  msec  )    obj_containers.metadata_storage  The metadata storage feature is pluggable which depends on  bitcask  and  leveldb . ( Default: leveldb )    num_of_vnodes  The total number of virtual-nodes of a LeoStorage node for generating the distributed hashtable, RING ( Default: 168 )    object_storage.is_strict_check  Enable strict check between checksum of a metadata and checksum of an object. ( Default: false )    object_storage.threshold_of_slow_processing  Threshold of slow processing ( Default: 1000, Unit:  msec  )    seeking_timeout_per_metadata  Timeout of seeking metadatas per a metadata ( Default: 10, Unit:  msec  )    max_num_of_procs  Maximum number of processes for both write and read operation ( Default: 3000 )    num_of_obj_storage_read_procs  Total number of obj-storage-read processes per object-container, AVS Range: [1..100] ( Default: 3 )    Watchdog     watchdog.common.loosen_control_at_safe_count  When reach a number of safe  (clear watchdog) , a watchdog loosen the control ( Default: 1 )    Watchdog / REX     watchdog.rex.is_enabled  Enables or disables the rex-watchdog which monitors the memory usage of  Erlang's RPC component . ( Default: true )    watchdog.rex.interval  An interval of executing the watchdog processing ( Default: 10, Unit:  sec  )    watchdog.rex.threshold_mem_capacity  Threshold of memory capacity of binary for Erlang rex ( Default: 33554432, Unit:  byte  )    Watchdog / CPU     watchdog.cpu.is_enabled  Enables or disables the CPU-watchdog which monitors both  CPU load average  and  CPU utilization ( Default: false )    watchdog.cpu.raised_error_times  Times of raising error to a client ( Default: 5 )    watchdog.cpu.interval  An interval of executing the watchdog processing ( Default: 10, Unit:  sec  )    watchdog.cpu.threshold_cpu_load_avg  Threshold of CPU load average ( Default: 5.0 )    watchdog.cpu.threshold_cpu_util  Threshold of CPU utilization ( Default: 100 )    Watchdog / DISK     watchdog.disk.is_enabled  Enables or disables the  ( Default: false )    watchdog.disk.raised_error_times  Times of raising error to a client ( Default: 5 )    watchdog.disk.interval  An interval of executing the watchdog processing ( Default: 10, Unit:  sec  )    watchdog.disk.threshold_disk_use  Threshold of Disk use(%) of a target disk's capacity ( Default: 85, Unit:  percent  )    watchdog.disk.threshold_disk_util  Threshold of Disk utilization ( Default: 90, Unit:  percent  )    watchdog.disk.threshold_disk_rkb  Threshold of disk read KB/sec ( Default: 98304, Unit:  KB  )    watchdog.disk.threshold_disk_wkb  Threshold of disk write KB/sec ( Default: 98304, Unit:  KB  )    watchdog.disk.target_devices  Target devices for checking disk utilization ( Default: [] )    Watchdog / CLUSTER     watchdog.cluster.is_enabled  Enables or disables the  ( Default: false )    watchdog.cluster.interval  An interval of executing the watchdog processing ( Default: 10 )    Watchdog / ERRORS     watchdog.error.is_enabled  Enables or disables the  ( Default: false )    watchdog.error.interval  An interval of executing the watchdog processing ( Default: 60 )    watchdog.error.threshold_count  Total counts of raising error to a client ( Default: 100 )    Data Compaction     Data Compaction / Basic     compaction.limit_num_of_compaction_procs  Limit of a number of procs to execute data-compaction in parallel ( Default: 4 )    compaction.skip_prefetch_size  Perfetch size when skipping garbage ( Default: 512 )    compaction.waiting_time_regular  Regular value of compaction-proc waiting time/batch-proc ( Default: 500, Unit:  msec  )    compaction.waiting_time_max  Maximum value of compaction-proc waiting time/batch-proc ( Default: 3000, Unit:  msec  )    compaction.batch_procs_regular  Total number of regular compaction batch processes ( Default: 1000 )    compaction.batch_procs_max  Maximum number of compaction batch processes ( Default: 1500 )    Data Compaction / Automated Data Compaction     autonomic_op.compaction.is_enabled  Enables or disables the auto-compaction ( Default: false )    autonomic_op.compaction.parallel_procs  Total number of parallel processes ( Default: 1 )    autonomic_op.compaction.interval  An interval time of between auto-comcations ( Default: 3600, Unit:  sec  )    autonomic_op.compaction.warn_active_size_ratio  Warning ratio of active size ( Default: 70, Unit:  percent  )    autonomic_op.compaction.threshold_active_size_ratio  Threshold ratio of active size. LeoStorage start data-comaction after reaching it ( Default: 60,  percent  )    MQ     mq.backend_db  The MQ storage feature is pluggable which depends on  bitcask  and  leveldb . ( Default: leveldb )    mq.num_of_mq_procs  A number of mq-server's processes ( Default: 8 )    mq.num_of_batch_process_max  Maximum number of bach processes of message ( Default: 3000 )    mq.num_of_batch_process_regular  Regular value of bach processes of message ( Default: 1600 )    mq.interval_between_batch_procs_max  Maximum value of interval between batch-procs ( Default: 3000, Unit:  msec  )    mq.interval_between_batch_procs_regular  Regular value of interval between batch-procs ( Default: 500, Unit:  msec  )    Backend DB / eleveldb     backend_db.eleveldb.write_buf_size  Write Buffer Size. Larger values increase performance, especially during bulk loads. Up to two write buffers may be held in memory at the same time, so you may wish to adjust this parameter to control memory usage.Also, a larger write buffer will result in a longer recovery time the next time the database is opened. As  backend_db.eleveldb.write_buf_size  *  obj_containers.num_of_containers  memory can be consumed in total, take both into account to meet with your memory footprint requirements on LeoStorage. ( Default: 62914560 )    backend_db.eleveldb.max_open_files  Max Open Files. Number of open files that can be used by the DB. You may need to increase this if your database has a large working set  (budget one open file per 2MB of working set) . ( Default: 1000 )    backend_db.eleveldb.sst_block_size  The size of a data block is controlled by the SST block size. The size represents a threshold, not a fixed count. Whenever a newly created block reaches this uncompressed size, leveldb considers it full and writes the block with its metadata to disk. The number of keys contained in the block depends upon the size of the values and keys. ( Default: 4096 )    Replication and Recovery object(s)     replication.rack_awareness.rack_id  Rack-Id  for the rack-awareness replica placement feature    replication.recovery.size_of_stacked_objs  Size of stacked objects. Objects are stacked to send as a bulked object to remote nodes. ( Default: 5242880, Unit:  byte  )    replication.recovery.stacking_timeout  Stacking timeout. A bulked object are sent to a remote node after reaching the timeout. ( Default: 1, Unit:  sec  )    Multi Data Center Replication / Basic     mdc_replication.size_of_stacked_objs  Size of stacked objects. Objects are stacked to send as a bulked object to a remote cluster. ( Default: 33554432, Unit:  byte  )    mdc_replication.stacking_timeout  Stacking timeout. A bulked object are sent to a remote cluster after reaching the timeout. ( Default: 30, Unit:  sec  )    mdc_replication.req_timeout  Request timeout between clusters ( Default: 30000, Unit:  msec  )    Log     log.log_level  Log level: 0:debug 1:info 2:warn 3:error ( Default: 1 )    log.is_enable_access_log  Enables or disables the access-log feature ( Default: false )    log.access_log_level  Access log's level: 0: only regular case 1: includes error cases ( Default: 0 )    log.erlang  Destination of log file(s) of Erlang's log ( Default: ./log/erlang )    log.app  Destination of log file(s) of LeoStorage ( Default: ./log/app )    log.member_dir  Destination of log file(s) of members of storage-cluster ( Default: ./log/ring )    log.ring_dir  Destination of log file(s) of RING ( Default: ./log/ring )    log.is_enable_diagnosis_log  Destination of data-diagnosis log(s) ( Default: true )    Other Directories Settings     queue_dir  Directory of queue for monitoring \"RING\" ( Default: ./work/queue )    snmp_agent  Directory of SNMP agent configuration ( Default: ./snmp/snmpa_storage_0/LEO-STORAGE )", 
            "title": "LeoStorage Configurations"
        }, 
        {
            "location": "/admin/settings/leo_storage/#erlang-vms-related-configurations", 
            "text": "Item  Description      nodename  The format of the node name is  NAME @ IP-ADDRESS , which must be unique always in a LeoFS system ( Default: storage_0@127.0.0.1 )    distributed_cookie  Sets the magic cookie of the node to  Cookie .  - See also:  Distributed Erlang ( Default: 401321b4 )    erlang.kernel_poll  Kernel poll reduces LeoFS' CPU usage when it has hundreds (or more) network connections. ( Default: true )    erlang.asyc_threads  The total number of Erlang aynch threads ( Default: 32 )    erlang.max_ports  The max_ports sets the default value of maximum number of ports. - See also:  Erlang erlang:open_port/2 ( Default: 64000 )    erlang.crash_dump  The output destination of an Erlang crash dump ( Default: ./log/erl_crash.dump )    erlang.max_ets_tables  The maxinum number of  Erlagn ETS  tables ( Default: 256000 )    erlang.smp  -smp  enable and  -smp  start the Erlang runtime system with  SMP  support enabled. ( Default: enable )    erlang.schedulers.compaction_of_load  Enables or disables scheduler compaction of load. If it's enabled, the Erlang VM will attempt to fully load as many scheduler threads as mush as possible. ( Default: true )    erlang.schedulers.utilization_balancing  Enables or disables scheduler utilization balancing of load. By default scheduler utilization balancing is disabled and instead scheduler compaction of load is enabled, which strives for a load distribution that causes as many scheduler threads as possible to be fully loaded (that is, not run out of work). ( Default: false )    erlang.distribution_buffer_size  Sender-side network distribution buffer size  (unit: KB) ( Default: 32768 )    erlang.fullsweep_after  Option fullsweep_after makes it possible to specify the maximum number of generational collections before forcing a fullsweep, even if there is room on the old heap. Setting the number to zero disables the general collection algorithm, that is, all live data is copied at every garbage collection. ( Default: 0 )    erlang.secio  Enables or disables eager check I/O scheduling. The flag effects when schedulers will check for I/O operations possible to execute, and when such I/O operations will execute. ( Default: true )    process_limit  The maxinum number of Erlang processes. Sets the maximum number of simultaneously existing processes for this system if a Number is passed as value. Valid range for Number is [1024-134217727] ( Default: 1048576 )", 
            "title": "Erlang VM's Related Configurations"
        }, 
        {
            "location": "/admin/settings/leo_storage/#notes-and-tips-of-the-configuration", 
            "text": "", 
            "title": "Notes and Tips of the Configuration"
        }, 
        {
            "location": "/admin/settings/leo_storage/#obj95containerspath-obj95containersnum95of95containers", 
            "text": "You can configure plural object containers with comma separated value of  obj_containers.path  and  obj_containers.num_of_containers .  1\n2 obj_containers.path   =   [/var/leofs/avs/1, /var/leofs/avs/2]  obj_containers.num_of_containers   =   [32, 64]", 
            "title": "obj_containers.path, obj_containers.num_of_containers"
        }, 
        {
            "location": "/admin/settings/leo_storage/#object95storageis95strict95check", 
            "text": "Without setting  object_storage.is_strict_check  to true, there is a little possibility your data could be broken without any caution even if a LeoFS system is running on a filesystem like ZFS 1  that protect both the metadata and the data blocks through the checksum when bugs of any unexpected or unknown software got AVS files broken.", 
            "title": "object_storage.is_strict_check"
        }, 
        {
            "location": "/admin/settings/leo_storage/#configuration-which-can-affect-load-and-cpu-usage", 
            "text": "mq.num_of_mq_procs  can affect not only the performance/load during recover/rebalance operations but the load while there is at least one node suspended/downed in the cluster. so that setting  mq.num_of_mq_procs  to an appropriate value based on the amount of expected traffic and hardware specs is really important. This section would give you the brief understanding on  mq.num_of_mq_procs  and how to choose the optimal value for your requirements.    How the  mq.num_of_mq_procs  setting affect the system operations   High  Fast recover/rebalance time  High CPU/Load on storage during recover/rebalance and also existing suspended/stopped nodes in the cluster    Low  Slow recover/rebalance time  Low CPU/Load on storage during recover/rebalance and also existing suspended/stopped nodes in the cluster       Recommend settings   If you have enough CPU resources on storage nodes then set it to a higher one as long as it doesn't affect the operations coming from LeoGateway  If you don't then set it to somewhat a lower one unless the recover take too much time     For more details, Please see Issue #987 2 .", 
            "title": "Configuration which can affect Load and CPU usage"
        }, 
        {
            "location": "/admin/settings/leo_storage/#configuration-related-to-mq", 
            "text": "LeoStorage's MQ mechanism depends on the watchdog mechanism to reduce costs of a message consumption. The MQ dynamically updates  a number of batch processes  and  an interval of a message consumption .  Figure: Number-of-batch-processes and interval:   As of Figure: Relationship of Watchdog and MQ, the watchdog can automatically adjust a value of  a number of batch processes  between  mq.num_of_batch_process_min  and  mq.num_of_batch_process_max , which is increased or decreased with  mq.num_of_batch_process_step .  On the other hands, a value of an interval is adjusted between  mq.interval_between_batch_procs_min  and  mq.interval_between_batch_procs_max , which is increased or decreased with  mq.interval_between_batch_procs_step .  When the each value reached the min value, the MQ changes the status to  suspending , after that the node\u2019s processing costs is changed to low, the MQ updates the status to  running , again.", 
            "title": "Configuration related to MQ"
        }, 
        {
            "location": "/admin/settings/leo_storage/#configuration-related-to-the-auto-compaction", 
            "text": "LeoStorage's auto-compaction mechanism also depends on the watchdog mechanism to reduce costs of processing. The Auto-compaction can dynamically update  a number of batch processes  and  an interval of a processing of seeking an object . The basic design of the relationship with the watchdog is similar to the MQ.  Figure: Number-of-batch-processes and interval   As of  Figure: Relationship of the watchdog and the auto-compaction , the watchdog automatically adjusts the value of  a number of batch processes  between  compaction.batch_procs_min  and  compaction.batch_procs_max , which is increased or decreased with  compaction.batch_procs_step .  On the other hand, the value of an interval is adjusted between  compaction.waiting_time_min  and  compaction.waiting_time_max , which is increased or decreased with  compaction.waiting_time_step .  When the each value reached the min value, the auto-compaction changes the status to  suspending , after that the node\u2019s processing costs is changed to low, the auto-compaction updates the status to  running , again.  Figure: Relationship of the watchdog and the auto-compaction", 
            "title": "Configuration related to the auto-compaction"
        }, 
        {
            "location": "/admin/settings/leo_storage/#related-links", 
            "text": "Concept and Architecture / LeoStorage's Architecture  For Administrators / System Administration / System Monitoring  For Administrators / System Operations / Cluster Operations  For Administrators / System Operations / Data Operations  For Administrators / Settings / Environment Configuration       ZFS    LeoFS' Issue #987, Measure rebalance/recover-node performance according to mq.num_of_mq_procs", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/settings/leo_gateway/", 
            "text": "LeoGateway Settings\n\n\nPrior Knowledge\n\n\nLeoGateway is a multi-protocols storage proxy, which supports REST-API over HTTP, Amazon S3-API\n1\n and NFS v3\n2\n. LeoGateway provides the object cache feature to handle requests efficiently and to keep the high performance of your storage system.\n\n\nOther Configurations\n\n\nIf you want to customize settings like where to place \nleo_gateway.conf\n, what user is starting a LeoGateway process and so on, refer \nFor Administrators / Settings / Environment Configuration\n for more information.\n\n\nConfiguration\n\n\nLeoGateway Configurations\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLeoManager Nodes\n\n\n\n\n\n\n\n\nmanagers\n\n\nName of LeoManager nodes. This configuration is necessary for communicating with \nLeoManager\ns master\n and \nLeoManager\ns slave\n.\n( Default: [manager_0@127.0.0.1, manager_1@127.0.0.1] )\n\n\n\n\n\n\nLeoGateway Basic\n\n\n\n\n\n\n\n\nprotocol\n\n\nGateway Protocol - [s3/rest/embed/nfs] \n( Default: s3 )\n\n\n\n\n\n\nHTTP Related (S3/REST)\n\n\n\n\n\n\n\n\nhttp.port\n\n\nPort number the Gateway uses for HTTP connections \n( Default: 8080 )\n\n\n\n\n\n\nhttp.num_of_acceptors\n\n\nNumbers of processes listening for connections \n( Default: 128 )\n\n\n\n\n\n\nhttp.max_keepalive\n\n\nMaximum number of requests allowed in a single keep-alive session \n( Default: 4096 )\n\n\n\n\n\n\nhttp.layer_of_dirs\n\n\nMaximum number of virtual directory levels \n( Default: 12 )\n\n\n\n\n\n\nhttp.ssl_port\n\n\nPort number the Gateway uses for HTTPS connections \n( Default: 8443 )\n\n\n\n\n\n\nhttp.ssl_certfile\n\n\nSSL Certificate file \n( Default: ./etc/server_cert.pem )\n\n\n\n\n\n\nhttp.ssl_keyfile\n\n\nSSL key file \n( Default: ./etc/server_key.pem )\n\n\n\n\n\n\nhttp.headers_config_file\n\n\nHTTP custom header configuration file \n( Default: ./etc/http_custom_header.conf )\n\n\n\n\n\n\nhttp.timeout_for_header\n\n\nHTTP timeout for reading header\n( Default: 5000, Unit: \nmsec\n)\n\n\n\n\n\n\nhttp.timeout_for_body\n\n\nHTTP timeout for reading body\n( Default: 15000, Unit: \nmsec\n)\n\n\n\n\n\n\nBucket Related\n\n\n\n\n\n\n\n\nbucket_prop_sync_interval\n\n\nSynchronization Interval of Bucket Properties\n( Default: 300, Unit: \nsec\n )\n\n\n\n\n\n\nNFS-related configurations\n\n\n\n\n\n\n\n\nnfs.mountd.port\n\n\nMountd\u2019s port number \n( Default: 22050 )\n\n\n\n\n\n\nnfs.mountd.acceptors\n\n\nMountd\u2019s the number of acceptors \n( Default: 128 )\n\n\n\n\n\n\nnfs.nfsd.port\n\n\nNFSd\u2019s port number \n( Default: 2049 )\n\n\n\n\n\n\nnfs.nfsd.acceptors\n\n\nNFSd\u2019s the number of acceptors \n( Default: 128 )\n\n\n\n\n\n\nLarge object processing configuration\n\n\n\n\n\n\n\n\nlarge_object.max_chunked_objs\n\n\nMaximum number of chunked objects \n( Default: 1000 )\n\n\n\n\n\n\nlarge_object.chunked_obj_len\n\n\nLength of a chunked object. This value must be \n= \nlarge_object.reading_chunked_obj_len\n \n( Default: 5242880, Unit: \nbyte\n )\n\n\n\n\n\n\nlarge_object.threshold_of_chunk_len\n\n\nThreshold when object is chunked \n( Default: 5767168, Unit: \nbyte\n )\n\n\n\n\n\n\nlarge_object.reading_chunked_obj_len\n\n\nRead length of a chunked object. This value must be \n= \nlarge_object.chunked_obj_len\n \n( Default: 5242880, Unit: \nbyte\n )\n\n\n\n\n\n\nCache configuration\n\n\n\n\n\n\n\n\ncache.http_cache\n\n\nEnable HTTP-Cache mode, working like Varnish/Squid. Otherwise as Object Cache\n( Default: false )\n\n\n\n\n\n\ncache.cache_workers\n\n\nNumber of cache workers \n( Default: 16 )\n\n\n\n\n\n\ncache.cache_ram_capacity\n\n\nMemory Cache Capacity, divide across workers. This has to satisfy \n(8 * 1024 * 1024) * cache.cache_workers \n= cache.cache_ram_capacity\n \n( Default: 268435456, Unit: \nbyte\n )\n\n\n\n\n\n\ncache.cache_disc_capacity\n\n\nDisk Cache Capacity, divide across workers. This has to satisfy \n(8 * 1024 * 1024) * cache.cache_workers \n= cache.cache_disc_capacity\n \n( Default: 524288000, Unit: \nbyte\n )\n\n\n\n\n\n\ncache.cache_disc_threshold_len\n\n\nThreshold when object is stored in disk cache \n( Default: 1048576, Unit: \nbyte\n )\n\n\n\n\n\n\ncache.cache_disc_dir_data\n\n\nDirectory for disk cache data \n( Default: ./cache/data )\n\n\n\n\n\n\ncache.cache_disc_dir_journal\n\n\nDirectory for disk cache journal \n( Default: ./cache/journal )\n\n\n\n\n\n\nHTTP-Cache related\n\n\n\n\n\n\n\n\ncache.cache_expire\n\n\nCache expiry time \n( Default: 300, Unit: \nsec\n)\n\n\n\n\n\n\ncache.cache_max_content_len\n\n\nMaximum length of cached object \n( Default: 1048576, Unit: \nbyte\n )\n\n\n\n\n\n\ncache.cachable_content_type\n\n\nObject types to be cached\n\n\n\n\n\n\ncache.cachable_path_pattern\n\n\nPath pattern(s) to be cached (regular expression)\n\n\n\n\n\n\nWatchdog / REX\n\n\n\n\n\n\n\n\nwatchdog.rex.is_enabled\n\n\nEnables or disables the rex-watchdog which monitors the memory usage of \nErlangs RPC component\n.\n( Default: true )\n\n\n\n\n\n\nwatchdog.rex.interval\n\n\nAn interval of executing the watchdog processing\n( Default: 10, Unit: \nsec\n )\n\n\n\n\n\n\nWatchdog / CPU\n\n\n\n\n\n\n\n\nwatchdog.cpu.is_enabled\n\n\nEnables or disables the CPU-watchdog which monitors both \nCPU load average\n and \nCPU utilization\n( Default: false )\n\n\n\n\n\n\nwatchdog.cpu.raised_error_times\n\n\nTimes of raising error to a client\n( Default: 5 )\n\n\n\n\n\n\nwatchdog.cpu.interval\n\n\nAn interval of executing the watchdog processing\n( Default: 10, Unit: \nsec\n )\n\n\n\n\n\n\nwatchdog.cpu.threshold_cpu_load_avg\n\n\nThreshold of CPU load average\n( Default: 5.0 )\n\n\n\n\n\n\nwatchdog.cpu.threshold_cpu_util\n\n\nThreshold of CPU utilization\n( Default: 100 )\n\n\n\n\n\n\nWatchdog / IO (Erlang VM Internal Traffic)\n\n\n\n\n\n\n\n\nwatchdog.io.is_enabled\n\n\nEnables or disables the IO-watchdog which monitors the \nErlang VM Internal Traffic\n \n( Default: false )\n\n\n\n\n\n\nwatchdog.io.interval\n\n\nWatchdog interval \n( Default: 1, Unit: \nsec\n )\n\n\n\n\n\n\nwatchdog.io.threshold_input_per_sec\n\n\nThreshold input per second \n( Default: 134217728, Unit: \nbyte\n )\n\n\n\n\n\n\nwatchdog.io.threshold_output_per_sec\n\n\nThreshold output per second \n( Default: 134217728, Unit: \nbyte\n )\n\n\n\n\n\n\nTimeout\n\n\n\n\n\n\n\n\ntimeout.level_1\n\n\nTimeout when put object to LeoStorage \n(~65536 bytes)\n \n \n( Default: 5000, Unit: \nmsec\n )\n\n\n\n\n\n\ntimeout.level_2\n\n\nTimeout when put object to LeoStorage \n(~131071 bytes)\n \n \n( Default: 7000, Unit: \nmsec\n )\n\n\n\n\n\n\ntimeout.level_3\n\n\nTimeout when put object to LeoStorage \n(~524287 bytes)\n \n \n( Default: 10000, Unit: \nmsec\n )\n\n\n\n\n\n\ntimeout.level_4\n\n\nTimeout when put object to LeoStorage \n(~1048576 bytes)\n \n \n( Default: 20000, Unit: \nmsec\n )\n\n\n\n\n\n\ntimeout.level_5\n\n\nTimeout when put object to LeoStorage \n(1048576~ bytes)\n \n \n( Default: 30000, Unit: \nmsec\n )\n\n\n\n\n\n\ntimeout.get\n\n\nTimeout when get object from LeoStorage\n \n( Default: 30000, Unit: \nmsec\n )\n\n\n\n\n\n\ntimeout.ls\n\n\nTimeout when list object from LeoStorage\n \n( Default: 30000, Unit: \nmsec\n )\n\n\n\n\n\n\nLog\n\n\n\n\n\n\n\n\nlog.log_level\n\n\nLog level:\n0:debug\n1:info\n2:warn\n3:error\n( Default: 1 )\n\n\n\n\n\n\nlog.is_enable_access_log\n\n\nEnables or disables the access-log feature\n( Default: false )\n\n\n\n\n\n\nlog.erlang\n\n\nDestination of log file(s) of Erlang's log\n( Default: ./log/erlang )\n\n\n\n\n\n\nlog.app\n\n\nDestination of log file(s) of LeoStorage\n( Default: ./log/app )\n\n\n\n\n\n\nlog.member_di\nr\n\n\nDestination of log file(s) of members of storage-cluster\n( Default: ./log/ring )\n\n\n\n\n\n\nlog.ring_dir\n\n\nDestination of log file(s) of RING\n( Default: ./log/ring )\n\n\n\n\n\n\nOther Directories Settings\n\n\n\n\n\n\n\n\nqueue_dir\n\n\nDirectory of queue for monitoring \"RING\"\n( Default: ./work/queue )\n\n\n\n\n\n\nsnmp_agent\n\n\nDirectory of SNMP agent configuration\n( Default: ./snmp/snmpa_gateway_0/LEO-GATEWAY )\n\n\n\n\n\n\n\n\nErlang VM's Related Configurations\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nnodename\n\n\nThe format of the node name is \nNAME\n@\nIP-ADDRESS\n, which must be unique always in a LeoFS system\n( Default: storage_0@127.0.0.1 )\n\n\n\n\n\n\ndistributed_cookie\n\n\nSets the magic cookie of the node to \nCookie\n. \n- See also: \nDistributed Erlang\n( Default: 401321b4 )\n\n\n\n\n\n\nerlang.kernel_poll\n\n\nKernel poll reduces LeoFS' CPU usage when it has hundreds (or more) network connections.\n( Default: true )\n\n\n\n\n\n\nerlang.asyc_threads\n\n\nThe total number of Erlang aynch threads\n( Default: 32 )\n\n\n\n\n\n\nerlang.max_ports\n\n\nThe max_ports sets the default value of maximum number of ports.\n- See also: \nErlang erlang:open_port/2\n( Default: 64000 )\n\n\n\n\n\n\nerlang.crash_dump\n\n\nThe output destination of an Erlang crash dump\n( Default: ./log/erl_crash.dump )\n\n\n\n\n\n\nerlang.max_ets_tables\n\n\nThe maxinum number of \nErlagn ETS\n tables\n( Default: 256000 )\n\n\n\n\n\n\nerlang.smp\n\n\n-smp\n enable and \n-smp\n start the Erlang runtime system with \nSMP\n support enabled.\n( Default: enable )\n\n\n\n\n\n\nerlang.schedulers.compaction_of_load\n\n\nEnables or disables scheduler compaction of load. If it's enabled, the Erlang VM will attempt to fully load as many scheduler threads as mush as possible.\n( Default: true )\n\n\n\n\n\n\nerlang.schedulers.utilization_balancing\n\n\nEnables or disables scheduler utilization balancing of load. By default scheduler utilization balancing is disabled and instead scheduler compaction of load is enabled, which strives for a load distribution that causes as many scheduler threads as possible to be fully loaded (that is, not run out of work).\n( Default: false )\n\n\n\n\n\n\nerlang.distribution_buffer_size\n\n\nSender-side network distribution buffer size \n(unit: KB)\n( Default: 32768 )\n\n\n\n\n\n\nerlang.fullsweep_after\n\n\nOption fullsweep_after makes it possible to specify the maximum number of generational collections before forcing a fullsweep, even if there is room on the old heap. Setting the number to zero disables the general collection algorithm, that is, all live data is copied at every garbage collection.\n( Default: 0 )\n\n\n\n\n\n\nerlang.secio\n\n\nEnables or disables eager check I/O scheduling. The flag effects when schedulers will check for I/O operations possible to execute, and when such I/O operations will execute.\n( Default: true )\n\n\n\n\n\n\nprocess_limit\n\n\nThe maxinum number of Erlang processes. Sets the maximum number of simultaneously existing processes for this system if a Number is passed as value. Valid range for Number is [1024-134217727]\n( Default: 1048576 )\n\n\n\n\n\n\n\n\nNotes and Tips of the Configuration\n\n\nCache Consistency between LeoGateway and LeoStorage\n\n\nLeoGateway's cache feature does not depend on the consistency level of a cluster. There is a possibility of object inconsistency.\n\n\nLeoGateway requests a storage node to compare a cached object's hash value with its stored object's hash value. LeoGateway selects a LeoStorage's node from RING, \na distributed hash table\n by a target object name, then LeoGateway requests a LeoStorage node of the redundant node. If the requested object is inconsistent in the replicas and LeoGateway cached it, a client may get inconsistent objects.\n\n\nIf you need strong consistency on a LeoFS system, you can disable the cache setting.\n\n\n1\n2\ncache.cache_ram_capacity\n \n=\n \n0\n\n\ncache.cache_disc_capacity\n \n=\n \n0\n\n\n\n\n\n\n\nConfiguration related to Disk Cache\n\n\nA total number of directories to store cache files is equal to \ncache.cache_workers\n. A maximum size of a cacheable object per a directory has been determined by \ncache.cache_disc_capacity / cache.cache_workers\n. If the size of a requested object more than the maximum size, LeoGateway avoids storing the object into the disk cache.\n\n\nAnd also, when size of a requested object more than \ncache.cache_max_content_len\n, LeoGateway similarly refuses to store the object into the disk cache.\n\n\n\n\nConfigurations which may affect the cache behavior\n\n\nWhether or not an object is cached on LeoGateway is determined by below logics.\n\n\n\n\nLeoGateway's cache feature is only available to small objects when \ncache.cache_disc_capacity = 0\n.\n\n\nAn object which size is larger than \ncache.cache_disc_threshold_len\n never be cached if \ncache.cache_disc_capacity = 0\n.\n\n\n\n\nHow LeoGateway's cache feature works with cache related configurations is described below.\n\n\n\n\nFor small objects\n\n\nCache happens in both cases handling a PUT request in the write through way and handling a GET in the read through way\n\n\nBeing stored into memory if the size of an object \n \ncache.cache_disc_threshold_len\n\n\nBeing stored into Disk if the size of an object \n= \ncache.cache_disc_threshold_len\n\n\n\n\n\n\nFor large objects\n\n\nCache happens ONLY in case handling a GET request in the read through way\n\n\nBeing stored into Disk ONLY if disk cache is enabled (\ncache.cache_disc_capacity\n \n 0)\n\n\nThe whole object is written as a file on the host file system during processing a GET request (a chunk level cache is not implemented)\n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nConcept and Architecture / LeoGateway's Architecture\n\n\nFor Administrators / Interface / S3-API\n\n\nFor Administrators / Interface / REST-API\n\n\nFor Administrators / Interface / NFS v3\n\n\nFor Administrators / System Administration / System Monitoring\n\n\nFor Administrators / System Operations / S3-API related Operations\n\n\nFor Administrators / Settings / Environment Configuration\n\n\n\n\n\n\n\n\n\n\n\n\nAmazon S3 API\n\n\n\n\n\n\nWikipedia: Network File System", 
            "title": "LeoGateway Settings"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#leogateway-settings", 
            "text": "", 
            "title": "LeoGateway Settings"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#prior-knowledge", 
            "text": "LeoGateway is a multi-protocols storage proxy, which supports REST-API over HTTP, Amazon S3-API 1  and NFS v3 2 . LeoGateway provides the object cache feature to handle requests efficiently and to keep the high performance of your storage system.", 
            "title": "Prior Knowledge"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#other-configurations", 
            "text": "If you want to customize settings like where to place  leo_gateway.conf , what user is starting a LeoGateway process and so on, refer  For Administrators / Settings / Environment Configuration  for more information.", 
            "title": "Other Configurations"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#leogateway-configurations", 
            "text": "Item  Description      LeoManager Nodes     managers  Name of LeoManager nodes. This configuration is necessary for communicating with  LeoManager s master  and  LeoManager s slave . ( Default: [manager_0@127.0.0.1, manager_1@127.0.0.1] )    LeoGateway Basic     protocol  Gateway Protocol - [s3/rest/embed/nfs]  ( Default: s3 )    HTTP Related (S3/REST)     http.port  Port number the Gateway uses for HTTP connections  ( Default: 8080 )    http.num_of_acceptors  Numbers of processes listening for connections  ( Default: 128 )    http.max_keepalive  Maximum number of requests allowed in a single keep-alive session  ( Default: 4096 )    http.layer_of_dirs  Maximum number of virtual directory levels  ( Default: 12 )    http.ssl_port  Port number the Gateway uses for HTTPS connections  ( Default: 8443 )    http.ssl_certfile  SSL Certificate file  ( Default: ./etc/server_cert.pem )    http.ssl_keyfile  SSL key file  ( Default: ./etc/server_key.pem )    http.headers_config_file  HTTP custom header configuration file  ( Default: ./etc/http_custom_header.conf )    http.timeout_for_header  HTTP timeout for reading header ( Default: 5000, Unit:  msec )    http.timeout_for_body  HTTP timeout for reading body ( Default: 15000, Unit:  msec )    Bucket Related     bucket_prop_sync_interval  Synchronization Interval of Bucket Properties ( Default: 300, Unit:  sec  )    NFS-related configurations     nfs.mountd.port  Mountd\u2019s port number  ( Default: 22050 )    nfs.mountd.acceptors  Mountd\u2019s the number of acceptors  ( Default: 128 )    nfs.nfsd.port  NFSd\u2019s port number  ( Default: 2049 )    nfs.nfsd.acceptors  NFSd\u2019s the number of acceptors  ( Default: 128 )    Large object processing configuration     large_object.max_chunked_objs  Maximum number of chunked objects  ( Default: 1000 )    large_object.chunked_obj_len  Length of a chunked object. This value must be  =  large_object.reading_chunked_obj_len   ( Default: 5242880, Unit:  byte  )    large_object.threshold_of_chunk_len  Threshold when object is chunked  ( Default: 5767168, Unit:  byte  )    large_object.reading_chunked_obj_len  Read length of a chunked object. This value must be  =  large_object.chunked_obj_len   ( Default: 5242880, Unit:  byte  )    Cache configuration     cache.http_cache  Enable HTTP-Cache mode, working like Varnish/Squid. Otherwise as Object Cache ( Default: false )    cache.cache_workers  Number of cache workers  ( Default: 16 )    cache.cache_ram_capacity  Memory Cache Capacity, divide across workers. This has to satisfy  (8 * 1024 * 1024) * cache.cache_workers  = cache.cache_ram_capacity   ( Default: 268435456, Unit:  byte  )    cache.cache_disc_capacity  Disk Cache Capacity, divide across workers. This has to satisfy  (8 * 1024 * 1024) * cache.cache_workers  = cache.cache_disc_capacity   ( Default: 524288000, Unit:  byte  )    cache.cache_disc_threshold_len  Threshold when object is stored in disk cache  ( Default: 1048576, Unit:  byte  )    cache.cache_disc_dir_data  Directory for disk cache data  ( Default: ./cache/data )    cache.cache_disc_dir_journal  Directory for disk cache journal  ( Default: ./cache/journal )    HTTP-Cache related     cache.cache_expire  Cache expiry time  ( Default: 300, Unit:  sec )    cache.cache_max_content_len  Maximum length of cached object  ( Default: 1048576, Unit:  byte  )    cache.cachable_content_type  Object types to be cached    cache.cachable_path_pattern  Path pattern(s) to be cached (regular expression)    Watchdog / REX     watchdog.rex.is_enabled  Enables or disables the rex-watchdog which monitors the memory usage of  Erlangs RPC component . ( Default: true )    watchdog.rex.interval  An interval of executing the watchdog processing ( Default: 10, Unit:  sec  )    Watchdog / CPU     watchdog.cpu.is_enabled  Enables or disables the CPU-watchdog which monitors both  CPU load average  and  CPU utilization ( Default: false )    watchdog.cpu.raised_error_times  Times of raising error to a client ( Default: 5 )    watchdog.cpu.interval  An interval of executing the watchdog processing ( Default: 10, Unit:  sec  )    watchdog.cpu.threshold_cpu_load_avg  Threshold of CPU load average ( Default: 5.0 )    watchdog.cpu.threshold_cpu_util  Threshold of CPU utilization ( Default: 100 )    Watchdog / IO (Erlang VM Internal Traffic)     watchdog.io.is_enabled  Enables or disables the IO-watchdog which monitors the  Erlang VM Internal Traffic   ( Default: false )    watchdog.io.interval  Watchdog interval  ( Default: 1, Unit:  sec  )    watchdog.io.threshold_input_per_sec  Threshold input per second  ( Default: 134217728, Unit:  byte  )    watchdog.io.threshold_output_per_sec  Threshold output per second  ( Default: 134217728, Unit:  byte  )    Timeout     timeout.level_1  Timeout when put object to LeoStorage  (~65536 bytes)     ( Default: 5000, Unit:  msec  )    timeout.level_2  Timeout when put object to LeoStorage  (~131071 bytes)     ( Default: 7000, Unit:  msec  )    timeout.level_3  Timeout when put object to LeoStorage  (~524287 bytes)     ( Default: 10000, Unit:  msec  )    timeout.level_4  Timeout when put object to LeoStorage  (~1048576 bytes)     ( Default: 20000, Unit:  msec  )    timeout.level_5  Timeout when put object to LeoStorage  (1048576~ bytes)     ( Default: 30000, Unit:  msec  )    timeout.get  Timeout when get object from LeoStorage   ( Default: 30000, Unit:  msec  )    timeout.ls  Timeout when list object from LeoStorage   ( Default: 30000, Unit:  msec  )    Log     log.log_level  Log level: 0:debug 1:info 2:warn 3:error ( Default: 1 )    log.is_enable_access_log  Enables or disables the access-log feature ( Default: false )    log.erlang  Destination of log file(s) of Erlang's log ( Default: ./log/erlang )    log.app  Destination of log file(s) of LeoStorage ( Default: ./log/app )    log.member_di r  Destination of log file(s) of members of storage-cluster ( Default: ./log/ring )    log.ring_dir  Destination of log file(s) of RING ( Default: ./log/ring )    Other Directories Settings     queue_dir  Directory of queue for monitoring \"RING\" ( Default: ./work/queue )    snmp_agent  Directory of SNMP agent configuration ( Default: ./snmp/snmpa_gateway_0/LEO-GATEWAY )", 
            "title": "LeoGateway Configurations"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#erlang-vms-related-configurations", 
            "text": "Item  Description      nodename  The format of the node name is  NAME @ IP-ADDRESS , which must be unique always in a LeoFS system ( Default: storage_0@127.0.0.1 )    distributed_cookie  Sets the magic cookie of the node to  Cookie .  - See also:  Distributed Erlang ( Default: 401321b4 )    erlang.kernel_poll  Kernel poll reduces LeoFS' CPU usage when it has hundreds (or more) network connections. ( Default: true )    erlang.asyc_threads  The total number of Erlang aynch threads ( Default: 32 )    erlang.max_ports  The max_ports sets the default value of maximum number of ports. - See also:  Erlang erlang:open_port/2 ( Default: 64000 )    erlang.crash_dump  The output destination of an Erlang crash dump ( Default: ./log/erl_crash.dump )    erlang.max_ets_tables  The maxinum number of  Erlagn ETS  tables ( Default: 256000 )    erlang.smp  -smp  enable and  -smp  start the Erlang runtime system with  SMP  support enabled. ( Default: enable )    erlang.schedulers.compaction_of_load  Enables or disables scheduler compaction of load. If it's enabled, the Erlang VM will attempt to fully load as many scheduler threads as mush as possible. ( Default: true )    erlang.schedulers.utilization_balancing  Enables or disables scheduler utilization balancing of load. By default scheduler utilization balancing is disabled and instead scheduler compaction of load is enabled, which strives for a load distribution that causes as many scheduler threads as possible to be fully loaded (that is, not run out of work). ( Default: false )    erlang.distribution_buffer_size  Sender-side network distribution buffer size  (unit: KB) ( Default: 32768 )    erlang.fullsweep_after  Option fullsweep_after makes it possible to specify the maximum number of generational collections before forcing a fullsweep, even if there is room on the old heap. Setting the number to zero disables the general collection algorithm, that is, all live data is copied at every garbage collection. ( Default: 0 )    erlang.secio  Enables or disables eager check I/O scheduling. The flag effects when schedulers will check for I/O operations possible to execute, and when such I/O operations will execute. ( Default: true )    process_limit  The maxinum number of Erlang processes. Sets the maximum number of simultaneously existing processes for this system if a Number is passed as value. Valid range for Number is [1024-134217727] ( Default: 1048576 )", 
            "title": "Erlang VM's Related Configurations"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#notes-and-tips-of-the-configuration", 
            "text": "", 
            "title": "Notes and Tips of the Configuration"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#cache-consistency-between-leogateway-and-leostorage", 
            "text": "LeoGateway's cache feature does not depend on the consistency level of a cluster. There is a possibility of object inconsistency.  LeoGateway requests a storage node to compare a cached object's hash value with its stored object's hash value. LeoGateway selects a LeoStorage's node from RING,  a distributed hash table  by a target object name, then LeoGateway requests a LeoStorage node of the redundant node. If the requested object is inconsistent in the replicas and LeoGateway cached it, a client may get inconsistent objects.  If you need strong consistency on a LeoFS system, you can disable the cache setting.  1\n2 cache.cache_ram_capacity   =   0  cache.cache_disc_capacity   =   0", 
            "title": "Cache Consistency between LeoGateway and LeoStorage"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#configuration-related-to-disk-cache", 
            "text": "A total number of directories to store cache files is equal to  cache.cache_workers . A maximum size of a cacheable object per a directory has been determined by  cache.cache_disc_capacity / cache.cache_workers . If the size of a requested object more than the maximum size, LeoGateway avoids storing the object into the disk cache.  And also, when size of a requested object more than  cache.cache_max_content_len , LeoGateway similarly refuses to store the object into the disk cache.", 
            "title": "Configuration related to Disk Cache"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#configurations-which-may-affect-the-cache-behavior", 
            "text": "Whether or not an object is cached on LeoGateway is determined by below logics.   LeoGateway's cache feature is only available to small objects when  cache.cache_disc_capacity = 0 .  An object which size is larger than  cache.cache_disc_threshold_len  never be cached if  cache.cache_disc_capacity = 0 .   How LeoGateway's cache feature works with cache related configurations is described below.   For small objects  Cache happens in both cases handling a PUT request in the write through way and handling a GET in the read through way  Being stored into memory if the size of an object    cache.cache_disc_threshold_len  Being stored into Disk if the size of an object  =  cache.cache_disc_threshold_len    For large objects  Cache happens ONLY in case handling a GET request in the read through way  Being stored into Disk ONLY if disk cache is enabled ( cache.cache_disc_capacity    0)  The whole object is written as a file on the host file system during processing a GET request (a chunk level cache is not implemented)", 
            "title": "Configurations which may affect the cache behavior"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#related-links", 
            "text": "Concept and Architecture / LeoGateway's Architecture  For Administrators / Interface / S3-API  For Administrators / Interface / REST-API  For Administrators / Interface / NFS v3  For Administrators / System Administration / System Monitoring  For Administrators / System Operations / S3-API related Operations  For Administrators / Settings / Environment Configuration       Amazon S3 API    Wikipedia: Network File System", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/protocols/s3/", 
            "text": "Interface: S3-API\n\n\nConfiguration of LeoGateway\n\n\nUpdate LeoGateway's protocol configuration to \ns3\n in your \nLeoGateway configuration\n\n\n1\n2\n3\n4\n5\n## --------------------------------------------------------------------\n\n\n## GATEWAY Protocol\n\n\n## --------------------------------------------------------------------\n\n\n## Gateway Protocol to use: [s3 | rest | embed | nfs]\n\n\nprotocol\n \n=\n \ns3\n\n\n\n\n\n\n\nS3 API Implementation Status\n\n\nObject Operation\n\n\n\n\n\n\n\n\nNo\n\n\nAPI\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\nDELETE Object\n\n\nYes\n\n\n\n\n\n\n2\n\n\nDelete Multiple Objects\n\n\nYes\n\n\n\n\n\n\n3\n\n\nGET Object\n\n\nYes\n\n\n\n\n\n\n4\n\n\nGET Object ACL\n\n\nPlans to support with v1.5\n\n\n\n\n\n\n5\n\n\nGET Object torrent\n\n\nNo\n\n\n\n\n\n\n6\n\n\nHEAD Object\n\n\nYes\n\n\n\n\n\n\n7\n\n\nPOST Object\n\n\nYes\n\n\n\n\n\n\n8\n\n\nPUT Object\n\n\nYes\n\n\n\n\n\n\n9\n\n\nPUT Object ACL\n\n\nPlans to support with v1.5\n\n\n\n\n\n\n10\n\n\nPUT Object - Copy\n\n\nYes\n\n\n\n\n\n\n11\n\n\nInitiate Multipart Upload\n\n\nYes\n\n\n\n\n\n\n12\n\n\nUpload Part\n\n\nYes\n\n\n\n\n\n\n13\n\n\nUpload Part - Copy\n\n\nYes\n\n\n\n\n\n\n14\n\n\nComplete Multipart Upload\n\n\nYes\n\n\n\n\n\n\n15\n\n\nAbort Multipart Upload\n\n\nYes\n\n\n\n\n\n\n16\n\n\nList Parts\n\n\nNo\n\n\n\n\n\n\n\n\nBucket Operation\n\n\n\n\n\n\n\n\nNo\n\n\nAPI\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\nDELETE Bucket\n\n\nYes\n\n\n\n\n\n\n2\n\n\nDELETE Bucket life cycle\n\n\nNo\n\n\n\n\n\n\n3\n\n\nDELETE Bucket policy\n\n\nNo\n\n\n\n\n\n\n4\n\n\nDELETE Bucket website\n\n\nNo\n\n\n\n\n\n\n5\n\n\nGET Bucket (List Objects)\n\n\nYes\n\n\n\n\n\n\n6\n\n\nGET Bucket ACL\n\n\nYes\n\n\n\n\n\n\n7\n\n\nGET Bucket life cycle\n\n\nNo\n\n\n\n\n\n\n8\n\n\nGET Bucket policy\n\n\nNo\n\n\n\n\n\n\n9\n\n\nGET Bucket location\n\n\nYes\n\n\n\n\n\n\n10\n\n\nGET Bucket logging\n\n\nNo\n\n\n\n\n\n\n11\n\n\nGET Bucket notification\n\n\nNo\n\n\n\n\n\n\n12\n\n\nGET Bucket Object versions\n\n\nPlan to support with v2.0\n\n\n\n\n\n\n13\n\n\nGET Bucket requestPayment\n\n\nNo\n\n\n\n\n\n\n14\n\n\nGET Bucket versioning\n\n\nPlan to support with v2.0\n\n\n\n\n\n\n15\n\n\nGET Bucket website\n\n\nNo\n\n\n\n\n\n\n16\n\n\nHEAD Bucket\n\n\nYes\n\n\n\n\n\n\n17\n\n\nList Multipart Uploads\n\n\nNo\n\n\n\n\n\n\n18\n\n\nPUT Bucket\n\n\nYes\n\n\n\n\n\n\n19\n\n\nPUT Bucket ACL\n\n\nYes\n\n\n\n\n\n\n20\n\n\nPUT Bucket life cycle\n\n\nNo\n\n\n\n\n\n\n21\n\n\nPUT Bucket policy\n\n\nNo\n\n\n\n\n\n\n22\n\n\nPUT Bucket logging\n\n\nNo\n\n\n\n\n\n\n23\n\n\nPUT Bucket notification\n\n\nNo\n\n\n\n\n\n\n24\n\n\nPUT Bucket requestPayment\n\n\nNo\n\n\n\n\n\n\n24\n\n\nPUT Bucket versioning\n\n\nPlan to support with v2.0\n\n\n\n\n\n\n25\n\n\nPUT Bucket website\n\n\nNo\n\n\n\n\n\n\n\n\nAmazon S3 Interface\n\n\nAmazon S3 Official Documentation\n\n\n\n\nAWS Documentation / Amazon Simple Storage Service (S3) / Developer Guide / What Is Amazon S3?\n\n\n\n\nHow to determine the name of Bucket\n\n\n\n\nVirtual Hosting of Buckets\n\n\nValues used for determining the name:\n\n\nS3 uses a HTTP host header or a path segment in the HTTP request line\n\n\nHow S3 determines what to use depends on a domain name\n\n\n\n\n\n\n\n\n1. \nhttps://s3.amazonaws.com/bucket/path_to_file\n\n\nIn this case, a bucket's name is the first segment of a path.\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nRequest line\n\n\nGET /bucket/path_to_file HTTP/1.1\n\n\n\n\n\n\nHost header\n\n\nHost: s3.amazonaws.com\n\n\n\n\n\n\n\n\n2. \nhttps://www.example.com.s3.amazonaws.com/path_to_file\n\n\nIn this case, the name of the bucket is the part of the domain name before \n.s3.amazonaws.com\n.\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nRequest line\n\n\nGET /path_to_file HTTP/1.1\n\n\n\n\n\n\nHost header\n\n\nHost: \nwww.example.com.s3.amazonaws.com\n\n\n\n\n\n\n\n\n3. \nhttps://www.example.com/path_to_file\n\n\nIn this case, a bucket's name is equal to the \nFQDN\n.\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nRequest line\n\n\nGET /path_to_file HTTP/1.1\n\n\n\n\n\n\nHost header\n\n\nHost header\n\n\n\n\n\n\n\n\nThe argument of LeoFS\u2019 \nwhereis\n OR \npurge\n commands should be \"\nwww.example.com/path_to_file\n\".\n\n\nNaming Rule of a Bucket\n\n\nWe recommend as a best practice that you always use DNS-compliant bucket names regardless of the region in which you create the bucket.\n\n\n\n\nBucket names can be as long as between 3 and 255 characters.\n\n\nBucket names can contain lowercase letters, numbers, periods (.), dashes (-) and underscores (_).\n\n\nBucket names must not be formatted as an IP address (e.g., 192.168.5.4).\n\n\nBucket name cannot start and end with periods (.), dashes (-) and underscores (_).\n\n\n\n\nS3 Clients\n\n\n\n\nAWS SDKs\n\n\nDragon Disk, A cloud Storage client\n\n\nS3cmd, Command Line S3 Client and Backup for Linux and Mac\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoGateway Settings", 
            "title": "S3-API"
        }, 
        {
            "location": "/admin/protocols/s3/#interface-s3-api", 
            "text": "", 
            "title": "Interface: S3-API"
        }, 
        {
            "location": "/admin/protocols/s3/#configuration-of-leogateway", 
            "text": "Update LeoGateway's protocol configuration to  s3  in your  LeoGateway configuration  1\n2\n3\n4\n5 ## --------------------------------------------------------------------  ## GATEWAY Protocol  ## --------------------------------------------------------------------  ## Gateway Protocol to use: [s3 | rest | embed | nfs]  protocol   =   s3", 
            "title": "Configuration of LeoGateway"
        }, 
        {
            "location": "/admin/protocols/s3/#s3-api-implementation-status", 
            "text": "", 
            "title": "S3 API Implementation Status"
        }, 
        {
            "location": "/admin/protocols/s3/#object-operation", 
            "text": "No  API  Status      1  DELETE Object  Yes    2  Delete Multiple Objects  Yes    3  GET Object  Yes    4  GET Object ACL  Plans to support with v1.5    5  GET Object torrent  No    6  HEAD Object  Yes    7  POST Object  Yes    8  PUT Object  Yes    9  PUT Object ACL  Plans to support with v1.5    10  PUT Object - Copy  Yes    11  Initiate Multipart Upload  Yes    12  Upload Part  Yes    13  Upload Part - Copy  Yes    14  Complete Multipart Upload  Yes    15  Abort Multipart Upload  Yes    16  List Parts  No", 
            "title": "Object Operation"
        }, 
        {
            "location": "/admin/protocols/s3/#bucket-operation", 
            "text": "No  API  Status      1  DELETE Bucket  Yes    2  DELETE Bucket life cycle  No    3  DELETE Bucket policy  No    4  DELETE Bucket website  No    5  GET Bucket (List Objects)  Yes    6  GET Bucket ACL  Yes    7  GET Bucket life cycle  No    8  GET Bucket policy  No    9  GET Bucket location  Yes    10  GET Bucket logging  No    11  GET Bucket notification  No    12  GET Bucket Object versions  Plan to support with v2.0    13  GET Bucket requestPayment  No    14  GET Bucket versioning  Plan to support with v2.0    15  GET Bucket website  No    16  HEAD Bucket  Yes    17  List Multipart Uploads  No    18  PUT Bucket  Yes    19  PUT Bucket ACL  Yes    20  PUT Bucket life cycle  No    21  PUT Bucket policy  No    22  PUT Bucket logging  No    23  PUT Bucket notification  No    24  PUT Bucket requestPayment  No    24  PUT Bucket versioning  Plan to support with v2.0    25  PUT Bucket website  No", 
            "title": "Bucket Operation"
        }, 
        {
            "location": "/admin/protocols/s3/#amazon-s3-interface", 
            "text": "", 
            "title": "Amazon S3 Interface"
        }, 
        {
            "location": "/admin/protocols/s3/#amazon-s3-official-documentation", 
            "text": "AWS Documentation / Amazon Simple Storage Service (S3) / Developer Guide / What Is Amazon S3?", 
            "title": "Amazon S3 Official Documentation"
        }, 
        {
            "location": "/admin/protocols/s3/#how-to-determine-the-name-of-bucket", 
            "text": "Virtual Hosting of Buckets  Values used for determining the name:  S3 uses a HTTP host header or a path segment in the HTTP request line  How S3 determines what to use depends on a domain name", 
            "title": "How to determine the name of Bucket"
        }, 
        {
            "location": "/admin/protocols/s3/#1-httpss3amazonawscombucketpath_to_file", 
            "text": "In this case, a bucket's name is the first segment of a path.     Name  Value      Request line  GET /bucket/path_to_file HTTP/1.1    Host header  Host: s3.amazonaws.com", 
            "title": "1. https://s3.amazonaws.com/bucket/path_to_file"
        }, 
        {
            "location": "/admin/protocols/s3/#2-httpswwwexamplecoms3amazonawscompath_to_file", 
            "text": "In this case, the name of the bucket is the part of the domain name before  .s3.amazonaws.com .     Name  Value      Request line  GET /path_to_file HTTP/1.1    Host header  Host:  www.example.com.s3.amazonaws.com", 
            "title": "2. https://www.example.com.s3.amazonaws.com/path_to_file"
        }, 
        {
            "location": "/admin/protocols/s3/#3-httpswwwexamplecompath_to_file", 
            "text": "In this case, a bucket's name is equal to the  FQDN .     Name  Value      Request line  GET /path_to_file HTTP/1.1    Host header  Host header     The argument of LeoFS\u2019  whereis  OR  purge  commands should be \" www.example.com/path_to_file \".", 
            "title": "3. https://www.example.com/path_to_file"
        }, 
        {
            "location": "/admin/protocols/s3/#naming-rule-of-a-bucket", 
            "text": "We recommend as a best practice that you always use DNS-compliant bucket names regardless of the region in which you create the bucket.   Bucket names can be as long as between 3 and 255 characters.  Bucket names can contain lowercase letters, numbers, periods (.), dashes (-) and underscores (_).  Bucket names must not be formatted as an IP address (e.g., 192.168.5.4).  Bucket name cannot start and end with periods (.), dashes (-) and underscores (_).", 
            "title": "Naming Rule of a Bucket"
        }, 
        {
            "location": "/admin/protocols/s3/#s3-clients", 
            "text": "AWS SDKs  Dragon Disk, A cloud Storage client  S3cmd, Command Line S3 Client and Backup for Linux and Mac", 
            "title": "S3 Clients"
        }, 
        {
            "location": "/admin/protocols/s3/#related-links", 
            "text": "For Administrators / Settings / LeoGateway Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/protocols/rest/", 
            "text": "Interface: REST-API\n\n\nConfiguration\n\n\nUpdate LeoGateway's protocol configuration to \nrest\n in your \nLeoGateway configuration\n\n\n1\n2\n3\n4\n5\n## --------------------------------------------------------------------\n\n\n## GATEWAY Protocol\n\n\n## --------------------------------------------------------------------\n\n\n## Gateway Protocol to use: [s3 | rest | embed | nfs]\n\n\nprotocol\n \n=\n \nrest\n\n\n\n\n\n\n\nInterface\n\n\nDescription of LeoFS\u2019 behavior for each HTTP verb\n\n\n\n\n\n\n\n\nHTTP Verb\n\n\nLeoFS\u2019 Behavior\n\n\n\n\n\n\n\n\n\n\nPUT/POST\n\n\nInsert an object into the storage cluster\n\n\n\n\n\n\nGET\n\n\nRetrieve an object from the storage cluster\n\n\n\n\n\n\nDELETE\n\n\nRemove an object from the storage cluster\n\n\n\n\n\n\n\n\nURL format\n\n\n\n\nURL format: http[s]://\nHOST\n:8080/\nFILEPATH\n\n\nLeoFS only uses the \nFILEPATH\n which part of the URL to identify objects.\n\n\nYou can check that an object exists in a LeoFS' cluster by using \nleofs-adm whereis\n command.\n\n\n\n\n\n\n\n\n1\n$ leofs-adm whereis \nFILEPATH\n\n\n\n\n\n\n\nExamples\n\n\nPOST/PUT\n\n\n1\n2\n3\n4\n5\n$ curl -X POST -H \nContent-Type:image/jpg\n \n\\\n\n          --data-binary @test_1.jpg https://hostname:8080/_test/_image/file.png\n\n$ curl -X PUT -H \nContent-Type:image/jpg\n \n\\\n\n          --data-binary @test_2.jpg https://hostname:8080/_test/_image/file.png\n\n\n\n\n\n\nGET\n\n\n1\n$ curl -X GET https://hostname:8080/_test/_image/test_2.jpg\n\n\n\n\n\n\nDELETE\n\n\n1\n$ curl -X DELETE https://hostname:8080/_test/_image/file.png\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoGateway Settings", 
            "title": "REST-API"
        }, 
        {
            "location": "/admin/protocols/rest/#interface-rest-api", 
            "text": "", 
            "title": "Interface: REST-API"
        }, 
        {
            "location": "/admin/protocols/rest/#configuration", 
            "text": "Update LeoGateway's protocol configuration to  rest  in your  LeoGateway configuration  1\n2\n3\n4\n5 ## --------------------------------------------------------------------  ## GATEWAY Protocol  ## --------------------------------------------------------------------  ## Gateway Protocol to use: [s3 | rest | embed | nfs]  protocol   =   rest", 
            "title": "Configuration"
        }, 
        {
            "location": "/admin/protocols/rest/#interface", 
            "text": "", 
            "title": "Interface"
        }, 
        {
            "location": "/admin/protocols/rest/#description-of-leofs-behavior-for-each-http-verb", 
            "text": "HTTP Verb  LeoFS\u2019 Behavior      PUT/POST  Insert an object into the storage cluster    GET  Retrieve an object from the storage cluster    DELETE  Remove an object from the storage cluster", 
            "title": "Description of LeoFS\u2019 behavior for each HTTP verb"
        }, 
        {
            "location": "/admin/protocols/rest/#url-format", 
            "text": "URL format: http[s]:// HOST :8080/ FILEPATH  LeoFS only uses the  FILEPATH  which part of the URL to identify objects.  You can check that an object exists in a LeoFS' cluster by using  leofs-adm whereis  command.     1 $ leofs-adm whereis  FILEPATH", 
            "title": "URL format"
        }, 
        {
            "location": "/admin/protocols/rest/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/admin/protocols/rest/#postput", 
            "text": "1\n2\n3\n4\n5 $ curl -X POST -H  Content-Type:image/jpg   \\ \n          --data-binary @test_1.jpg https://hostname:8080/_test/_image/file.png\n\n$ curl -X PUT -H  Content-Type:image/jpg   \\ \n          --data-binary @test_2.jpg https://hostname:8080/_test/_image/file.png", 
            "title": "POST/PUT"
        }, 
        {
            "location": "/admin/protocols/rest/#get", 
            "text": "1 $ curl -X GET https://hostname:8080/_test/_image/test_2.jpg", 
            "title": "GET"
        }, 
        {
            "location": "/admin/protocols/rest/#delete", 
            "text": "1 $ curl -X DELETE https://hostname:8080/_test/_image/file.png", 
            "title": "DELETE"
        }, 
        {
            "location": "/admin/protocols/rest/#related-links", 
            "text": "For Administrators / Settings / LeoGateway Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/", 
            "text": "Interface: NFS v3\n\n\nPre-requirement\n\n\nWe have checked the \nNFSv3\n server feature with CentOS 6.5, 7.x and Ubuntu Server 14.04, 16.04 LTS, but we did not strictly test other platforms, FreeBSD and SmartOS yet.\n\n\nConfiguration\n\n\nUpdate LeoGateway's protocol configuration to \nnfs\n, and configure NFS related configurations in your \nLeoGateway configuration\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n## --------------------------------------------------------------------\n\n\n## GATEWAY Protocol\n\n\n## --------------------------------------------------------------------\n\n\n## Gateway Protocol to use: [s3 | rest | embed | nfs]\n\n\nprotocol\n \n=\n \nnfs\n\n\n.\n\n\n.\n\n\n.\n\n\n## --------------------------------------------------------------------\n\n\n## GATEWAY - NFS-related configurations\n\n\n## --------------------------------------------------------------------\n\n\n## Mountd\ns port number\n\n\nnfs.mountd.port\n \n=\n \n22050\n\n\n\n## Mountd\ns the number of acceptors\n\n\nnfs.mountd.acceptors\n \n=\n \n128\n\n\n\n## NFSd\ns port number\n\n\nnfs.nfsd.port\n \n=\n \n2049\n\n\n\n## NFSd\ns the number of acceptors\n\n\nnfs.nfsd.acceptors\n \n=\n \n128\n\n\n\n\n\n\n\nInstallation\n\n\nCentOS 6.x / 7.x\n\n\n1\n$ sudo yum install nfs-utils\n\n\n\n\n\n\nUbnutu 14.04 / 16.04\n\n\n1\n$ sudo apt-get install nfs-common\n\n\n\n\n\n\nStart LeoFS as a NFS Server with other dependent programs\n\n\n\n\nStart a LeoFS storage system\n\n\nRef: \nQuick Installation and Setup\n\n\nRef: \nBuilding a LeoFS' cluster with Ansible\n\n\n\n\n\n\nStart \nrpcbind\n\n\n\n\n1\n$ sudo service rpcbind start\n\n\n\n\n\n\n\n\nCreate a bucket and a token for LeoFS' NFSv3 server with \nleofs-adm gen-nfs-mnt-key \nBUCKET\n \nACCESS-KEY-ID\n \nCLIENT-IP_ADDRESS\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n$ ./leofs-adm add-bucket \ntest\n \n05236\n\nOK\n\n$ ./leofs-adm get-buckets\ncluster id   \n|\n bucket   \n|\n owner       \n|\n permissions      \n|\n created at\n-------------+----------+-------------+------------------+---------------------------\nleofs_1      \n|\n \ntest\n     \n|\n _test_leofs \n|\n Me\n(\nfull_control\n)\n \n|\n \n2014\n-07-31 \n10\n:20:42 +0900\n\n$ ./leofs-adm gen-nfs-mnt-key \ntest\n \n05236\n \n127\n.0.0.1\nbb5034f0c740148a346ed663ca0cf5157efb439f\n\n\n\n\n\n\n\n\nCreate a mount point and Mount\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n$ sudo mkdir /mnt/leofs\n\n\n## for Linux - \nsudo mount -t nfs -o nolock \nhost\n:/\nbucket\n/\naccesskey_id\n/\ntoken\n \ndir\n\n$ sudo mount -t nfs -o nolock \n127\n.0.0.1:/test/05236/bb5034f0c740148a346ed663ca0cf5157efb439f /mnt/leofs\n\n\n## for FreeBSD - \nsudo mount -t nfs -o nolockd \nhost\n:/\nbucket\n/\naccesskey_id\n/\ntoken\n \ndir\n\n$ sudo mount -t nfs -o nolockd \n127\n.0.0.1:/test/05236/bb5034f0c740148a346ed663ca0cf5157efb439f /mnt/leofs\n\n\n\n\n\n\n\n\nNow you can operate the bucket test in LeoFS as a filesystem via \n/mnt/leofs\n.\n\n\n\n\nConfirm that NFS works\n\n\n\n\nCreate a file\n\n\n\n\n1\n2\n3\n4\n5\n6\n$ touch /mnt/leofs/newfile\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx. \n0\n root root \n4096\n Jul \n31\n \n10\n:09 \n2014\n .\ndrwxr-xr-x. \n6\n root root \n4096\n Jul \n11\n \n12\n:38 \n2014\n ..\n-rw-rw-rw-  \n0\n root root    \n0\n Jul \n31\n \n10\n:25 \n2014\n newfile\n\n\n\n\n\n\n\n\nModify a file\n\n\n\n\n1\n2\n3\n4\n$ \necho\n \nhello world\n \n /mnt/leofs/newfile\n$ cat /mnt/leofs/newfile\n\nhello world\n\n\n\n\n\n\n\n\nCopy a file\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n$ cp /mnt/leofs/newfile /mnt/leofs/newfile.copy\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx  \n0\n root root \n4096\n Jul \n31\n \n10\n:09 \n2014\n .\ndrwxr-xr-x. \n6\n root root \n4096\n Jul \n11\n \n12\n:38 \n2014\n ..\n-rw-rw-rw-  \n0\n root root   \n12\n Jul \n31\n \n10\n:29 \n2014\n newfile\n-rw-rw-rw-  \n0\n root root   \n12\n Jul \n31\n \n10\n:31 \n2014\n newfile.copy\n\n\n\n\n\n\n\n\nCheck the file whether to store it into LeoFS with the \nleofs-adm whereis\n command\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n$ ./leofs-adm whereis test/newfile\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n del?  \n|\n           node           \n|\n             ring address             \n|\n    size    \n|\n   checksum   \n|\n  \n# of chunks   |     clock      |             when\n\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n       \n|\n storage_0@127.0.0.1      \n|\n 22f3d93762d31abc5f5704f78edf1691     \n|\n        12B \n|\n   6f5902ac23 \n|\n              \n0\n \n|\n 4ffe2d105f1f4  \n|\n \n2014\n-07-31 \n10\n:29:01 +0900\n\n$ ./leofs-adm whereis test/newfile.copy\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n del?  \n|\n           node           \n|\n             ring address             \n|\n    size    \n|\n   checksum   \n|\n  \n# of chunks   |     clock      |             when\n\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n       \n|\n storage_0@127.0.0.1      \n|\n d02e1e52d93242d2dcdb98224421a1fb     \n|\n        12B \n|\n   6f5902ac23 \n|\n              \n0\n \n|\n 4ffe2d20343a3  \n|\n \n2014\n-07-31 \n10\n:31:17 +0900\n\n\n\n\n\n\n\n\nDiff files\n\n\n\n\n1\n$ diff /mnt/leofs/newfile /mnt/leofs/newfile.copy\n\n\n\n\n\n\n\n\nRemove a file\n\n\n\n\n1\n2\n3\n4\n5\n6\n$ rm /mnt/leofs/newfile\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx  \n0\n root root \n4096\n Jul \n31\n \n10\n:09 \n2014\n .\ndrwxr-xr-x. \n6\n root root \n4096\n Jul \n11\n \n12\n:38 \n2014\n ..\n-rw-rw-rw-  \n0\n root root   \n12\n Jul \n31\n \n10\n:31 \n2014\n newfile.copy\n\n\n\n\n\n\n\n\nCheck the file whether to remove it into LeoFS with the \nleofs-adm whereis\n command\n\n\n\n\n1\n2\n3\n4\n5\n$ ./leofs-adm whereis test/newfile\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n del?  \n|\n           node           \n|\n             ring address             \n|\n    size    \n|\n   checksum   \n|\n  \n# of chunks   |     clock      |             when\n\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n  *    \n|\n storage_0@127.0.0.1      \n|\n 22f3d93762d31abc5f5704f78edf1691     \n|\n         0B \n|\n   d41d8cd98f \n|\n              \n0\n \n|\n 4ffe2e5d9cffe  \n|\n \n2014\n-07-31 \n10\n:34:50 +0900\n\n\n\n\n\n\n\n\nCreate a directory\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n$ mkdir -p /mnt/leofs/1/2/3\n$ ls -alR /mnt/leofs/1\n\n/mnt/leofs/1:\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n19\n:37 \n2014\n .\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n10\n:09 \n2014\n ..\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n10\n:37 \n2014\n \n2\n\n\n/mnt/leofs/1/2:\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n19\n:37 \n2014\n .\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n19\n:37 \n2014\n ..\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n10\n:37 \n2014\n \n3\n\n\n/mnt/leofs/1/2/3:\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n19\n:37 \n2014\n .\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n19\n:37 \n2014\n ..\n\n\n\n\n\n\n\n\nRemove files recursively\n\n\n\n\n1\n2\n3\n4\n5\n6\n$ rm -rf /mnt/leofs/1/\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx  \n0\n root root \n4096\n Jul \n31\n \n10\n:09 \n2014\n .\ndrwxr-xr-x. \n6\n root root \n4096\n Jul \n11\n \n12\n:38 \n2014\n ..\n-rw-rw-rw-  \n0\n root root   \n12\n Jul \n31\n \n10\n:31 \n2014\n leofs.copy\n\n\n\n\n\n\nOther basic file OR directory operations also should work except controlling owners/permissions/symbolic links/special files.\n\n\nLimits\n\n\nSince LeoFS NFS implementation is still the beta version, there are some limitations. The details are described at \nLeoFS Limits\n.\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoGateway Settings", 
            "title": "NFS v3"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#interface-nfs-v3", 
            "text": "", 
            "title": "Interface: NFS v3"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#pre-requirement", 
            "text": "We have checked the  NFSv3  server feature with CentOS 6.5, 7.x and Ubuntu Server 14.04, 16.04 LTS, but we did not strictly test other platforms, FreeBSD and SmartOS yet.", 
            "title": "Pre-requirement"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#configuration", 
            "text": "Update LeoGateway's protocol configuration to  nfs , and configure NFS related configurations in your  LeoGateway configuration   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 ## --------------------------------------------------------------------  ## GATEWAY Protocol  ## --------------------------------------------------------------------  ## Gateway Protocol to use: [s3 | rest | embed | nfs]  protocol   =   nfs  .  .  .  ## --------------------------------------------------------------------  ## GATEWAY - NFS-related configurations  ## --------------------------------------------------------------------  ## Mountd s port number  nfs.mountd.port   =   22050  ## Mountd s the number of acceptors  nfs.mountd.acceptors   =   128  ## NFSd s port number  nfs.nfsd.port   =   2049  ## NFSd s the number of acceptors  nfs.nfsd.acceptors   =   128", 
            "title": "Configuration"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#centos-6x-7x", 
            "text": "1 $ sudo yum install nfs-utils", 
            "title": "CentOS 6.x / 7.x"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#ubnutu-1404-1604", 
            "text": "1 $ sudo apt-get install nfs-common", 
            "title": "Ubnutu 14.04 / 16.04"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#start-leofs-as-a-nfs-server-with-other-dependent-programs", 
            "text": "Start a LeoFS storage system  Ref:  Quick Installation and Setup  Ref:  Building a LeoFS' cluster with Ansible    Start  rpcbind   1 $ sudo service rpcbind start    Create a bucket and a token for LeoFS' NFSv3 server with  leofs-adm gen-nfs-mnt-key  BUCKET   ACCESS-KEY-ID   CLIENT-IP_ADDRESS    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 $ ./leofs-adm add-bucket  test   05236 \nOK\n\n$ ./leofs-adm get-buckets\ncluster id    |  bucket    |  owner        |  permissions       |  created at\n-------------+----------+-------------+------------------+---------------------------\nleofs_1       |   test       |  _test_leofs  |  Me ( full_control )   |   2014 -07-31  10 :20:42 +0900\n\n$ ./leofs-adm gen-nfs-mnt-key  test   05236   127 .0.0.1\nbb5034f0c740148a346ed663ca0cf5157efb439f    Create a mount point and Mount   1\n2\n3\n4\n5\n6\n7 $ sudo mkdir /mnt/leofs ## for Linux -  sudo mount -t nfs -o nolock  host :/ bucket / accesskey_id / token   dir \n$ sudo mount -t nfs -o nolock  127 .0.0.1:/test/05236/bb5034f0c740148a346ed663ca0cf5157efb439f /mnt/leofs ## for FreeBSD -  sudo mount -t nfs -o nolockd  host :/ bucket / accesskey_id / token   dir \n$ sudo mount -t nfs -o nolockd  127 .0.0.1:/test/05236/bb5034f0c740148a346ed663ca0cf5157efb439f /mnt/leofs    Now you can operate the bucket test in LeoFS as a filesystem via  /mnt/leofs .", 
            "title": "Start LeoFS as a NFS Server with other dependent programs"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#confirm-that-nfs-works", 
            "text": "Create a file   1\n2\n3\n4\n5\n6 $ touch /mnt/leofs/newfile\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx.  0  root root  4096  Jul  31   10 :09  2014  .\ndrwxr-xr-x.  6  root root  4096  Jul  11   12 :38  2014  ..\n-rw-rw-rw-   0  root root     0  Jul  31   10 :25  2014  newfile    Modify a file   1\n2\n3\n4 $  echo   hello world    /mnt/leofs/newfile\n$ cat /mnt/leofs/newfile\n\nhello world    Copy a file   1\n2\n3\n4\n5\n6\n7 $ cp /mnt/leofs/newfile /mnt/leofs/newfile.copy\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx   0  root root  4096  Jul  31   10 :09  2014  .\ndrwxr-xr-x.  6  root root  4096  Jul  11   12 :38  2014  ..\n-rw-rw-rw-   0  root root    12  Jul  31   10 :29  2014  newfile\n-rw-rw-rw-   0  root root    12  Jul  31   10 :31  2014  newfile.copy    Check the file whether to store it into LeoFS with the  leofs-adm whereis  command    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 $ ./leofs-adm whereis test/newfile\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n del?   |            node            |              ring address              |     size     |    checksum    |    # of chunks   |     clock      |             when \n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n        |  storage_0@127.0.0.1       |  22f3d93762d31abc5f5704f78edf1691      |         12B  |    6f5902ac23  |                0   |  4ffe2d105f1f4   |   2014 -07-31  10 :29:01 +0900\n\n$ ./leofs-adm whereis test/newfile.copy\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n del?   |            node            |              ring address              |     size     |    checksum    |    # of chunks   |     clock      |             when \n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n        |  storage_0@127.0.0.1       |  d02e1e52d93242d2dcdb98224421a1fb      |         12B  |    6f5902ac23  |                0   |  4ffe2d20343a3   |   2014 -07-31  10 :31:17 +0900    Diff files   1 $ diff /mnt/leofs/newfile /mnt/leofs/newfile.copy    Remove a file   1\n2\n3\n4\n5\n6 $ rm /mnt/leofs/newfile\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx   0  root root  4096  Jul  31   10 :09  2014  .\ndrwxr-xr-x.  6  root root  4096  Jul  11   12 :38  2014  ..\n-rw-rw-rw-   0  root root    12  Jul  31   10 :31  2014  newfile.copy    Check the file whether to remove it into LeoFS with the  leofs-adm whereis  command   1\n2\n3\n4\n5 $ ./leofs-adm whereis test/newfile\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n del?   |            node            |              ring address              |     size     |    checksum    |    # of chunks   |     clock      |             when \n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n  *     |  storage_0@127.0.0.1       |  22f3d93762d31abc5f5704f78edf1691      |          0B  |    d41d8cd98f  |                0   |  4ffe2e5d9cffe   |   2014 -07-31  10 :34:50 +0900    Create a directory    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 $ mkdir -p /mnt/leofs/1/2/3\n$ ls -alR /mnt/leofs/1\n\n/mnt/leofs/1:\ndrwxrwxrwx  0  root root  4096  Jul  31   19 :37  2014  .\ndrwxrwxrwx  0  root root  4096  Jul  31   10 :09  2014  ..\ndrwxrwxrwx  0  root root  4096  Jul  31   10 :37  2014   2 \n\n/mnt/leofs/1/2:\ndrwxrwxrwx  0  root root  4096  Jul  31   19 :37  2014  .\ndrwxrwxrwx  0  root root  4096  Jul  31   19 :37  2014  ..\ndrwxrwxrwx  0  root root  4096  Jul  31   10 :37  2014   3 \n\n/mnt/leofs/1/2/3:\ndrwxrwxrwx  0  root root  4096  Jul  31   19 :37  2014  .\ndrwxrwxrwx  0  root root  4096  Jul  31   19 :37  2014  ..    Remove files recursively   1\n2\n3\n4\n5\n6 $ rm -rf /mnt/leofs/1/\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx   0  root root  4096  Jul  31   10 :09  2014  .\ndrwxr-xr-x.  6  root root  4096  Jul  11   12 :38  2014  ..\n-rw-rw-rw-   0  root root    12  Jul  31   10 :31  2014  leofs.copy   Other basic file OR directory operations also should work except controlling owners/permissions/symbolic links/special files.", 
            "title": "Confirm that NFS works"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#limits", 
            "text": "Since LeoFS NFS implementation is still the beta version, there are some limitations. The details are described at  LeoFS Limits .", 
            "title": "Limits"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#related-links", 
            "text": "For Administrators / Settings / LeoGateway Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_operations/flow/", 
            "text": "Launch and Operation Flow\n\n\nOperation Flow\n\n\n\n\nBuild a LeoManager cluster\n\n\nConfirm the state of the LeoManager cluster\n\n\nBuild a LeoStorage cluster\n\n\nConfirm the state of the LeoStorage cluster\n\n\nLaunch LeoGateway nodes\n\n\nConfirm the state of the LeoFS system\n\n\nAfter launching the system, execute processing depending on the status of the system\n\n\nAttach a LeoStorage node\n\n\nDetach a LeoStorage node\n\n\nSuspend a LeoStorage node\n\n\nResume a LeoStorage node\n\n\n\n\n\n\n\n\n\n\n\n\nThe diagram only\n\n\n\n\nLaunch Order of LeoFS' Components\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLeoManager\n\n\n\n\n\n\n\n\n$ leofs_manager start\n\n\nStart LeoManager\u2019s master\n\n\n\n\n\n\n$ leofs_manager start\n\n\nStart LeoManager\u2019s slave\n\n\n\n\n\n\nLeoManager\n\n\n\n\n\n\n\n\n$ leofs_storage start\n\n\nStart a LeoStorage node\n\n\n\n\n\n\n(\nRepeatedly launch LeoStorage nodes\n)\n\n\n\n\n\n\n\n\n$ leofs-adm start\n\n\nStart Leostorage cluster\n\n\n\n\n\n\n$ leofs-adm status\n\n\nConfirm the current state of the LeoFS system (1)\n\n\n\n\n\n\nLeoGateway\n\n\n\n\n\n\n\n\n$ bin/leofs_gateway start\n\n\nStart a LeoGateway node\n\n\n\n\n\n\n(\nRepeatedly launch LeoGateway nodes\n)\n\n\n\n\n\n\n\n\n$ leofs-adm status\n\n\nConfirm the current state of the LeoFS system (2)\n\n\n\n\n\n\n\n\n\n\nNote: Restart a LeoManager when both of them are down\n\n\nWhen both of the LeoManagers are down and you try to restart a LeoManager that is NOT the one terminated at last, you can not restart the LeoManager because a sprit-brain could happen. If you make sure there is no case the data inconsistency could happen due to a sprit-brain then do \nleo_manager start force_load\n that allows you to restart a LeoManager in such cases. Please refer \nWhat is the significance of a Mnesia Master Node in a cluster\n for more information.\n\n\n\n\nRelated Links\n\n\n\n\nGetting Started / Quick Installation and Setup\n\n\nGetting Started / Building a LeoFS' cluster with Ansible\n\n\nFor Administrators / System Operations / Cluster Operations", 
            "title": "Operation Flow"
        }, 
        {
            "location": "/admin/system_operations/flow/#launch-and-operation-flow", 
            "text": "", 
            "title": "Launch and Operation Flow"
        }, 
        {
            "location": "/admin/system_operations/flow/#operation-flow", 
            "text": "Build a LeoManager cluster  Confirm the state of the LeoManager cluster  Build a LeoStorage cluster  Confirm the state of the LeoStorage cluster  Launch LeoGateway nodes  Confirm the state of the LeoFS system  After launching the system, execute processing depending on the status of the system  Attach a LeoStorage node  Detach a LeoStorage node  Suspend a LeoStorage node  Resume a LeoStorage node       The diagram only", 
            "title": "Operation Flow"
        }, 
        {
            "location": "/admin/system_operations/flow/#launch-order-of-leofs-components", 
            "text": "Command  Description      LeoManager     $ leofs_manager start  Start LeoManager\u2019s master    $ leofs_manager start  Start LeoManager\u2019s slave    LeoManager     $ leofs_storage start  Start a LeoStorage node    ( Repeatedly launch LeoStorage nodes )     $ leofs-adm start  Start Leostorage cluster    $ leofs-adm status  Confirm the current state of the LeoFS system (1)    LeoGateway     $ bin/leofs_gateway start  Start a LeoGateway node    ( Repeatedly launch LeoGateway nodes )     $ leofs-adm status  Confirm the current state of the LeoFS system (2)      Note: Restart a LeoManager when both of them are down  When both of the LeoManagers are down and you try to restart a LeoManager that is NOT the one terminated at last, you can not restart the LeoManager because a sprit-brain could happen. If you make sure there is no case the data inconsistency could happen due to a sprit-brain then do  leo_manager start force_load  that allows you to restart a LeoManager in such cases. Please refer  What is the significance of a Mnesia Master Node in a cluster  for more information.", 
            "title": "Launch Order of LeoFS' Components"
        }, 
        {
            "location": "/admin/system_operations/flow/#related-links", 
            "text": "Getting Started / Quick Installation and Setup  Getting Started / Building a LeoFS' cluster with Ansible  For Administrators / System Operations / Cluster Operations", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_operations/systemd/", 
            "text": "Systemd Services\n\n\nThis section describes how to manage LeoFS nodes on systemd-based Linux distributions.\n\n\nRequirements\n\n\nUsers of LeoFS v1.3.8 or higher running official Linux packages under supported systemd-based Linux distros (currently: Ubuntu 16.04 and EL7-based) can use supplied systemd units for starting/stopping nodes. Advantages offered by doing so include:\n\n\n\n\nWay to automatically start nodes on system startup and correctly shutdown them during power off / reboot.\n\n\nReliable way of controlling nodes (e.g. when LeoStorage node is launched in absence of LeoManager nodes, it will keep on restarting till LeoManager becomes available; nodes always being able to restart in case of unexpected situation such as crash or OOM kill)\n\n\nCorrect inter-node dependencies (affecting startup/shutdown order) for launching nodes in all-in-one cluster (when all nodes are running on the same system, e.g. for testing)\n\n\n\n\nIt's possible to switch to systemd-based way to control nodes (and back) anytime for users of v1.3.8 and higher running any supported systemd-based Linux distro.\n\n\nSwitching to systemd services\n\n\nFor most users, switch is as easy as stopping currently running node and starting corresponding systemd service instead:\n\n\n1\n2\n# /usr/local/leofs/current/leo_manager_0/bin/leo_manager stop\n# systemctl start leofs-manager-master\n\n\n\n\n\n\nNames of systemd units are: \nleofs-manager-master\n, \nleofs-manager-slave\n, \nleofs-gateway\n, \nleofs-storage\n. For example, to enable automatic startup of LeoStorage use command \nsystemctl enable leofs-storage\n.\n\n\nImmediate change when running nodes as systemd services is that \nerlang.log\n does not exist anymore; the information from that log file is available using journalctl, e.g.\n\n\n1\n# journalctl -u leofs-manager-master\n\n\n\n\n\n\n(currently, default configuration of Ubuntu 16.04 and EL7 additionally makes these messages available in syslog like \n/var/log/messages\n or \n/var/log/syslog\n, but this depends on system settings)\n\n\nCaveats when using systemd services\n\n\nMixing old and new way to launch nodes\n\n\nMixing launching / stopping nodes with launch scripts and systemd services is \nnot supported\n. Please don't try to start node through systemd and then stop with launch script. However, some commands of current launch scripts (other than \"start/stop/console/attach\") are perfectly compatible with systemd services, for example:\n\n\n1\n2\n3\n4\n5\n6\n7\n# systemctl start leofs-storage\n# /usr/local/leofs/current/leo_storage/bin/leo_storage ping\npong\n# /usr/local/leofs/current/leo_storage/bin/leo_storage remote_console\nErlang/OTP 20 [erts-9.0] [source] [64-bit] [smp:8:8] [ds:8:8:10] [async-threads:10] [hipe] [kernel-poll:false]\n\nEshell V9.0  (abort with ^G)\n\n\n\n\n\n\nThere might be some places in LeoFS documentation that suggest you to execute launch scripts with start/stop commands. Please pay attention and substitute these with \nsystemctl start/stop\n when running LeoFS nodes as systemd services.\n\n\nRunning nodes as user other than \nleofs\n\n\nCurrent systemd unit files try to launch LeoFS nodes as \nleofs\n user and group. Changing that through environment variable or environment config file won't work, because it's likely that \nleofs\n user does not have priviledge to further change user. In case running as different user is needed for some reason, please change service files using systemd's \noverride\n feature like in this example:\n\n\n1\n# systemctl edit leofs-manager-master\n\n\n\n\n\n\nThen enter\n\n\n1\n2\n3\n[Service]\n\n\nUser\n=\nsomeuser\n\n\nGroup\n=\nsomegroup\n\n\n\n\n\n\n\nSave and exit. Repeat for other unit files, if needed. For every node, user set in systemd unit files and in .environment config must be the same.\n\n\nLong shutdown times\n\n\nIn some cases (primary, LeoStorage under high write load) a node can take a long time to shutdown. When running node as systemd service, \nreboot\n command will ask node to shutdown safely before actually rebooting the system, which might take a long time under these conditions. Current limit for LeoStorage is set to 30 minutes, though this can be changed using systemd's override feature. Long shutdown / reboot times are expected for LeoStorage under high load.\n\n\nRunning LeoFS nodes on systems running other Erlang software\n\n\nThis section only applies to people who need to run LeoFS node \nand\n some other Erlang software on the same system.\n\n\nLeoFS, as well as certain types of other Erlang software relies on Erlang Port Mapper Daemon (\nepmd\n). Usually there can be only single instance of \nepmd\n running on the same node. To ensure smooth LeoFS operation, v1.3.8 and higher packages ensure that \nleofs-epmd.socket\n is enabled on boot and started right after upgrade. This is the case even when not using systemd-based services to launch LeoFS nodes. This service is running as \"leofs\" user by default (there should be no need to change that).\n\n\nUsually, service for LeoFS-supplied \nepmd\n (which uses socket activation) should work even for other Erlang software that might need \nepmd\n and there will be no problems. However, in case running some other instance of \nepmd\n is really needed, it should be possible to mask \nleofs-epmd.socket\n and \nleofs-epmd.service\n and remove dependencies on these units. It should work as long as \nepmd\n is always running before launching LeoFS nodes and is set up to listen on all interfaces (\n0.0.0.0\n), not just the local one.", 
            "title": "Systemd Services"
        }, 
        {
            "location": "/admin/system_operations/systemd/#systemd-services", 
            "text": "This section describes how to manage LeoFS nodes on systemd-based Linux distributions.", 
            "title": "Systemd Services"
        }, 
        {
            "location": "/admin/system_operations/systemd/#requirements", 
            "text": "Users of LeoFS v1.3.8 or higher running official Linux packages under supported systemd-based Linux distros (currently: Ubuntu 16.04 and EL7-based) can use supplied systemd units for starting/stopping nodes. Advantages offered by doing so include:   Way to automatically start nodes on system startup and correctly shutdown them during power off / reboot.  Reliable way of controlling nodes (e.g. when LeoStorage node is launched in absence of LeoManager nodes, it will keep on restarting till LeoManager becomes available; nodes always being able to restart in case of unexpected situation such as crash or OOM kill)  Correct inter-node dependencies (affecting startup/shutdown order) for launching nodes in all-in-one cluster (when all nodes are running on the same system, e.g. for testing)   It's possible to switch to systemd-based way to control nodes (and back) anytime for users of v1.3.8 and higher running any supported systemd-based Linux distro.", 
            "title": "Requirements"
        }, 
        {
            "location": "/admin/system_operations/systemd/#switching-to-systemd-services", 
            "text": "For most users, switch is as easy as stopping currently running node and starting corresponding systemd service instead:  1\n2 # /usr/local/leofs/current/leo_manager_0/bin/leo_manager stop\n# systemctl start leofs-manager-master   Names of systemd units are:  leofs-manager-master ,  leofs-manager-slave ,  leofs-gateway ,  leofs-storage . For example, to enable automatic startup of LeoStorage use command  systemctl enable leofs-storage .  Immediate change when running nodes as systemd services is that  erlang.log  does not exist anymore; the information from that log file is available using journalctl, e.g.  1 # journalctl -u leofs-manager-master   (currently, default configuration of Ubuntu 16.04 and EL7 additionally makes these messages available in syslog like  /var/log/messages  or  /var/log/syslog , but this depends on system settings)", 
            "title": "Switching to systemd services"
        }, 
        {
            "location": "/admin/system_operations/systemd/#caveats-when-using-systemd-services", 
            "text": "", 
            "title": "Caveats when using systemd services"
        }, 
        {
            "location": "/admin/system_operations/systemd/#mixing-old-and-new-way-to-launch-nodes", 
            "text": "Mixing launching / stopping nodes with launch scripts and systemd services is  not supported . Please don't try to start node through systemd and then stop with launch script. However, some commands of current launch scripts (other than \"start/stop/console/attach\") are perfectly compatible with systemd services, for example:  1\n2\n3\n4\n5\n6\n7 # systemctl start leofs-storage\n# /usr/local/leofs/current/leo_storage/bin/leo_storage ping\npong\n# /usr/local/leofs/current/leo_storage/bin/leo_storage remote_console\nErlang/OTP 20 [erts-9.0] [source] [64-bit] [smp:8:8] [ds:8:8:10] [async-threads:10] [hipe] [kernel-poll:false]\n\nEshell V9.0  (abort with ^G)   There might be some places in LeoFS documentation that suggest you to execute launch scripts with start/stop commands. Please pay attention and substitute these with  systemctl start/stop  when running LeoFS nodes as systemd services.", 
            "title": "Mixing old and new way to launch nodes"
        }, 
        {
            "location": "/admin/system_operations/systemd/#running-nodes-as-user-other-than-leofs", 
            "text": "Current systemd unit files try to launch LeoFS nodes as  leofs  user and group. Changing that through environment variable or environment config file won't work, because it's likely that  leofs  user does not have priviledge to further change user. In case running as different user is needed for some reason, please change service files using systemd's  override  feature like in this example:  1 # systemctl edit leofs-manager-master   Then enter  1\n2\n3 [Service]  User = someuser  Group = somegroup    Save and exit. Repeat for other unit files, if needed. For every node, user set in systemd unit files and in .environment config must be the same.", 
            "title": "Running nodes as user other than leofs"
        }, 
        {
            "location": "/admin/system_operations/systemd/#long-shutdown-times", 
            "text": "In some cases (primary, LeoStorage under high write load) a node can take a long time to shutdown. When running node as systemd service,  reboot  command will ask node to shutdown safely before actually rebooting the system, which might take a long time under these conditions. Current limit for LeoStorage is set to 30 minutes, though this can be changed using systemd's override feature. Long shutdown / reboot times are expected for LeoStorage under high load.", 
            "title": "Long shutdown times"
        }, 
        {
            "location": "/admin/system_operations/systemd/#running-leofs-nodes-on-systems-running-other-erlang-software", 
            "text": "This section only applies to people who need to run LeoFS node  and  some other Erlang software on the same system.  LeoFS, as well as certain types of other Erlang software relies on Erlang Port Mapper Daemon ( epmd ). Usually there can be only single instance of  epmd  running on the same node. To ensure smooth LeoFS operation, v1.3.8 and higher packages ensure that  leofs-epmd.socket  is enabled on boot and started right after upgrade. This is the case even when not using systemd-based services to launch LeoFS nodes. This service is running as \"leofs\" user by default (there should be no need to change that).  Usually, service for LeoFS-supplied  epmd  (which uses socket activation) should work even for other Erlang software that might need  epmd  and there will be no problems. However, in case running some other instance of  epmd  is really needed, it should be possible to mask  leofs-epmd.socket  and  leofs-epmd.service  and remove dependencies on these units. It should work as long as  epmd  is always running before launching LeoFS nodes and is set up to listen on all interfaces ( 0.0.0.0 ), not just the local one.", 
            "title": "Running LeoFS nodes on systems running other Erlang software"
        }, 
        {
            "location": "/admin/system_operations/s3/", 
            "text": "S3-API Related Operations\n\n\nPrior Knowledge\n\n\nWhen choosing Amazon S3 API\n1\n as LeoGateway's protocol, you need LeoFS' S3-API related operations including \nEndpoint\n, \nUser\n, and \nBucket\n operations.\n\n\nYou can access a LeoFS system whose LeoGateway's protocol is S3-API by following the flow.\n\n\n\n\nCreate an endpoint\n to be able to access a LeoFS system's resources.\n\n\nCreate a user\n to separately manage the user's buckets and objects.\n\n\nAfter creating a bucket, If you need to \nchange ACL of a bucket\n, you can set it as an administrator of a LeoFS system. The current version, v1.3 of LeoFS does not support\n\n\n\n\nOperations\n\n\nEndpoint\n\n\nYou can continue to use a LeoFS system's DNS name to access its resources after you've set up an endpoint, which has a policy that controls the use of the endpoint to access the resources.\n\n\nCreate an endpoint, \nadd-endpoint\n\n\nCreates an endpoint so users can access a LeoFS system's resources.\n\n\n1\n2\n3\n4\n5\n$ leofs-adm add-endpoint \nendpoint\n\n\n\n## Example\n\n$ leofs-adm add-endpoint leo-project.net\nOK\n\n\n\n\n\n\nRemove an endpoint, \ndelete-endpoint\n\n\nRemoves an endpoint if a LeoFS system's system does not need it.\n\n\n1\n2\n3\n4\n5\n$ leofs-adm delete-endpoint \nendpoint\n\n\n\n## Example\n\n$ leofs-adm delete-endpoint leo-project.net\nOK\n\n\n\n\n\n\nRetrieve a list of endpoints, \nget-endpoints\n\n\nRetrieves a list of the endpoints if you need to know the existing endpoints.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n$ leofs-adm get-endpoints\n\n\n## Example\n\n$ leofs-adm get-endpoints\nendpoint         \n|\n created at\n-----------------+---------------------------\nleo-project.net  \n|\n \n2017\n-04-07 \n10\n:07:31 +0900\nlocalhost        \n|\n \n2017\n-04-07 \n10\n:06:18 +0900\ns3.amazonaws.com \n|\n \n2017\n-04-07 \n10\n:06:18 +0900\n\n\n\n\n\n\nUser\n\n\nTo access a LeoFS system, its system's users need to create an account and retrieve \nAccessKeyID\n and \nSecretAccessKey\n.\n\n\nCreate a user, \ncreate-user\n\n\nCreates a user's account so its user can access a LeoFS system.\n\n\n1\n2\n3\n4\n5\n6\n$ leofs-adm create-user \nuser-id\n \npassword\n\n\n\n## Example\n\n$ leofs-adm create-user leofs-user-1 PASSWORD\naccess-key-id: b5f0413d45855fcc055e\nsecret-access-key: b74f9ae91dd0de81f71e19f8d455a7c081f34c57\n\n\n\n\n\n\nRemove a user, \ndelete-user\n\n\nRemoves a user account if a LeoFS system does not need it.\n\n\n1\n2\n3\n4\n5\n$ leofs-adm delete-user \nuser-id\n\n\n\n## Example\n\n$ leofs-adm delete-user leofs-user-1\nOK\n\n\n\n\n\n\nRetrieve a list of users, \nget-users\n\n\nRetrieves a list of the users if you need to know the existing users.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n$ leofs-adm get-users\n\n\n## Example\n\n$ leofs-adm get-users\nuser_id      \n|\n role_id \n|\n access_key_id          \n|\n created_at\n-------------+---------+------------------------+---------------------------\n_test_leofs  \n|\n \n9\n       \n|\n \n05236\n                  \n|\n \n2017\n-04-10 \n09\n:52:39 +0900\nleofs-user-2 \n|\n \n1\n       \n|\n b5f0413d45855fcc055e   \n|\n \n2017\n-04-10 \n09\n:54:35 +0900\n\n\n\n\n\n\nChange role of a user, \nupdate-user-role\n\n\nUpdates a user's role if you need to change the role of a LeoFS system. You can choose a role from \nAdministrator\n or \nGeneral user\n.\n\n\n\n\n\n\n\n\nRole Id\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nAdministrator\n\n\n\n\n\n\n9\n\n\nGeneral User\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n$ leofs-adm update-user-role \nuser-id\n \nrole-id\n\n\n\n## Example\n\n$ leofs-adm update-user-role leofs-user-2 \n9\n\nOK\n\n\n## Confirm the latest list of the users\n\n$ leofs-adm get-users\nuser_id      \n|\n role_id \n|\n access_key_id          \n|\n created_at\n-------------+---------+------------------------+---------------------------\n_test_leofs  \n|\n \n9\n       \n|\n \n05236\n                  \n|\n \n2017\n-04-10 \n09\n:52:39 +0900\nleofs-user   \n|\n \n1\n       \n|\n 3c1d889cdd2088fb9482   \n|\n \n2017\n-04-10 \n09\n:53:33 +0900\nleofs-user-2 \n|\n \n9\n       \n|\n b5f0413d45855fcc055e   \n|\n \n2017\n-04-10 \n09\n:54:35 +0900\n\n\n\n\n\n\nBucket\n\n\nTo manage objects under a bucket, you need to create a bucket with \nleofs-adm\n or any client for Amazon S3 such as s3cmd\n2\n and DragonDisk\n3\n.\n\n\nCreate a bucket, \nadd-bucket\n\n\nCreates a bucket so a LeoFS system' users can manage resources under its bucket.\n\n\n1\n2\n3\n4\n5\n$ leofs-adm add-bucket \nbucket-name\n \naccess-key-id\n\n\n\n## Example\n\n$ leofs-adm add-bucket backup \n05236\n\nOK\n\n\n\n\n\n\n\n\nNote: Creating bucket with the same name as one being removed\n\n\nIt's not possible to create bucket with the same name as one currently being removed. Deletion of all objects belonging to former bucket must complete before bucket with the same name can be created again.\n\n\n\n\nRemove a bucket, \ndelete-bucket\n\n\nRemoves a bucket if its user does not need it. The bucket name is removed right away, while objects from that bucket will be deleted in asynchronous fashion after that.\n\n\n1\n2\n3\n4\n5\n$ leofs-adm delete-bucket \nbucket-name\n \naccess-key-id\n\n\n\n## Example\n\n$ leofs-adm delete-bucket backup \n05236\n\nOK\n\n\n\n\n\n\n\n\nNote: How many delete-bucket requests can be proceeded at once\n\n\n8 delete-bucket requests can be proceeded at once and if there are over 8 ones then exceeded ones are pushed on a queue and will be proceeded later once the number of processing delete-bucket go down under 8.\n\n\n\n\nRetrieve a list of the ongoing delete-buckets, \ndelete-bucket-stats\n\n\nRetrieves a list of the ongoing delete-buckets if you need to know the progress of the ongoing delete-buckets.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n$ leofs-adm delete-bucket-stats \nbucket-name\n\n\n\n## Example\n\n$ leofs-adm delete-bucket-stats\n- Bucket: test_2\nnode                         \n|\n state            \n|\n date\n-----------------------------+------------------+-----------------------------\nstorage_3@127.0.0.1          \n|\n pending          \n|\n \n2017\n-07-27 \n09\n:20:47 +0900\nstorage_2@127.0.0.1          \n|\n monitoring       \n|\n \n2017\n-07-27 \n09\n:21:21 +0900\nstorage_1@127.0.0.1          \n|\n monitoring       \n|\n \n2017\n-07-27 \n09\n:21:21 +0900\nstorage_0@127.0.0.1          \n|\n monitoring       \n|\n \n2017\n-07-27 \n09\n:21:22 +0900\n\n- Bucket: test_4\nnode                         \n|\n state            \n|\n date\n-----------------------------+------------------+-----------------------------\nstorage_3@127.0.0.1          \n|\n finished         \n|\n \n2017\n-07-27 \n09\n:21:20 +0900\nstorage_2@127.0.0.1          \n|\n finished         \n|\n \n2017\n-07-27 \n09\n:21:18 +0900\nstorage_1@127.0.0.1          \n|\n finished         \n|\n \n2017\n-07-27 \n09\n:21:21 +0900\nstorage_0@127.0.0.1          \n|\n monitoring       \n|\n \n2017\n-07-27 \n09\n:21:10 +0900\n\n\n## Example (No ongoing delete-buckets)\n\n$ leofs-adm delete-bucket-stats\n\n[\nERROR\n]\n Delete-bucket\ns stats not found\n\n\n\n\n\n\nThe below table explains what each state means.\n\n\n\n\n\n\n\n\nState\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npending\n\n\nWaiting on the queue.\n\n\n\n\n\n\nenqueuing\n\n\nIterating all objects and enqueuing object identifiers into the queue if those belong to the bucket deleted.\n\n\n\n\n\n\nmonitoring\n\n\nIterating all items in the queue and deleting objects one by one.\n\n\n\n\n\n\nfinished\n\n\nFinished delete-bucket and waiting for other storage nodes.\n\n\n\n\n\n\n\n\n\n\nNote: How long the delete-bucket records exist\n\n\nOnce the state of every storage node transit into finished, the delete-bucket records get deleted within several tens of seconds.\n\n\n\n\nReset a delete-bucket-stats record, \nreset-delete-bucket-stats\n\n\nReset a delete-bucket-stats record if you face some trouble that inconsistency between manager nodes and storage nodes happened and only way to solve this problem is to reset(delete) a delete-bucket-stats record.\n\n\n1\n2\n3\n4\n5\n$ leofs-adm reset-delete-bucket-stats \nbucket-name\n\n\n\n## Example\n\n$ leofs-adm reset-delete-bucket-stats bucket\nOK\n\n\n\n\n\n\n\n\nNote: Removing inaccessible objects after \nreset-delete-bucket-stats\n\n\nSince this operation aborts deletion of objects belonging to (already deleted) bucket, it might be a good idea to create and delete bucket with same name again after \nreset-delete-bucket-stats\n, to make sure that bucket deletion completes properly and no stale objects remain. \n\n\n\n\nRetrieve a list of buckets, \nget-buckets\n\n\nRetrieves a list of the buckets if you need to know the existing buckets.\n\n\n1\n2\n3\n4\n5\n6\n7\n$ leofs-adm get-buckets\n\n\n## Example\n\n$ leofs-adm get-buckets\ncluster id   \n|\n bucket   \n|\n owner       \n|\n permissions      \n|\n redundancy method            \n|\n created at\n-------------+----------+-------------+------------------+------------------------------+---------------------------\nleofs_1      \n|\n \ntest\n     \n|\n _test_leofs \n|\n Me\n(\nfull_control\n)\n \n|\n copy, \n{\nn:1, w:1, r:1, d:1\n}\n   \n|\n \n2017\n-04-10 \n10\n:57:29 +0900\n\n\n\n\n\n\nRetrieve a bucket, \nget-bucket\n\n\nRetrieves a bucket if you need to know its bucket's information.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n$ leofs-adm get-bucket \naccess-key-id\n\n\n\n## Example\n\n$ leofs-adm get-bucket \n05236\n\nbucket   \n|\n permissions                            \n|\n created at\n---------+----------------------------------------+---------------------------\nbackup   \n|\n Me\n(\nfull_control\n)\n, Everyone\n(\nread\n)\n       \n|\n \n2017\n-04-10 \n10\n:39:01 +0900\ndocs     \n|\n Me\n(\nfull_control\n)\n, Everyone\n(\nread\n)\n       \n|\n \n2017\n-04-10 \n10\n:39:25 +0900\nlogs     \n|\n Me\n(\nfull_control\n)\n, Everyone\n(\nread,write\n)\n \n|\n \n2017\n-04-10 \n10\n:39:38 +0900\nmovie    \n|\n Me\n(\nfull_control\n)\n                       \n|\n \n2017\n-04-10 \n10\n:39:45 +0900\n\n\n\n\n\n\nChange an owner of a bucket, \nchown-bucket\n\n\nUpdates an owner of a bucket if you need to change its owner to someone else.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n$ leofs-adm chown-bucket \nbucket\n \naccess-key-id\n\n\n\n## Example\n\n$ leofs-adm chown-bucket \ntest\n b5f0413d45855fcc055e\nOK\n\n\n## Confirm the latest list of the buckets\n\n$ leofs-adm get-buckets\ncluster id   \n|\n bucket   \n|\n owner        \n|\n permissions      \n|\n redundancy method            \n|\n created at\n-------------+----------+--------------+------------------+------------------------------+---------------------------\nleofs_1      \n|\n \ntest\n     \n|\n leofs-user-2 \n|\n Me\n(\nfull_control\n)\n \n|\n copy, \n{\nn:1, w:1, r:1, d:1\n}\n   \n|\n \n2017\n-04-10 \n10\n:57:29 +0900\n\n\n\n\n\n\nUpdate ACL of a bucket, \nupdate-acl\n\n\nUpdates ACL of a bucket if you need to change it.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n$ leofs-adm update-acl \nbucket\n \naccess-key-id\n \ncanned-ACL\n\n\n\n## Example\n\n$ leofs-adm update-acl \ntest\n \n05236\n private\nok\n$ leofs-adm update-acl \ntest\n \n05236\n public-read\nok\n$ leofs-adm update-acl \ntest\n \n05236\n public-read-write\nok\n\n\n\n\n\n\nCanned ACL\n\n\nWhen using S3-API, LeoFS supports a set of predefined grants, known as canned ACLs. Each canned ACL has a predefined a set of grantees and permissions. The following table lists the set of canned ACLs and the associated predefined grants.\n\n\n\n\n\n\n\n\nCanned ACL\n\n\nApplies To\n\n\nPermissions Added To ACL\n\n\n\n\n\n\n\n\n\n\nprivate\n\n\nBucket and object\n\n\n[default]\n Owner gets FULL_CONTROL. No one else has access rights.\n\n\n\n\n\n\npublic-read\n\n\nBucket and object\n\n\nOwner gets FULL_CONTROL. The AllUsers group gets READ access.\n\n\n\n\n\n\npublic-read-write\n\n\nBucket and object\n\n\nOwner gets FULL_CONTROL. The AllUsers group gets READ and WRITE access. Granting this on a bucket is generally not recommended.\n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Interface / S3-API\n\n\n\n\n\n\n\n\n\n\n\n\nAmazon S3 API\n\n\n\n\n\n\nS3cmd: A command Line S3 Client and Backup for Linux and Mac\n\n\n\n\n\n\nDragonDisk: A cloud storage client", 
            "title": "S3-API related Operations"
        }, 
        {
            "location": "/admin/system_operations/s3/#s3-api-related-operations", 
            "text": "", 
            "title": "S3-API Related Operations"
        }, 
        {
            "location": "/admin/system_operations/s3/#prior-knowledge", 
            "text": "When choosing Amazon S3 API 1  as LeoGateway's protocol, you need LeoFS' S3-API related operations including  Endpoint ,  User , and  Bucket  operations.  You can access a LeoFS system whose LeoGateway's protocol is S3-API by following the flow.   Create an endpoint  to be able to access a LeoFS system's resources.  Create a user  to separately manage the user's buckets and objects.  After creating a bucket, If you need to  change ACL of a bucket , you can set it as an administrator of a LeoFS system. The current version, v1.3 of LeoFS does not support", 
            "title": "Prior Knowledge"
        }, 
        {
            "location": "/admin/system_operations/s3/#operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/admin/system_operations/s3/#endpoint", 
            "text": "You can continue to use a LeoFS system's DNS name to access its resources after you've set up an endpoint, which has a policy that controls the use of the endpoint to access the resources.", 
            "title": "Endpoint"
        }, 
        {
            "location": "/admin/system_operations/s3/#create-an-endpoint-add-endpoint", 
            "text": "Creates an endpoint so users can access a LeoFS system's resources.  1\n2\n3\n4\n5 $ leofs-adm add-endpoint  endpoint  ## Example \n$ leofs-adm add-endpoint leo-project.net\nOK", 
            "title": "Create an endpoint, add-endpoint"
        }, 
        {
            "location": "/admin/system_operations/s3/#remove-an-endpoint-delete-endpoint", 
            "text": "Removes an endpoint if a LeoFS system's system does not need it.  1\n2\n3\n4\n5 $ leofs-adm delete-endpoint  endpoint  ## Example \n$ leofs-adm delete-endpoint leo-project.net\nOK", 
            "title": "Remove an endpoint, delete-endpoint"
        }, 
        {
            "location": "/admin/system_operations/s3/#retrieve-a-list-of-endpoints-get-endpoints", 
            "text": "Retrieves a list of the endpoints if you need to know the existing endpoints.  1\n2\n3\n4\n5\n6\n7\n8\n9 $ leofs-adm get-endpoints ## Example \n$ leofs-adm get-endpoints\nendpoint          |  created at\n-----------------+---------------------------\nleo-project.net   |   2017 -04-07  10 :07:31 +0900\nlocalhost         |   2017 -04-07  10 :06:18 +0900\ns3.amazonaws.com  |   2017 -04-07  10 :06:18 +0900", 
            "title": "Retrieve a list of endpoints, get-endpoints"
        }, 
        {
            "location": "/admin/system_operations/s3/#user", 
            "text": "To access a LeoFS system, its system's users need to create an account and retrieve  AccessKeyID  and  SecretAccessKey .", 
            "title": "User"
        }, 
        {
            "location": "/admin/system_operations/s3/#create-a-user-create-user", 
            "text": "Creates a user's account so its user can access a LeoFS system.  1\n2\n3\n4\n5\n6 $ leofs-adm create-user  user-id   password  ## Example \n$ leofs-adm create-user leofs-user-1 PASSWORD\naccess-key-id: b5f0413d45855fcc055e\nsecret-access-key: b74f9ae91dd0de81f71e19f8d455a7c081f34c57", 
            "title": "Create a user, create-user"
        }, 
        {
            "location": "/admin/system_operations/s3/#remove-a-user-delete-user", 
            "text": "Removes a user account if a LeoFS system does not need it.  1\n2\n3\n4\n5 $ leofs-adm delete-user  user-id  ## Example \n$ leofs-adm delete-user leofs-user-1\nOK", 
            "title": "Remove a user, delete-user"
        }, 
        {
            "location": "/admin/system_operations/s3/#retrieve-a-list-of-users-get-users", 
            "text": "Retrieves a list of the users if you need to know the existing users.  1\n2\n3\n4\n5\n6\n7\n8 $ leofs-adm get-users ## Example \n$ leofs-adm get-users\nuser_id       |  role_id  |  access_key_id           |  created_at\n-------------+---------+------------------------+---------------------------\n_test_leofs   |   9         |   05236                    |   2017 -04-10  09 :52:39 +0900\nleofs-user-2  |   1         |  b5f0413d45855fcc055e    |   2017 -04-10  09 :54:35 +0900", 
            "title": "Retrieve a list of users, get-users"
        }, 
        {
            "location": "/admin/system_operations/s3/#change-role-of-a-user-update-user-role", 
            "text": "Updates a user's role if you need to change the role of a LeoFS system. You can choose a role from  Administrator  or  General user .     Role Id  Description      1  Administrator    9  General User      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 $ leofs-adm update-user-role  user-id   role-id  ## Example \n$ leofs-adm update-user-role leofs-user-2  9 \nOK ## Confirm the latest list of the users \n$ leofs-adm get-users\nuser_id       |  role_id  |  access_key_id           |  created_at\n-------------+---------+------------------------+---------------------------\n_test_leofs   |   9         |   05236                    |   2017 -04-10  09 :52:39 +0900\nleofs-user    |   1         |  3c1d889cdd2088fb9482    |   2017 -04-10  09 :53:33 +0900\nleofs-user-2  |   9         |  b5f0413d45855fcc055e    |   2017 -04-10  09 :54:35 +0900", 
            "title": "Change role of a user, update-user-role"
        }, 
        {
            "location": "/admin/system_operations/s3/#bucket", 
            "text": "To manage objects under a bucket, you need to create a bucket with  leofs-adm  or any client for Amazon S3 such as s3cmd 2  and DragonDisk 3 .", 
            "title": "Bucket"
        }, 
        {
            "location": "/admin/system_operations/s3/#create-a-bucket-add-bucket", 
            "text": "Creates a bucket so a LeoFS system' users can manage resources under its bucket.  1\n2\n3\n4\n5 $ leofs-adm add-bucket  bucket-name   access-key-id  ## Example \n$ leofs-adm add-bucket backup  05236 \nOK    Note: Creating bucket with the same name as one being removed  It's not possible to create bucket with the same name as one currently being removed. Deletion of all objects belonging to former bucket must complete before bucket with the same name can be created again.", 
            "title": "Create a bucket, add-bucket"
        }, 
        {
            "location": "/admin/system_operations/s3/#remove-a-bucket-delete-bucket", 
            "text": "Removes a bucket if its user does not need it. The bucket name is removed right away, while objects from that bucket will be deleted in asynchronous fashion after that.  1\n2\n3\n4\n5 $ leofs-adm delete-bucket  bucket-name   access-key-id  ## Example \n$ leofs-adm delete-bucket backup  05236 \nOK    Note: How many delete-bucket requests can be proceeded at once  8 delete-bucket requests can be proceeded at once and if there are over 8 ones then exceeded ones are pushed on a queue and will be proceeded later once the number of processing delete-bucket go down under 8.", 
            "title": "Remove a bucket, delete-bucket"
        }, 
        {
            "location": "/admin/system_operations/s3/#retrieve-a-list-of-the-ongoing-delete-buckets-delete-bucket-stats", 
            "text": "Retrieves a list of the ongoing delete-buckets if you need to know the progress of the ongoing delete-buckets.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 $ leofs-adm delete-bucket-stats  bucket-name  ## Example \n$ leofs-adm delete-bucket-stats\n- Bucket: test_2\nnode                          |  state             |  date\n-----------------------------+------------------+-----------------------------\nstorage_3@127.0.0.1           |  pending           |   2017 -07-27  09 :20:47 +0900\nstorage_2@127.0.0.1           |  monitoring        |   2017 -07-27  09 :21:21 +0900\nstorage_1@127.0.0.1           |  monitoring        |   2017 -07-27  09 :21:21 +0900\nstorage_0@127.0.0.1           |  monitoring        |   2017 -07-27  09 :21:22 +0900\n\n- Bucket: test_4\nnode                          |  state             |  date\n-----------------------------+------------------+-----------------------------\nstorage_3@127.0.0.1           |  finished          |   2017 -07-27  09 :21:20 +0900\nstorage_2@127.0.0.1           |  finished          |   2017 -07-27  09 :21:18 +0900\nstorage_1@127.0.0.1           |  finished          |   2017 -07-27  09 :21:21 +0900\nstorage_0@127.0.0.1           |  monitoring        |   2017 -07-27  09 :21:10 +0900 ## Example (No ongoing delete-buckets) \n$ leofs-adm delete-bucket-stats [ ERROR ]  Delete-bucket s stats not found   The below table explains what each state means.     State  Description      pending  Waiting on the queue.    enqueuing  Iterating all objects and enqueuing object identifiers into the queue if those belong to the bucket deleted.    monitoring  Iterating all items in the queue and deleting objects one by one.    finished  Finished delete-bucket and waiting for other storage nodes.      Note: How long the delete-bucket records exist  Once the state of every storage node transit into finished, the delete-bucket records get deleted within several tens of seconds.", 
            "title": "Retrieve a list of the ongoing delete-buckets, delete-bucket-stats"
        }, 
        {
            "location": "/admin/system_operations/s3/#reset-a-delete-bucket-stats-record-reset-delete-bucket-stats", 
            "text": "Reset a delete-bucket-stats record if you face some trouble that inconsistency between manager nodes and storage nodes happened and only way to solve this problem is to reset(delete) a delete-bucket-stats record.  1\n2\n3\n4\n5 $ leofs-adm reset-delete-bucket-stats  bucket-name  ## Example \n$ leofs-adm reset-delete-bucket-stats bucket\nOK    Note: Removing inaccessible objects after  reset-delete-bucket-stats  Since this operation aborts deletion of objects belonging to (already deleted) bucket, it might be a good idea to create and delete bucket with same name again after  reset-delete-bucket-stats , to make sure that bucket deletion completes properly and no stale objects remain.", 
            "title": "Reset a delete-bucket-stats record, reset-delete-bucket-stats"
        }, 
        {
            "location": "/admin/system_operations/s3/#retrieve-a-list-of-buckets-get-buckets", 
            "text": "Retrieves a list of the buckets if you need to know the existing buckets.  1\n2\n3\n4\n5\n6\n7 $ leofs-adm get-buckets ## Example \n$ leofs-adm get-buckets\ncluster id    |  bucket    |  owner        |  permissions       |  redundancy method             |  created at\n-------------+----------+-------------+------------------+------------------------------+---------------------------\nleofs_1       |   test       |  _test_leofs  |  Me ( full_control )   |  copy,  { n:1, w:1, r:1, d:1 }     |   2017 -04-10  10 :57:29 +0900", 
            "title": "Retrieve a list of buckets, get-buckets"
        }, 
        {
            "location": "/admin/system_operations/s3/#retrieve-a-bucket-get-bucket", 
            "text": "Retrieves a bucket if you need to know its bucket's information.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 $ leofs-adm get-bucket  access-key-id  ## Example \n$ leofs-adm get-bucket  05236 \nbucket    |  permissions                             |  created at\n---------+----------------------------------------+---------------------------\nbackup    |  Me ( full_control ) , Everyone ( read )         |   2017 -04-10  10 :39:01 +0900\ndocs      |  Me ( full_control ) , Everyone ( read )         |   2017 -04-10  10 :39:25 +0900\nlogs      |  Me ( full_control ) , Everyone ( read,write )   |   2017 -04-10  10 :39:38 +0900\nmovie     |  Me ( full_control )                         |   2017 -04-10  10 :39:45 +0900", 
            "title": "Retrieve a bucket, get-bucket"
        }, 
        {
            "location": "/admin/system_operations/s3/#change-an-owner-of-a-bucket-chown-bucket", 
            "text": "Updates an owner of a bucket if you need to change its owner to someone else.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 $ leofs-adm chown-bucket  bucket   access-key-id  ## Example \n$ leofs-adm chown-bucket  test  b5f0413d45855fcc055e\nOK ## Confirm the latest list of the buckets \n$ leofs-adm get-buckets\ncluster id    |  bucket    |  owner         |  permissions       |  redundancy method             |  created at\n-------------+----------+--------------+------------------+------------------------------+---------------------------\nleofs_1       |   test       |  leofs-user-2  |  Me ( full_control )   |  copy,  { n:1, w:1, r:1, d:1 }     |   2017 -04-10  10 :57:29 +0900", 
            "title": "Change an owner of a bucket, chown-bucket"
        }, 
        {
            "location": "/admin/system_operations/s3/#update-acl-of-a-bucket-update-acl", 
            "text": "Updates ACL of a bucket if you need to change it.  1\n2\n3\n4\n5\n6\n7\n8\n9 $ leofs-adm update-acl  bucket   access-key-id   canned-ACL  ## Example \n$ leofs-adm update-acl  test   05236  private\nok\n$ leofs-adm update-acl  test   05236  public-read\nok\n$ leofs-adm update-acl  test   05236  public-read-write\nok", 
            "title": "Update ACL of a bucket, update-acl"
        }, 
        {
            "location": "/admin/system_operations/s3/#canned-acl", 
            "text": "When using S3-API, LeoFS supports a set of predefined grants, known as canned ACLs. Each canned ACL has a predefined a set of grantees and permissions. The following table lists the set of canned ACLs and the associated predefined grants.     Canned ACL  Applies To  Permissions Added To ACL      private  Bucket and object  [default]  Owner gets FULL_CONTROL. No one else has access rights.    public-read  Bucket and object  Owner gets FULL_CONTROL. The AllUsers group gets READ access.    public-read-write  Bucket and object  Owner gets FULL_CONTROL. The AllUsers group gets READ and WRITE access. Granting this on a bucket is generally not recommended.", 
            "title": "Canned ACL"
        }, 
        {
            "location": "/admin/system_operations/s3/#related-links", 
            "text": "For Administrators / Interface / S3-API       Amazon S3 API    S3cmd: A command Line S3 Client and Backup for Linux and Mac    DragonDisk: A cloud storage client", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_operations/cluster/", 
            "text": "Cluster Operations\n\n\nPrior Knowledge\n\n\nLeoFS provides the cluster operation features which are implemented on \nleofs-adm\n, LeoFS CLI for administration. LeoFS supports \nnode addition\n and \nnode deletion\n, and already covers as unique features of LeoFS, \nnode suspension\n, \nnode restart\n, and \nnode takeover\n. You can use those functions after starting a LeoFS system.\n\n\nOperations\n\n\nAdd a Node\n\n\nLeoFS temporally adds a node into the member table of LeoManager's database after launching a new LeoStorage node. If you decide to join it in the cluster, you need to execute \nleofs-adm rebalance\n command.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n## Example:\n\n\n## 1. Launch a new LeoStorage node\n\n\n\n## 2. Check the current state of the cluster\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n d5d667a6\n                previous ring-hash \n|\n d5d667a6\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_0@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:20:19 +0900\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:20:19 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:20:19 +0900\n  S    \n|\n storage_3@127.0.0.1      \n|\n attached     \n|\n                \n|\n                \n|\n \n2017\n-04-18 \n18\n:20:37 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:20:21 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n## 3. Execute `rebalance`\n\n$ leofs-adm rebalance\nGenerating rebalance-list...\nGenerated rebalance-list\nDistributing rebalance-list to the storage nodes\nOK  \n25\n% - storage_0@127.0.0.1\nOK  \n50\n% - storage_1@127.0.0.1\nOK  \n75\n% - storage_2@127.0.0.1\nOK \n100\n% - storage_3@127.0.0.1\nOK\n\n\n## 4. Check the latest state of cluster after rebalancing the cluster\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n ce4bece1\n                previous ring-hash \n|\n 3923d007\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_0@127.0.0.1      \n|\n running      \n|\n ce4bece1       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:20:19 +0900\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n ce4bece1       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:20:19 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n ce4bece1       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:20:19 +0900\n  S    \n|\n storage_3@127.0.0.1      \n|\n running      \n|\n ce4bece1       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:21:25 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n ce4bece1       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:20:21 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\nRemove a Node\n\n\nIf you need to shrink a target LeoFS' cluster size, you can realize that by following the operation flow.\n\n\n\n\nDecide to remove a LeoStorage node, whose state must be \nrunning\n or \nstop\n\n\nThen execute \nleofs-adm detach\n command\n\n\nFinally, execute \nleofs-adm rebalance\n command to start rebalancing data in the cluster\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n## Example:\n\n\n## 1. Check the current state of the cluster\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n 3923d007\n                previous ring-hash \n|\n 3923d007\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_0@127.0.0.1      \n|\n running      \n|\n 3923d007       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:31:37 +0900\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n 3923d007       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:31:37 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n 3923d007       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:31:37 +0900\n  S    \n|\n storage_3@127.0.0.1      \n|\n running      \n|\n 3923d007       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:31:37 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n 3923d007       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:31:55 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n## 2. Remove a LeoStorage node\n\n$ leofs-adm detach storage_3@127.0.0.1\nOK\n\n\n## 3. Execute `rebalance`\n\n$ leofs-adm rebalance\nGenerating rebalance-list...\nGenerated rebalance-list\nDistributing rebalance-list to the storage nodes\nOK  \n33\n% - storage_0@127.0.0.1\nOK  \n67\n% - storage_1@127.0.0.1\nOK \n100\n% - storage_2@127.0.0.1\nOK\n\n\n## 3. Check the latest state of cluster after rebalancing the cluster\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n d5d667a6\n                previous ring-hash \n|\n d5d667a6\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_0@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:31:37 +0900\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:31:37 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:31:37 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:31:55 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\nTake Over a Node\n\n\nIf a new LeoStorage node takes over a detached node, you can realize that by following the operation flow.\n\n\n\n\nExecute \nleofs-adm detach\n command to remove a target node in the cluster\n\n\nThen launch a new node to take over the detached node\n\n\nFinally, execute \nleofs-adm reebalance\n command to start rebalancing data in the cluster\n\n\n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n## Example:\n\n\n## 1. Check the current state of the cluster (1)\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n d5d667a6\n                previous ring-hash \n|\n d5d667a6\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_0@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n## 2. Remove a LeoStorage node\n\n$ leofs-adm detach storage_0@127.0.0.1\nOK\n\n\n## 3. Launch a new LeoStorage node\n\n\n\n## 4. Check the current state of the cluster(2)\n\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n d5d667a6\n                previous ring-hash \n|\n d5d667a6\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_0@127.0.0.1      \n|\n detached     \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:56:32 +0900\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_3@127.0.0.1      \n|\n attached     \n|\n                \n|\n                \n|\n \n2017\n-04-18 \n18\n:56:47 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n## 5. Execute `rebalance`\n\n$ leofs-adm rebalance\nGenerating rebalance-list...\nGenerated rebalance-list\nDistributing rebalance-list to the storage nodes\nOK  \n33\n% - storage_2@127.0.0.1\nOK  \n67\n% - storage_3@127.0.0.1\nOK \n100\n% - storage_1@127.0.0.1\nOK\n\n\n## 6. Check the latest state of the cluster\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n c613a468\n                previous ring-hash \n|\n c613a468\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_3@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:58:16 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\nSuspend a Node\n\n\nWhen maintenance of a node is necessary, you can suspend a target node temporally. A suspended node does not receive requests from LeoGateway nodes and LeoStorage nodes. LeoFS eventually distributes the state of the cluster to every node.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n## Example:\n\n\n## 1. Execute `suspend`\n\n$ leofs-adm \nsuspend\n storage_1@127.0.0.1\nOK\n\n\n\n## 2. Check the latest state of the cluster\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n c613a468\n                previous ring-hash \n|\n c613a468\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_1@127.0.0.1      \n|\n \nsuspend\n      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_3@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:58:16 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\nResume a Node\n\n\nAfter suspending a node, if its node restarts and rejoins the cluster, execute \nleofs-adm resume\n command.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n## Example:\n\n\n## 1. Execute `resume`\n\n$ leofs-adm resume storage_1@127.0.0.1\nOK\n\n\n## 2. Check the latest state of the cluster\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n c613a468\n                previous ring-hash \n|\n c613a468\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n19\n:01:48 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_3@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:58:16 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / Cluster Settings", 
            "title": "Cluster Operations"
        }, 
        {
            "location": "/admin/system_operations/cluster/#cluster-operations", 
            "text": "", 
            "title": "Cluster Operations"
        }, 
        {
            "location": "/admin/system_operations/cluster/#prior-knowledge", 
            "text": "LeoFS provides the cluster operation features which are implemented on  leofs-adm , LeoFS CLI for administration. LeoFS supports  node addition  and  node deletion , and already covers as unique features of LeoFS,  node suspension ,  node restart , and  node takeover . You can use those functions after starting a LeoFS system.", 
            "title": "Prior Knowledge"
        }, 
        {
            "location": "/admin/system_operations/cluster/#operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/admin/system_operations/cluster/#add-a-node", 
            "text": "LeoFS temporally adds a node into the member table of LeoManager's database after launching a new LeoStorage node. If you decide to join it in the cluster, you need to execute  leofs-adm rebalance  command.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93 ## Example:  ## 1. Launch a new LeoStorage node  ## 2. Check the current state of the cluster \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  d5d667a6\n                previous ring-hash  |  d5d667a6\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_0@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :20:19 +0900\n  S     |  storage_1@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :20:19 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :20:19 +0900\n  S     |  storage_3@127.0.0.1       |  attached      |                  |                  |   2017 -04-18  18 :20:37 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :20:21 +0900\n-------+--------------------------+--------------+----------------+----------------+---------------------------- ## 3. Execute `rebalance` \n$ leofs-adm rebalance\nGenerating rebalance-list...\nGenerated rebalance-list\nDistributing rebalance-list to the storage nodes\nOK   25 % - storage_0@127.0.0.1\nOK   50 % - storage_1@127.0.0.1\nOK   75 % - storage_2@127.0.0.1\nOK  100 % - storage_3@127.0.0.1\nOK ## 4. Check the latest state of cluster after rebalancing the cluster \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  ce4bece1\n                previous ring-hash  |  3923d007\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_0@127.0.0.1       |  running       |  ce4bece1        |  3923d007        |   2017 -04-18  18 :20:19 +0900\n  S     |  storage_1@127.0.0.1       |  running       |  ce4bece1        |  3923d007        |   2017 -04-18  18 :20:19 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  ce4bece1        |  3923d007        |   2017 -04-18  18 :20:19 +0900\n  S     |  storage_3@127.0.0.1       |  running       |  ce4bece1        |  3923d007        |   2017 -04-18  18 :21:25 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  ce4bece1        |  3923d007        |   2017 -04-18  18 :20:21 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Add a Node"
        }, 
        {
            "location": "/admin/system_operations/cluster/#remove-a-node", 
            "text": "If you need to shrink a target LeoFS' cluster size, you can realize that by following the operation flow.   Decide to remove a LeoStorage node, whose state must be  running  or  stop  Then execute  leofs-adm detach  command  Finally, execute  leofs-adm rebalance  command to start rebalancing data in the cluster    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93 ## Example:  ## 1. Check the current state of the cluster \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  3923d007\n                previous ring-hash  |  3923d007\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_0@127.0.0.1       |  running       |  3923d007        |  3923d007        |   2017 -04-18  18 :31:37 +0900\n  S     |  storage_1@127.0.0.1       |  running       |  3923d007        |  3923d007        |   2017 -04-18  18 :31:37 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  3923d007        |  3923d007        |   2017 -04-18  18 :31:37 +0900\n  S     |  storage_3@127.0.0.1       |  running       |  3923d007        |  3923d007        |   2017 -04-18  18 :31:37 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  3923d007        |  3923d007        |   2017 -04-18  18 :31:55 +0900\n-------+--------------------------+--------------+----------------+----------------+---------------------------- ## 2. Remove a LeoStorage node \n$ leofs-adm detach storage_3@127.0.0.1\nOK ## 3. Execute `rebalance` \n$ leofs-adm rebalance\nGenerating rebalance-list...\nGenerated rebalance-list\nDistributing rebalance-list to the storage nodes\nOK   33 % - storage_0@127.0.0.1\nOK   67 % - storage_1@127.0.0.1\nOK  100 % - storage_2@127.0.0.1\nOK ## 3. Check the latest state of cluster after rebalancing the cluster \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  d5d667a6\n                previous ring-hash  |  d5d667a6\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_0@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :31:37 +0900\n  S     |  storage_1@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :31:37 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :31:37 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :31:55 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Remove a Node"
        }, 
        {
            "location": "/admin/system_operations/cluster/#take-over-a-node", 
            "text": "If a new LeoStorage node takes over a detached node, you can realize that by following the operation flow.   Execute  leofs-adm detach  command to remove a target node in the cluster  Then launch a new node to take over the detached node  Finally, execute  leofs-adm reebalance  command to start rebalancing data in the cluster     1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135 ## Example:  ## 1. Check the current state of the cluster (1) \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  d5d667a6\n                previous ring-hash  |  d5d667a6\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_0@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_1@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :55:35 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+---------------------------- ## 2. Remove a LeoStorage node \n$ leofs-adm detach storage_0@127.0.0.1\nOK ## 3. Launch a new LeoStorage node  ## 4. Check the current state of the cluster(2) \n\n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  d5d667a6\n                previous ring-hash  |  d5d667a6\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_0@127.0.0.1       |  detached      |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :56:32 +0900\n  S     |  storage_1@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_3@127.0.0.1       |  attached      |                  |                  |   2017 -04-18  18 :56:47 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+---------------------------- ## 5. Execute `rebalance` \n$ leofs-adm rebalance\nGenerating rebalance-list...\nGenerated rebalance-list\nDistributing rebalance-list to the storage nodes\nOK   33 % - storage_2@127.0.0.1\nOK   67 % - storage_3@127.0.0.1\nOK  100 % - storage_1@127.0.0.1\nOK ## 6. Check the latest state of the cluster \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  c613a468\n                previous ring-hash  |  c613a468\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_1@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_3@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :58:16 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Take Over a Node"
        }, 
        {
            "location": "/admin/system_operations/cluster/#suspend-a-node", 
            "text": "When maintenance of a node is necessary, you can suspend a target node temporally. A suspended node does not receive requests from LeoGateway nodes and LeoStorage nodes. LeoFS eventually distributes the state of the cluster to every node.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44 ## Example:  ## 1. Execute `suspend` \n$ leofs-adm  suspend  storage_1@127.0.0.1\nOK ## 2. Check the latest state of the cluster \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  c613a468\n                previous ring-hash  |  c613a468\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_1@127.0.0.1       |   suspend        |  c613a468        |  c613a468        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_3@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :58:16 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Suspend a Node"
        }, 
        {
            "location": "/admin/system_operations/cluster/#resume-a-node", 
            "text": "After suspending a node, if its node restarts and rejoins the cluster, execute  leofs-adm resume  command.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43 ## Example:  ## 1. Execute `resume` \n$ leofs-adm resume storage_1@127.0.0.1\nOK ## 2. Check the latest state of the cluster \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  c613a468\n                previous ring-hash  |  c613a468\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_1@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  19 :01:48 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_3@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :58:16 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Resume a Node"
        }, 
        {
            "location": "/admin/system_operations/cluster/#related-links", 
            "text": "For Administrators / Settings / Cluster Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_operations/data/", 
            "text": "Data Operations\n\n\nThis section provides information to operate data stored in LeoFS as well as the prior knowledge that gives you the better understanding about what will happen behind the scene when you invoke a command for a data operation.\n\n\nStorage Architecture\n\n\nA brief introduction how LeoFS organize data into actual OS files.\n\n\nAppend-Only for Content\n\n\nOnce a PUT/DELETE remote procedure call, \nRPC\n arrives on LeoStorage appends new blocks including the object information such as the key, data itself and also various associated metadata to the end of a file.\n\n\nThis \nAppend-Only-File\n we call \nAVS, Aria Vector Storage\n which is referenced when retrieving the data through \nGET RPCs\n. With its inherent nature of the \nAppend-Only-File\n, a data-compaction process is needed to clean up the orphaned space in an AVS.\n\n\nEmbedded KVS for Metadata\n\n\nAfter having succeeded in appending new blocks to an AVS, then leo_object_storage updates a corresponding \nKVS\n file with the key and its associated metadata \n(same information stored in the AVS except for the data itself)\n and its offset where the new blocks stored in AVS as a value. This KVS file is referenced when retrieving the data/metadata through GET/HEAD RPCs.\n\n\nMessage Queue for Async Operations\n\n\nSome data can be stored into a \nQueue\n for processing later in the case\n\n\n\n\nA PUT/DELETE operation failed\n\n\nA Multi DC Replication, \nMDCR\n failed\n\n\nrebalance/recover-(file|node|cluster)\n invoked through leofs-adm\n\n\n\n\nFile Structure\n\n\nMultiple AVS/KVS pairs can be placed on one node to enable LeoFS handling as much use cases and hardware requirements as possible. See \nConcept and Architecture / LeoStorage's Architecture - Data Structure\n.\n\n\n\n\nContainer : AVS/KVS pair = 1 : N\n\n\nMultiple AVS/KVS pairs can be stored under one OS directory. It is called \nContainer\n.\n\n\n'N' can be specified through \nleo_storage.conf\n.\n\n\nHow to choose optimal 'N'\n\n\nAs a data-compaction is executed per AVS/KVS pair, at least the size of a AVS/KVS pair is needed to run data-compaction so that the larger 'N', the less disk space LeoFS uses for data-compaction.\n\n\nHowever the larger N, the more disk seeks LeoFS suffers.\n\n\nTha said, the optimal N is determined by setting the largest value that doesn't affect the online throughput you would expect.\n\n\n\n\n\n\n\n\n\n\nNode : Container = 1 : N\n\n\nEach Container can be stored under a different OS directory.\n\n\nN can be specified through leo_storage.conf.\n\n\nSetting \nN \n 1\n can be useful when there are multiple JBOD disks on the node. The one JBOD disk array can be map to the one container.\n\n\n\n\n\n\n\n\nData Compaction\n\n\nThis section provides information about what/how data-compaction can affect the online performance.\n\n\n\n\nParallelism\n\n\nA data-compaction can be executed across multiple AVS/KVS pairs in parallel.\n\n\nThe number of data-compaction processes can be specified through an argument of leofs-adm.\n\n\nIncreasing the number can be useful when the load coming from online is relatively low and want a data-compaction process to boost as much as possible.\n\n\nBe careful that too much number can make a data-compaction process slow down.\n\n\n\n\n\n\nConcurrent with any operation coming from online.\n\n\nGET/HEAD never be blocked by a data-compaction.\n\n\nPUT/DELETE can be blocked while a data-compaction is processing the tail part of an AVS.\n\n\nGiven that the above limitation, We would recommend suspending a node you are supposed to run a data-compaction if a LeoFS system's cluster handles write intensive workload.\n\n\n\n\n\n\n\n\nHow To Operate Data Compaction\n\n\nAs described in the previous section, A compaction process is needed to remove logically deleted objects and its corresponding metadata.\n\n\nState Transition\n\n\n\n\nCommands\n\n\nCommands related to Compaction as well as Disk Usage.\n\n\n\n\n\n\n\n\nShell\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCompaction Commands\n\n\n\n\n\n\n\n\nleofs-adm compact-start \nstorage-node\n (all/\nnum-of-targets\n) [\nnum-of-compaction-proc\n]\n\n\nStart Compaction \n(Transfer its state to \nrunning\n)\n.\nnum-of-targets\n: How many AVS/KVS pairs are compacted.\nnum-of-compaction-pro\n: How many processes are run in parallel.\n\n\n\n\n\n\nleofs-adm compact-suspend \nstorage-node\n\n\nSuspend Compaction \n(Transfer its state to 'suspend' from running)\n.\n\n\n\n\n\n\nleofs-adm compact-resume \nstorage-node\n\n\nResume Compaction \n(Transfer its state to 'running' from suspend)\n.\n\n\n\n\n\n\nleofs-adm compact-status \nstorage-node\n\n\nSee the Current Compaction Status.\n\n\n\n\n\n\nleofs-adm diagnose-start \nstorage-node\n\n\nStart Diagnose \n(Not actually doing Compaction but scanning all AVS/KVS pairs and reporting what objects/metadatas exist as a file)\n.\n\n\n\n\n\n\nDisk Usage\n\n\n\n\n\n\n\n\nleofs-adm du \nstorage-node\n\n\nSee the Current Disk Usage.\n\n\n\n\n\n\nleofs-adm du detail \nstorage-node\n\n\nSee the Current Disk Usage in detail.\n\n\n\n\n\n\n\n\ncompact-start\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n## Note:\n\n\n##   All AVS/KVS pairs on storage_0@127.0.0.1\n\n\n##   will be compacted with 3 concurrent processes (default concurrency is 3)\n\n\n## Example:\n\n$ leofs-adm compact-start storage_0@127.0.0.1 all\nOK\n\n\n## Note:\n\n\n##   Five AVS/KVS pairs on storage_0@127.0.0.1\n\n\n##   will be compacted with 2 concurrent processes\n\n$ leofs-adm compact-start storage_0@127.0.0.1 \n5\n \n2\n\nOK\n\n\n\n\n\n\ncompact-suspend\n\n\n1\n2\n3\n## Example:\n\n$ leofs-adm compact-suspend storage_0@127.0.0.1\nOK\n\n\n\n\n\n\ncompact-resume\n\n\n1\n2\n3\n## Example:\n\n$ leofs-adm compact-resume storage_0@127.0.0.1\nOK\n\n\n\n\n\n\ncompact-status\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## Example:\n\n$ leofs-adm compact-status storage_0@127.0.0.1\n        current status: running\n last compaction start: \n2013\n-03-04 \n12\n:39:47 +0900\n         total targets: \n64\n\n  \n# of pending targets: 5\n\n  \n# of ongoing targets: 3\n\n  \n# of out of targets : 56\n\n\n\n\n\n\n\ndiagnose-start\n\n\nSee also \ndiagnosis-log format\n to understand the output log format.\n\n\n1\n2\n3\n## Example:\n\n$ leofs-adm diagnose-start storage_0@127.0.0.1\nOK\n\n\n\n\n\n\ndu\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## Example:\n\n$ leofs-adm du storage_0@127.0.0.1\n active number of objects: \n19968\n\n  total number of objects: \n39936\n\n   active size of objects: \n198256974\n.0\n    total size of objects: \n254725020\n.0\n     ratio of active size: \n77\n.83%\n    last compaction start: \n2013\n-03-04 \n12\n:39:47 +0900\n      last compaction end: \n2013\n-03-04 \n12\n:39:55 +0900\n\n\n\n\n\n\ndu detail\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n## Example:\n\n$ leofs-adm du detail storage_0@127.0.0.1\n\n[\ndu\n(\nstorage stats\n)]\n\n                file path: /home/leofs/dev/leofs/package/leofs/storage/avs/object/0.avs\n active number of objects: \n320\n\n  total number of objects: \n640\n\n   active size of objects: \n3206378\n.0\n    total size of objects: \n4082036\n.0\n     ratio of active size: \n78\n.55%\n    last compaction start: \n2013\n-03-04 \n12\n:39:47 +0900\n      last compaction end: \n2013\n-03-04 \n12\n:39:55 +0900\n.\n.\n.\n                file path: /home/leofs/dev/leofs/package/leofs/storage/avs/object/63.avs\n active number of objects: \n293\n\n  total number of objects: \n586\n\n   active size of objects: \n2968909\n.0\n    total size of objects: \n3737690\n.0\n     ratio of active size: \n79\n.43%\n    last compaction start: ____-__-__ __:__:__\n      last compaction end: ____-__-__ __:__:__\n\n\n\n\n\n\nDiagnosis\n\n\nThis section explains the file format generated by \nleofs-adm diagnose-start\n in detail.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n## Example:\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\nOffset| RING\ns address-id                        | Filename                                                   | Child num | File Size  | Unixtime         | Localtime                |del?\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\n194     296754181484029444656944009564610621293   photo/leo_redundant_manager/Makefile                             0       2034        1413348050768344   2014-10-15 13:40:50 +0900   0\n2400    185993533055981727582172380494809056426   photo/leo_redundant_manager/ebin/leo_redundant_manager.beam      0       24396       1413348050869454   2014-10-15 13:40:50 +0900   0\n38446   53208912738248114804281793572563205919    photo/leo_rpc/.git/refs/remotes/origin/HEAD                      0       33          1413348057441546   2014-10-15 13:40:57 +0900   0\n38658   57520977797167422772945547576980778561    photo/leo_rpc/ebin/leo_rpc_client_utils.beam                     0       2576        1413348057512261   2014-10-15 13:40:57 +0900   0\n69506   187294034498591995039607573685274229706   photo/leo_backend_db/src/leo_backend_db_server.erl               0       13911       1413348068031188   2014-10-15 13:41:08 +0900   0\n83603   316467020376888598364250682951088839795   photo/leo_backend_db/test/leo_backend_db_api_prop.erl            0       3507        1413348068052219   2014-10-15 13:41:08 +0900   1\n\n\n\n\n\n\nThe file is formatted as Tab Separated Values \n(TSV)\n except headers \n(head three lines of a file)\n. The detail of each column are described below:\n\n\n\n\n\n\n\n\nColumn Number\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nbyte-wise Offset where the object is located in an AVS.\n\n\n\n\n\n\n2\n\n\nAddress ID on RING \n(Distribute Hash Routing Table)\n.\n\n\n\n\n\n\n3\n\n\nFile Name.\n\n\n\n\n\n\n4\n\n\nThe Number of Children in a File.\n\n\n\n\n\n\n5\n\n\nFile Size in bytes.\n\n\n\n\n\n\n6\n\n\nTimestamp in Unix Time.\n\n\n\n\n\n\n7\n\n\nTimestamp in Local Time.\n\n\n\n\n\n\n8\n\n\nFlag \n(0/1)\n representing whether the object is removed.\n\n\n\n\n\n\n\n\nRecover Objects\n\n\nThis section provides information about the recovery commands that can be used in order to recover inconsistencies in a LeoFS cluster according to failures.\n\n\nCommands\n\n\n\n\n\n\n\n\nShell\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nleofs-adm recover-file \nfile-path\n\n\nRecover the inconsistent object specified by the file-path.\n\n\n\n\n\n\nleofs-adm recover-disk \nstorage-node\n \ndisk-id\n\n\nRecover all inconsistent objects on the specified disk in the specified storage-node. \nNote that this command can be used ONLY in case all LeoStorage have the same obj_containers configuration.\n\n\n\n\n\n\nleofs-adm recover-ring \nstorage-node\n\n\nRecover \nRING, a routing table\n of the specified node of the local cluster\n\n\n\n\n\n\nleofs-adm recover-node \nstorage-node\n\n\nRecover all inconsistent objects in the specified storage-node.\n\n\n\n\n\n\nleofs-adm recover-cluster \nremote-cluster-id\n\n\nRecover \nall inconsistent objects in the specified remote cluster\n \n(NOT the local cluster)\n in case of using \nthe multi datacenter replication\n.\n\n\n\n\n\n\n\n\nrecover-file\n\n\n1\n2\n3\n## Example:\n\n$ leofs-adm recover-file leo/fast/storage.key\nOK\n\n\n\n\n\n\nrecover-disk\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n## Note:\n\n\n##   If you have the following configuration in leo_storage.conf\n\n\n##   obj_containers.path = [./avs1,./avs2]\n\n\n##   then the below command will recover files stored under ./avs1\n\n\n## Example:\n\n$ leofs-adm recover-disk storage_0@127.0.0.1 \n1\n\nOK\n\n\n## Note:\n\n\n##  If you want to recover files stored under ./avs2\n\n\n##  then issue the below one.\n\n\n## Example:\n\n$ leofs-adm recover-disk storage_0@127.0.0.1 \n2\n\nOK\n\n\n\n\n\n\nrecover-node\n\n\n1\n2\n3\n## Example:\n\n$ leofs-adm recover-node storage_0@127.0.0.1\nOK\n\n\n\n\n\n\nrecover-cluster\n\n\n1\n2\n3\n4\n5\n6\n## Note:\n\n\n##   If your LeoFS already uses the multi data center replication,\n\n\n##   you can execute this command.\n\n\n## Example:\n\n$ leofs-adm recover-cluster remote-leofs\nOK\n\n\n\n\n\n\nUse Cases\n\n\nWhen/How to use recover commands.\n\n\n\n\nAVS/KVS Broken\n\n\nInvoke \nrecover-node\n with a node having broken AVS/KVS files or \nrecover-disk\n with a disk having broken AVS/KVS files if you have multiple container directories.\n\n\n\n\n\n\nQueue Broken\n\n\nInvoke \nrecover-node\n with every node except which having broken Queue files.\n\n\nThe procedure might be improved in future when \nissue#618\n solved.\n\n\n\n\n\n\nDisk Broken\n\n\nInvoke \nsuspend\n with a node having broken Disk arrays and subsequently run \nleo_storage stop\n.\n\n\nExchange broken Disk arrays.\n\n\nRun \nleo_storage start\n and subsequently Invoke \nresume\n with the node.\n\n\nInvoke \nrecover-node\n with the node or \nrecover-disk\n with the broken disk if you have multiple container directories.\n\n\n\n\n\n\nNode Broken\n\n\nInvoke \ndetach\n with a broken node.\n\n\nPrepare a new node that will take over all objects assigned to a detached node.\n\n\nInvoke \nrebalance\n.\n\n\n\n\n\n\nSource/Destination Cluster Down\n\n\nInvoke \nrecover-cluster\n with a downed cluster.\n\n\n\n\n\n\nSource/Destination Cluster Down and delete operations on the other side got lost \n(compacted)\n.\n\n\nSet up the cluster from scratch\n\n\ninvoke \nrecover-cluster\n with the new cluster\n\n\nSee also \nissue#636\n for more information.\n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nConcept and Architecture / LeoStorage's Architecture", 
            "title": "Data Operations"
        }, 
        {
            "location": "/admin/system_operations/data/#data-operations", 
            "text": "This section provides information to operate data stored in LeoFS as well as the prior knowledge that gives you the better understanding about what will happen behind the scene when you invoke a command for a data operation.", 
            "title": "Data Operations"
        }, 
        {
            "location": "/admin/system_operations/data/#storage-architecture", 
            "text": "A brief introduction how LeoFS organize data into actual OS files.", 
            "title": "Storage Architecture"
        }, 
        {
            "location": "/admin/system_operations/data/#append-only-for-content", 
            "text": "Once a PUT/DELETE remote procedure call,  RPC  arrives on LeoStorage appends new blocks including the object information such as the key, data itself and also various associated metadata to the end of a file.  This  Append-Only-File  we call  AVS, Aria Vector Storage  which is referenced when retrieving the data through  GET RPCs . With its inherent nature of the  Append-Only-File , a data-compaction process is needed to clean up the orphaned space in an AVS.", 
            "title": "Append-Only for Content"
        }, 
        {
            "location": "/admin/system_operations/data/#embedded-kvs-for-metadata", 
            "text": "After having succeeded in appending new blocks to an AVS, then leo_object_storage updates a corresponding  KVS  file with the key and its associated metadata  (same information stored in the AVS except for the data itself)  and its offset where the new blocks stored in AVS as a value. This KVS file is referenced when retrieving the data/metadata through GET/HEAD RPCs.", 
            "title": "Embedded KVS for Metadata"
        }, 
        {
            "location": "/admin/system_operations/data/#message-queue-for-async-operations", 
            "text": "Some data can be stored into a  Queue  for processing later in the case   A PUT/DELETE operation failed  A Multi DC Replication,  MDCR  failed  rebalance/recover-(file|node|cluster)  invoked through leofs-adm", 
            "title": "Message Queue for Async Operations"
        }, 
        {
            "location": "/admin/system_operations/data/#file-structure", 
            "text": "Multiple AVS/KVS pairs can be placed on one node to enable LeoFS handling as much use cases and hardware requirements as possible. See  Concept and Architecture / LeoStorage's Architecture - Data Structure .   Container : AVS/KVS pair = 1 : N  Multiple AVS/KVS pairs can be stored under one OS directory. It is called  Container .  'N' can be specified through  leo_storage.conf .  How to choose optimal 'N'  As a data-compaction is executed per AVS/KVS pair, at least the size of a AVS/KVS pair is needed to run data-compaction so that the larger 'N', the less disk space LeoFS uses for data-compaction.  However the larger N, the more disk seeks LeoFS suffers.  Tha said, the optimal N is determined by setting the largest value that doesn't affect the online throughput you would expect.      Node : Container = 1 : N  Each Container can be stored under a different OS directory.  N can be specified through leo_storage.conf.  Setting  N   1  can be useful when there are multiple JBOD disks on the node. The one JBOD disk array can be map to the one container.", 
            "title": "File Structure"
        }, 
        {
            "location": "/admin/system_operations/data/#data-compaction", 
            "text": "This section provides information about what/how data-compaction can affect the online performance.   Parallelism  A data-compaction can be executed across multiple AVS/KVS pairs in parallel.  The number of data-compaction processes can be specified through an argument of leofs-adm.  Increasing the number can be useful when the load coming from online is relatively low and want a data-compaction process to boost as much as possible.  Be careful that too much number can make a data-compaction process slow down.    Concurrent with any operation coming from online.  GET/HEAD never be blocked by a data-compaction.  PUT/DELETE can be blocked while a data-compaction is processing the tail part of an AVS.  Given that the above limitation, We would recommend suspending a node you are supposed to run a data-compaction if a LeoFS system's cluster handles write intensive workload.", 
            "title": "Data Compaction"
        }, 
        {
            "location": "/admin/system_operations/data/#how-to-operate-data-compaction", 
            "text": "As described in the previous section, A compaction process is needed to remove logically deleted objects and its corresponding metadata.", 
            "title": "How To Operate Data Compaction"
        }, 
        {
            "location": "/admin/system_operations/data/#state-transition", 
            "text": "", 
            "title": "State Transition"
        }, 
        {
            "location": "/admin/system_operations/data/#commands", 
            "text": "Commands related to Compaction as well as Disk Usage.     Shell  Description      Compaction Commands     leofs-adm compact-start  storage-node  (all/ num-of-targets ) [ num-of-compaction-proc ]  Start Compaction  (Transfer its state to  running ) . num-of-targets : How many AVS/KVS pairs are compacted. num-of-compaction-pro : How many processes are run in parallel.    leofs-adm compact-suspend  storage-node  Suspend Compaction  (Transfer its state to 'suspend' from running) .    leofs-adm compact-resume  storage-node  Resume Compaction  (Transfer its state to 'running' from suspend) .    leofs-adm compact-status  storage-node  See the Current Compaction Status.    leofs-adm diagnose-start  storage-node  Start Diagnose  (Not actually doing Compaction but scanning all AVS/KVS pairs and reporting what objects/metadatas exist as a file) .    Disk Usage     leofs-adm du  storage-node  See the Current Disk Usage.    leofs-adm du detail  storage-node  See the Current Disk Usage in detail.", 
            "title": "Commands"
        }, 
        {
            "location": "/admin/system_operations/data/#compact-start", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 ## Note:  ##   All AVS/KVS pairs on storage_0@127.0.0.1  ##   will be compacted with 3 concurrent processes (default concurrency is 3)  ## Example: \n$ leofs-adm compact-start storage_0@127.0.0.1 all\nOK ## Note:  ##   Five AVS/KVS pairs on storage_0@127.0.0.1  ##   will be compacted with 2 concurrent processes \n$ leofs-adm compact-start storage_0@127.0.0.1  5   2 \nOK", 
            "title": "compact-start"
        }, 
        {
            "location": "/admin/system_operations/data/#compact-suspend", 
            "text": "1\n2\n3 ## Example: \n$ leofs-adm compact-suspend storage_0@127.0.0.1\nOK", 
            "title": "compact-suspend"
        }, 
        {
            "location": "/admin/system_operations/data/#compact-resume", 
            "text": "1\n2\n3 ## Example: \n$ leofs-adm compact-resume storage_0@127.0.0.1\nOK", 
            "title": "compact-resume"
        }, 
        {
            "location": "/admin/system_operations/data/#compact-status", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8 ## Example: \n$ leofs-adm compact-status storage_0@127.0.0.1\n        current status: running\n last compaction start:  2013 -03-04  12 :39:47 +0900\n         total targets:  64 \n   # of pending targets: 5 \n   # of ongoing targets: 3 \n   # of out of targets : 56", 
            "title": "compact-status"
        }, 
        {
            "location": "/admin/system_operations/data/#diagnose-start", 
            "text": "See also  diagnosis-log format  to understand the output log format.  1\n2\n3 ## Example: \n$ leofs-adm diagnose-start storage_0@127.0.0.1\nOK", 
            "title": "diagnose-start"
        }, 
        {
            "location": "/admin/system_operations/data/#du", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 ## Example: \n$ leofs-adm du storage_0@127.0.0.1\n active number of objects:  19968 \n  total number of objects:  39936 \n   active size of objects:  198256974 .0\n    total size of objects:  254725020 .0\n     ratio of active size:  77 .83%\n    last compaction start:  2013 -03-04  12 :39:47 +0900\n      last compaction end:  2013 -03-04  12 :39:55 +0900", 
            "title": "du"
        }, 
        {
            "location": "/admin/system_operations/data/#du-detail", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 ## Example: \n$ leofs-adm du detail storage_0@127.0.0.1 [ du ( storage stats )] \n                file path: /home/leofs/dev/leofs/package/leofs/storage/avs/object/0.avs\n active number of objects:  320 \n  total number of objects:  640 \n   active size of objects:  3206378 .0\n    total size of objects:  4082036 .0\n     ratio of active size:  78 .55%\n    last compaction start:  2013 -03-04  12 :39:47 +0900\n      last compaction end:  2013 -03-04  12 :39:55 +0900\n.\n.\n.\n                file path: /home/leofs/dev/leofs/package/leofs/storage/avs/object/63.avs\n active number of objects:  293 \n  total number of objects:  586 \n   active size of objects:  2968909 .0\n    total size of objects:  3737690 .0\n     ratio of active size:  79 .43%\n    last compaction start: ____-__-__ __:__:__\n      last compaction end: ____-__-__ __:__:__", 
            "title": "du detail"
        }, 
        {
            "location": "/admin/system_operations/data/#diagnosis", 
            "text": "This section explains the file format generated by  leofs-adm diagnose-start  in detail.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 ## Example:\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\nOffset| RING s address-id                        | Filename                                                   | Child num | File Size  | Unixtime         | Localtime                |del?\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\n194     296754181484029444656944009564610621293   photo/leo_redundant_manager/Makefile                             0       2034        1413348050768344   2014-10-15 13:40:50 +0900   0\n2400    185993533055981727582172380494809056426   photo/leo_redundant_manager/ebin/leo_redundant_manager.beam      0       24396       1413348050869454   2014-10-15 13:40:50 +0900   0\n38446   53208912738248114804281793572563205919    photo/leo_rpc/.git/refs/remotes/origin/HEAD                      0       33          1413348057441546   2014-10-15 13:40:57 +0900   0\n38658   57520977797167422772945547576980778561    photo/leo_rpc/ebin/leo_rpc_client_utils.beam                     0       2576        1413348057512261   2014-10-15 13:40:57 +0900   0\n69506   187294034498591995039607573685274229706   photo/leo_backend_db/src/leo_backend_db_server.erl               0       13911       1413348068031188   2014-10-15 13:41:08 +0900   0\n83603   316467020376888598364250682951088839795   photo/leo_backend_db/test/leo_backend_db_api_prop.erl            0       3507        1413348068052219   2014-10-15 13:41:08 +0900   1   The file is formatted as Tab Separated Values  (TSV)  except headers  (head three lines of a file) . The detail of each column are described below:     Column Number  Description      1  byte-wise Offset where the object is located in an AVS.    2  Address ID on RING  (Distribute Hash Routing Table) .    3  File Name.    4  The Number of Children in a File.    5  File Size in bytes.    6  Timestamp in Unix Time.    7  Timestamp in Local Time.    8  Flag  (0/1)  representing whether the object is removed.", 
            "title": "Diagnosis"
        }, 
        {
            "location": "/admin/system_operations/data/#recover-objects", 
            "text": "This section provides information about the recovery commands that can be used in order to recover inconsistencies in a LeoFS cluster according to failures.", 
            "title": "Recover Objects"
        }, 
        {
            "location": "/admin/system_operations/data/#commands_1", 
            "text": "Shell  Description      leofs-adm recover-file  file-path  Recover the inconsistent object specified by the file-path.    leofs-adm recover-disk  storage-node   disk-id  Recover all inconsistent objects on the specified disk in the specified storage-node.  Note that this command can be used ONLY in case all LeoStorage have the same obj_containers configuration.    leofs-adm recover-ring  storage-node  Recover  RING, a routing table  of the specified node of the local cluster    leofs-adm recover-node  storage-node  Recover all inconsistent objects in the specified storage-node.    leofs-adm recover-cluster  remote-cluster-id  Recover  all inconsistent objects in the specified remote cluster   (NOT the local cluster)  in case of using  the multi datacenter replication .", 
            "title": "Commands"
        }, 
        {
            "location": "/admin/system_operations/data/#recover-file", 
            "text": "1\n2\n3 ## Example: \n$ leofs-adm recover-file leo/fast/storage.key\nOK", 
            "title": "recover-file"
        }, 
        {
            "location": "/admin/system_operations/data/#recover-disk", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 ## Note:  ##   If you have the following configuration in leo_storage.conf  ##   obj_containers.path = [./avs1,./avs2]  ##   then the below command will recover files stored under ./avs1  ## Example: \n$ leofs-adm recover-disk storage_0@127.0.0.1  1 \nOK ## Note:  ##  If you want to recover files stored under ./avs2  ##  then issue the below one.  ## Example: \n$ leofs-adm recover-disk storage_0@127.0.0.1  2 \nOK", 
            "title": "recover-disk"
        }, 
        {
            "location": "/admin/system_operations/data/#recover-node", 
            "text": "1\n2\n3 ## Example: \n$ leofs-adm recover-node storage_0@127.0.0.1\nOK", 
            "title": "recover-node"
        }, 
        {
            "location": "/admin/system_operations/data/#recover-cluster", 
            "text": "1\n2\n3\n4\n5\n6 ## Note:  ##   If your LeoFS already uses the multi data center replication,  ##   you can execute this command.  ## Example: \n$ leofs-adm recover-cluster remote-leofs\nOK", 
            "title": "recover-cluster"
        }, 
        {
            "location": "/admin/system_operations/data/#use-cases", 
            "text": "When/How to use recover commands.   AVS/KVS Broken  Invoke  recover-node  with a node having broken AVS/KVS files or  recover-disk  with a disk having broken AVS/KVS files if you have multiple container directories.    Queue Broken  Invoke  recover-node  with every node except which having broken Queue files.  The procedure might be improved in future when  issue#618  solved.    Disk Broken  Invoke  suspend  with a node having broken Disk arrays and subsequently run  leo_storage stop .  Exchange broken Disk arrays.  Run  leo_storage start  and subsequently Invoke  resume  with the node.  Invoke  recover-node  with the node or  recover-disk  with the broken disk if you have multiple container directories.    Node Broken  Invoke  detach  with a broken node.  Prepare a new node that will take over all objects assigned to a detached node.  Invoke  rebalance .    Source/Destination Cluster Down  Invoke  recover-cluster  with a downed cluster.    Source/Destination Cluster Down and delete operations on the other side got lost  (compacted) .  Set up the cluster from scratch  invoke  recover-cluster  with the new cluster  See also  issue#636  for more information.", 
            "title": "Use Cases"
        }, 
        {
            "location": "/admin/system_operations/data/#related-links", 
            "text": "Concept and Architecture / LeoStorage's Architecture", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/", 
            "text": "Multi Data Center Replication\n\n\nConfiguration\n\n\nLeoFS provides the multi data center replication related configuration items, which contain \nleo_manager_0.conf\n. Modify those configuration items to work its feature correctly.\n\n\nLeoManager Master's Configuration For MDCR\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmdc_replication.num_of_replicas_a_dc\n\n\nA remote cluster of a LeoFS system which receives this cluster's objects, and then replicates them, which adhere to a replication method of each object\n\n\n\n\n\n\nmdc_replication.consistency.write\n\n\nA number of replicas needed for a successful WRITE-operation\n\n\n\n\n\n\nmdc_replication.consistency.read\n\n\nA number of replicas needed for a successful READ-operation\n\n\n\n\n\n\nmdc_replication.consistency.delete\n\n\nA number of replicas needed for a successful DELETE-operation\n\n\n\n\n\n\n\n\nHow To Operate Multi Data Center Replication\n\n\nCommands\n\n\nThere are three commands, \njoin-cluster\n, \nremove-cluster\n, and \ncluster-status\n. You can control the multi data center repliation by LeoFS' CUI, \nleofs-adm\n.\n\n\n\n\n\n\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njoin-cluster \nRMM\n \nRMS\n\n\nBegin to communicate between the local cluster and the remote cluster\n\n\n\n\n\n\nremove-cluster \nRMM\n \nRMS\n\n\nTerminate to communicate between the local cluster and the remote cluster\n\n\n\n\n\n\ncluster-status\n\n\nSee the current state of cluster(s)\n\n\n\n\n\n\n\n\n\n\nRMM\n: A Remote cluster's LeoMnager-Master\n\n\nRMS\n: A Remote cluster's LeoManager-Slave\n\n\n\n\njoin-cluster\n\n\n1\n2\n$ leofs-adm join-cluster manager_10@127.0.0.1:13095 manager_11@127.0.0.1:13096\nOK\n\n\n\n\n\n\nremove-cluster\n\n\n1\n2\n$ leofs-adm remove-cluster manager_10@127.0.0.1:13095 manager_11@127.0.0.1:13096\nOK\n\n\n\n\n\n\ncluster-status\n\n\n1\n2\n3\n4\n$ leofs-adm cluster-status\ncluster id |   dc id    |    status    | # of storages  |          updated at\n-----------+------------+--------------+----------------+-----------------------------\nleofs_2    | dc_2       | running      |              3 | 2017-04-26 10:23:59 +0900\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoManager Settings", 
            "title": "Multi Data Center Replication"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#multi-data-center-replication", 
            "text": "", 
            "title": "Multi Data Center Replication"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#configuration", 
            "text": "LeoFS provides the multi data center replication related configuration items, which contain  leo_manager_0.conf . Modify those configuration items to work its feature correctly.", 
            "title": "Configuration"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#leomanager-masters-configuration-for-mdcr", 
            "text": "Item  Description      mdc_replication.num_of_replicas_a_dc  A remote cluster of a LeoFS system which receives this cluster's objects, and then replicates them, which adhere to a replication method of each object    mdc_replication.consistency.write  A number of replicas needed for a successful WRITE-operation    mdc_replication.consistency.read  A number of replicas needed for a successful READ-operation    mdc_replication.consistency.delete  A number of replicas needed for a successful DELETE-operation", 
            "title": "LeoManager Master's Configuration For MDCR"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#how-to-operate-multi-data-center-replication", 
            "text": "", 
            "title": "How To Operate Multi Data Center Replication"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#commands", 
            "text": "There are three commands,  join-cluster ,  remove-cluster , and  cluster-status . You can control the multi data center repliation by LeoFS' CUI,  leofs-adm .     Command  Description      join-cluster  RMM   RMS  Begin to communicate between the local cluster and the remote cluster    remove-cluster  RMM   RMS  Terminate to communicate between the local cluster and the remote cluster    cluster-status  See the current state of cluster(s)      RMM : A Remote cluster's LeoMnager-Master  RMS : A Remote cluster's LeoManager-Slave", 
            "title": "Commands"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#join-cluster", 
            "text": "1\n2 $ leofs-adm join-cluster manager_10@127.0.0.1:13095 manager_11@127.0.0.1:13096\nOK", 
            "title": "join-cluster"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#remove-cluster", 
            "text": "1\n2 $ leofs-adm remove-cluster manager_10@127.0.0.1:13095 manager_11@127.0.0.1:13096\nOK", 
            "title": "remove-cluster"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#cluster-status", 
            "text": "1\n2\n3\n4 $ leofs-adm cluster-status\ncluster id |   dc id    |    status    | # of storages  |          updated at\n-----------+------------+--------------+----------------+-----------------------------\nleofs_2    | dc_2       | running      |              3 | 2017-04-26 10:23:59 +0900", 
            "title": "cluster-status"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#related-links", 
            "text": "For Administrators / Settings / LeoManager Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_admin/log_management/", 
            "text": "Log Management\n\n\nLeoGateway Access-log\n\n\nOverview\n\n\nLeoGateway provides \naccess-log\n output feature so that you can investigate requests from users.\n\n\nHow To Output Access-log\n\n\nModify \naccess-log\n configuration item which include \nleo_gateway.conf\n. After starting LeoGateway node(s), those nodes output \naccess-log\n into the local disk, which is under each LeoGateway\u2019s log directory. See more detail \nFor Administrators / Settings / LeoGateway Settings\n.\n\n\nAccess-log Configuration\n\n\n1\n2\n## Is enable access-log [true, false]\n\n\nlog.is_enable_access_log\n \n=\n \ntrue\n\n\n\n\n\n\n\nLeoGateway's Log Files\n\n\n\n\n\n\n\n\nLog File\n\n\nFile Name\n\n\n\n\n\n\n\n\n\n\naccess-log\n\n\n/log/app/access.*\n\n\n\n\n\n\nerror-log\n\n\n/log/app/error.*\n\n\n\n\n\n\ninfo-log\n\n\n/log/app/info.*\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nlog/\n\u251c\u2500\u2500 [ 272 Apr 28 14:13]  app\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [  98 Apr 28 14:13]  access -\n /leofs/package/leo_gateway/log/app/access.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [7.4K Apr 28 14:13]  access.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [  97 Apr 28 14:13]  error -\n /leofs/package/leo_gateway/log/app/error.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [   0 Apr 28 14:13]  error.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [  96 Apr 28 14:13]  info -\n /leofs/package/leo_gateway/log/app/info.20170428.14.1\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [5.4K Apr 28 14:13]  info.20170428.14.1\n\n\n\n\n\n\nExample\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n--------+-------+--------------------+----------+-------+---------------------------------------+-----------------------+----------\nMethod  | Bucket| Path               |Child Num |  Size | Timestamp                             | Unixtime              | Response\n--------+-------+--------------------+----------|-------+---------------------------------------+-----------------------+----------\n[HEAD]   photo   photo/1              0          0       2013-10-18 13:28:56.148269 +0900        1381206536148320        500\n[HEAD]   photo   photo/1              0          0       2013-10-18 13:28:56.465670 +0900        1381206536465735        404\n[HEAD]   photo   photo/city/tokyo.png 0          0       2013-10-18 13:28:56.489234 +0900        1381206536489289        200\n[GET]    photo   photo/1              0          1024    2013-10-18 13:28:56.518631 +0900        1381206536518693        500\n[GET]    photo   photo/city/paris.png 0          2048    2013-10-18 13:28:56.550376 +0900        1381206536550444        404\n[PUT]    logs    logs/leofs           1          5242880 2013-10-18 13:28:56.518631 +0900        1381206536518693        500\n[PUT]    logs    logs/leofs           2          5242880 2013-10-18 13:28:56.518631 +0900        1381206536518693        500\n[PUT]    logs    logs/leofs           3          5120    2013-10-18 13:28:56.518631 +0900        1381206536518693        500\n\n\n\n\n\n\nFormat\n\n\nAn access-log's format is TSV, \nTab Separated Values\n.\n\n\n\n\n\n\n\n\nColumn Number\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nMethod: [HEAD\n\n\n\n\n\n\n2\n\n\nBucket\n\n\n\n\n\n\n3\n\n\nFilename (including path)\n\n\n\n\n\n\n4\n\n\nChild number of a file\n\n\n\n\n\n\n5\n\n\nFile Size (byte)\n\n\n\n\n\n\n6\n\n\nTimestamp with timezone\n\n\n\n\n\n\n7\n\n\nUnixtime (including micro-second)\n\n\n\n\n\n\n8\n\n\nResponse (HTTP Status Code)\n\n\n\n\n\n\n\n\nLeoStorage Data Diagnosis-log\n\n\nOverview\n\n\nLeoStorage provides \ndiagnosis-log\n output feature so that you can investigate a LeoStorage's data. If you would like to turn on its feature, you need to modify \nleo_storage.conf\n which contain LeoStorage's package.\n\n\nHow To Output Diagnosis-log\n\n\nModify \ndiagnosis-log\n configuration item which include \nleo_storage.conf\n. After executing the \ndiagnose-start\n command, a specified node outputs \ndiagnosis-log\n into the local disk, which is under each LeoStorage\u2019s AVS directory. See more detail \nFor Administrators / Settings / LeoStorage Settings\n.\n\n\nExecute the \ndiagnose-start\n command with a specified LeoStorage node, then its LeoStorage output diagnosis-log(s) into the local disk, which is under its LeoStorage\u2019s log directory. You can configure its log directory. See more detail \nFor Administrators / System Operations / Data Operations - Diagnosis\n and \nFor Administrators / Settings / LeoStorage Settings\n.\n\n\n1\n$ leofs-adm diagnose-start \nstorage-node\n\n\n\n\n\n\n\nDiagnosis-log Configuration\n\n\n1\n2\n## Output data-diagnosis log\n\n\nlog.is_enable_diagnosis_log\n \n=\n \ntrue\n\n\n\n\n\n\n\nLeoStorage's Log Files\n\n\n\n\n\n\n\n\nLog File\n\n\nFile Name\n\n\n\n\n\n\n\n\n\n\naccess-log\n\n\n/log/app/access.*\n\n\n\n\n\n\ndiagnosis-log\n\n\n/avs/log/leo_object_storage_\n.\n\n\n\n\n\n\nerror-log\n\n\n/log/app/error.*\n\n\n\n\n\n\ninfo-log\n\n\n/log/app/info.*\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n## diagnosis-log files\n/leofs/package/leo_storage/avs/log/\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_0 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_0.20170428.14.2\n\u251c\u2500\u2500 [ 683 Apr 28 14:40]  leo_object_storage_0.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_0.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_0.report.63660577222\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_1 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_1.20170428.14.2\n\u251c\u2500\u2500 [ 779 Apr 28 14:40]  leo_object_storage_1.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_1.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_1.report.63660577222\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_2 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_2.20170428.14.2\n\u251c\u2500\u2500 [ 786 Apr 28 14:40]  leo_object_storage_2.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_2.20170428.14.2\n\u251c\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_2.report.63660577223\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_3 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_3.20170428.14.2\n\u251c\u2500\u2500 [1.3K Apr 28 14:40]  leo_object_storage_3.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_3.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_3.report.63660577223\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_4 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_4.20170428.14.2\n\u251c\u2500\u2500 [1.3K Apr 28 14:40]  leo_object_storage_4.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_4.20170428.14.2\n\u251c\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_4.report.63660577224\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_5 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_5.20170428.14.2\n\u251c\u2500\u2500 [ 943 Apr 28 14:40]  leo_object_storage_5.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_5.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_5.report.63660577225\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_6 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_6.20170428.14.2\n\u251c\u2500\u2500 [1.5K Apr 28 14:40]  leo_object_storage_6.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_6.20170428.14.2\n\u251c\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_6.report.63660577225\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_7 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_7.20170428.14.2\n\u251c\u2500\u2500 [1.1K Apr 28 14:40]  leo_object_storage_7.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_7.20170428.14.2\n\u2514\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_7.report.63660577226\n\n0 directories, 32 files\n\n\n\n\n\n\nExample\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n## Example:\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\nOffset| RING\ns address-id                        | Filename                                                   | Child num | File Size  | Unixtime         | Localtime                |del?\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\n194     296754181484029444656944009564610621293   photo/leo_redundant_manager/Makefile                             0       2034        1413348050768344   2014-10-15 13:40:50 +0900   0\n2400    185993533055981727582172380494809056426   photo/leo_redundant_manager/ebin/leo_redundant_manager.beam      0       24396       1413348050869454   2014-10-15 13:40:50 +0900   0\n38446   53208912738248114804281793572563205919    photo/leo_rpc/.git/refs/remotes/origin/HEAD                      0       33          1413348057441546   2014-10-15 13:40:57 +0900   0\n38658   57520977797167422772945547576980778561    photo/leo_rpc/ebin/leo_rpc_client_utils.beam                     0       2576        1413348057512261   2014-10-15 13:40:57 +0900   0\n69506   187294034498591995039607573685274229706   photo/leo_backend_db/src/leo_backend_db_server.erl               0       13911       1413348068031188   2014-10-15 13:41:08 +0900   0\n83603   316467020376888598364250682951088839795   photo/leo_backend_db/test/leo_backend_db_api_prop.erl            0       3507        1413348068052219   2014-10-15 13:41:08 +0900   1\n\n\n\n\n\n\nFormat\n\n\nA diagnose-log's format is TSV, \nTab Separated Values\n.\n\n\n\n\n\n\n\n\nColumn Number\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nOffset of the AVS-file\n\n\n\n\n\n\n2\n\n\nRING\u2019s address id (routing-table)\n\n\n\n\n\n\n3\n\n\nFilename\n\n\n\n\n\n\n4\n\n\nChild number of a file\n\n\n\n\n\n\n5\n\n\nFile Size (byte)\n\n\n\n\n\n\n6\n\n\nTimestamp - unixtime\n\n\n\n\n\n\n7\n\n\nTimestamp - localtime\n\n\n\n\n\n\n8\n\n\nRemoved file?\n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoGateway Settings\n\n\nFor Administrators / Settings / LeoStorage Settings\n\n\nFor Administrators / System Operations / Data Operations - Diagnosis", 
            "title": "Log Management"
        }, 
        {
            "location": "/admin/system_admin/log_management/#log-management", 
            "text": "", 
            "title": "Log Management"
        }, 
        {
            "location": "/admin/system_admin/log_management/#leogateway-access-log", 
            "text": "", 
            "title": "LeoGateway Access-log"
        }, 
        {
            "location": "/admin/system_admin/log_management/#overview", 
            "text": "LeoGateway provides  access-log  output feature so that you can investigate requests from users.", 
            "title": "Overview"
        }, 
        {
            "location": "/admin/system_admin/log_management/#how-to-output-access-log", 
            "text": "Modify  access-log  configuration item which include  leo_gateway.conf . After starting LeoGateway node(s), those nodes output  access-log  into the local disk, which is under each LeoGateway\u2019s log directory. See more detail  For Administrators / Settings / LeoGateway Settings .", 
            "title": "How To Output Access-log"
        }, 
        {
            "location": "/admin/system_admin/log_management/#access-log-configuration", 
            "text": "1\n2 ## Is enable access-log [true, false]  log.is_enable_access_log   =   true", 
            "title": "Access-log Configuration"
        }, 
        {
            "location": "/admin/system_admin/log_management/#leogateways-log-files", 
            "text": "Log File  File Name      access-log  /log/app/access.*    error-log  /log/app/error.*    info-log  /log/app/info.*     1\n2\n3\n4\n5\n6\n7\n8 log/\n\u251c\u2500\u2500 [ 272 Apr 28 14:13]  app\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [  98 Apr 28 14:13]  access -  /leofs/package/leo_gateway/log/app/access.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [7.4K Apr 28 14:13]  access.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [  97 Apr 28 14:13]  error -  /leofs/package/leo_gateway/log/app/error.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [   0 Apr 28 14:13]  error.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [  96 Apr 28 14:13]  info -  /leofs/package/leo_gateway/log/app/info.20170428.14.1\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [5.4K Apr 28 14:13]  info.20170428.14.1", 
            "title": "LeoGateway's Log Files"
        }, 
        {
            "location": "/admin/system_admin/log_management/#example", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 --------+-------+--------------------+----------+-------+---------------------------------------+-----------------------+----------\nMethod  | Bucket| Path               |Child Num |  Size | Timestamp                             | Unixtime              | Response\n--------+-------+--------------------+----------|-------+---------------------------------------+-----------------------+----------\n[HEAD]   photo   photo/1              0          0       2013-10-18 13:28:56.148269 +0900        1381206536148320        500\n[HEAD]   photo   photo/1              0          0       2013-10-18 13:28:56.465670 +0900        1381206536465735        404\n[HEAD]   photo   photo/city/tokyo.png 0          0       2013-10-18 13:28:56.489234 +0900        1381206536489289        200\n[GET]    photo   photo/1              0          1024    2013-10-18 13:28:56.518631 +0900        1381206536518693        500\n[GET]    photo   photo/city/paris.png 0          2048    2013-10-18 13:28:56.550376 +0900        1381206536550444        404\n[PUT]    logs    logs/leofs           1          5242880 2013-10-18 13:28:56.518631 +0900        1381206536518693        500\n[PUT]    logs    logs/leofs           2          5242880 2013-10-18 13:28:56.518631 +0900        1381206536518693        500\n[PUT]    logs    logs/leofs           3          5120    2013-10-18 13:28:56.518631 +0900        1381206536518693        500", 
            "title": "Example"
        }, 
        {
            "location": "/admin/system_admin/log_management/#format", 
            "text": "An access-log's format is TSV,  Tab Separated Values .     Column Number  Description      1  Method: [HEAD    2  Bucket    3  Filename (including path)    4  Child number of a file    5  File Size (byte)    6  Timestamp with timezone    7  Unixtime (including micro-second)    8  Response (HTTP Status Code)", 
            "title": "Format"
        }, 
        {
            "location": "/admin/system_admin/log_management/#leostorage-data-diagnosis-log", 
            "text": "", 
            "title": "LeoStorage Data Diagnosis-log"
        }, 
        {
            "location": "/admin/system_admin/log_management/#overview_1", 
            "text": "LeoStorage provides  diagnosis-log  output feature so that you can investigate a LeoStorage's data. If you would like to turn on its feature, you need to modify  leo_storage.conf  which contain LeoStorage's package.", 
            "title": "Overview"
        }, 
        {
            "location": "/admin/system_admin/log_management/#how-to-output-diagnosis-log", 
            "text": "Modify  diagnosis-log  configuration item which include  leo_storage.conf . After executing the  diagnose-start  command, a specified node outputs  diagnosis-log  into the local disk, which is under each LeoStorage\u2019s AVS directory. See more detail  For Administrators / Settings / LeoStorage Settings .  Execute the  diagnose-start  command with a specified LeoStorage node, then its LeoStorage output diagnosis-log(s) into the local disk, which is under its LeoStorage\u2019s log directory. You can configure its log directory. See more detail  For Administrators / System Operations / Data Operations - Diagnosis  and  For Administrators / Settings / LeoStorage Settings .  1 $ leofs-adm diagnose-start  storage-node", 
            "title": "How To Output Diagnosis-log"
        }, 
        {
            "location": "/admin/system_admin/log_management/#diagnosis-log-configuration", 
            "text": "1\n2 ## Output data-diagnosis log  log.is_enable_diagnosis_log   =   true", 
            "title": "Diagnosis-log Configuration"
        }, 
        {
            "location": "/admin/system_admin/log_management/#leostorages-log-files", 
            "text": "Log File  File Name      access-log  /log/app/access.*    diagnosis-log  /avs/log/leo_object_storage_ .    error-log  /log/app/error.*    info-log  /log/app/info.*      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36 ## diagnosis-log files\n/leofs/package/leo_storage/avs/log/\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_0 -  /leofs/package/leo_storage/avs/log/leo_object_storage_0.20170428.14.2\n\u251c\u2500\u2500 [ 683 Apr 28 14:40]  leo_object_storage_0.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_0.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_0.report.63660577222\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_1 -  /leofs/package/leo_storage/avs/log/leo_object_storage_1.20170428.14.2\n\u251c\u2500\u2500 [ 779 Apr 28 14:40]  leo_object_storage_1.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_1.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_1.report.63660577222\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_2 -  /leofs/package/leo_storage/avs/log/leo_object_storage_2.20170428.14.2\n\u251c\u2500\u2500 [ 786 Apr 28 14:40]  leo_object_storage_2.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_2.20170428.14.2\n\u251c\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_2.report.63660577223\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_3 -  /leofs/package/leo_storage/avs/log/leo_object_storage_3.20170428.14.2\n\u251c\u2500\u2500 [1.3K Apr 28 14:40]  leo_object_storage_3.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_3.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_3.report.63660577223\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_4 -  /leofs/package/leo_storage/avs/log/leo_object_storage_4.20170428.14.2\n\u251c\u2500\u2500 [1.3K Apr 28 14:40]  leo_object_storage_4.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_4.20170428.14.2\n\u251c\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_4.report.63660577224\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_5 -  /leofs/package/leo_storage/avs/log/leo_object_storage_5.20170428.14.2\n\u251c\u2500\u2500 [ 943 Apr 28 14:40]  leo_object_storage_5.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_5.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_5.report.63660577225\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_6 -  /leofs/package/leo_storage/avs/log/leo_object_storage_6.20170428.14.2\n\u251c\u2500\u2500 [1.5K Apr 28 14:40]  leo_object_storage_6.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_6.20170428.14.2\n\u251c\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_6.report.63660577225\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_7 -  /leofs/package/leo_storage/avs/log/leo_object_storage_7.20170428.14.2\n\u251c\u2500\u2500 [1.1K Apr 28 14:40]  leo_object_storage_7.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_7.20170428.14.2\n\u2514\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_7.report.63660577226\n\n0 directories, 32 files", 
            "title": "LeoStorage's Log Files"
        }, 
        {
            "location": "/admin/system_admin/log_management/#example_1", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 ## Example:\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\nOffset| RING s address-id                        | Filename                                                   | Child num | File Size  | Unixtime         | Localtime                |del?\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\n194     296754181484029444656944009564610621293   photo/leo_redundant_manager/Makefile                             0       2034        1413348050768344   2014-10-15 13:40:50 +0900   0\n2400    185993533055981727582172380494809056426   photo/leo_redundant_manager/ebin/leo_redundant_manager.beam      0       24396       1413348050869454   2014-10-15 13:40:50 +0900   0\n38446   53208912738248114804281793572563205919    photo/leo_rpc/.git/refs/remotes/origin/HEAD                      0       33          1413348057441546   2014-10-15 13:40:57 +0900   0\n38658   57520977797167422772945547576980778561    photo/leo_rpc/ebin/leo_rpc_client_utils.beam                     0       2576        1413348057512261   2014-10-15 13:40:57 +0900   0\n69506   187294034498591995039607573685274229706   photo/leo_backend_db/src/leo_backend_db_server.erl               0       13911       1413348068031188   2014-10-15 13:41:08 +0900   0\n83603   316467020376888598364250682951088839795   photo/leo_backend_db/test/leo_backend_db_api_prop.erl            0       3507        1413348068052219   2014-10-15 13:41:08 +0900   1", 
            "title": "Example"
        }, 
        {
            "location": "/admin/system_admin/log_management/#format_1", 
            "text": "A diagnose-log's format is TSV,  Tab Separated Values .     Column Number  Description      1  Offset of the AVS-file    2  RING\u2019s address id (routing-table)    3  Filename    4  Child number of a file    5  File Size (byte)    6  Timestamp - unixtime    7  Timestamp - localtime    8  Removed file?", 
            "title": "Format"
        }, 
        {
            "location": "/admin/system_admin/log_management/#related-links", 
            "text": "For Administrators / Settings / LeoGateway Settings  For Administrators / Settings / LeoStorage Settings  For Administrators / System Operations / Data Operations - Diagnosis", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_admin/persistent_configuration/", 
            "text": "Persistent Configuration Location\n\n\nNode configuration in \n/etc/leofs\n\n\nStarting from v1.3.8, LeoFS nodes support loading configuration from \n/etc/leofs\n hierarchy. This means that there are two locations from which nodes can load their configuration (but only one can be enabled at any time). Official Linux packages ship a copy of config files in \n/etc/leofs/leo_*\n, though the feature itself should work on any system. However, it is \ndisabled\n by default for compatibility with older versions.\n\n\nDifferent configuration directory for each LeoFS version\n\n\nCurrently, this is default for all installations.\n\n\nConfig files are loaded from these locations when \n/etc/leofs/leofs.conf\n does not exist, is empty or contains \nGLOBAL_CONFIG=no\n (default). \"%(version)\" here is LeoFS version like \"1.3.7\":\n\n\n\n\n\n\n\n\nNode Type\n\n\nFull Path\n\n\n\n\n\n\n\n\n\n\nManager Master\n\n\n/usr/local/leofs/%(version)/leo_manager_0/etc\n\n\n\n\n\n\nManager Slave\n\n\n/usr/local/leofs/%(version)/leo_manager_1/etc\n\n\n\n\n\n\nStorage\n\n\n/usr/local/leofs/%(version)/leo_storage/etc\n\n\n\n\n\n\nGateway\n\n\n/usr/local/leofs/%(version)/leo_gateway/etc\n\n\n\n\n\n\n\n\nFor each LeoFS version, these directories will be different (e.g. \n/usr/local/leofs/1.3.6/leo_storage/etc\n and \n/usr/local/leofs/1.3.7/leo_storage/etc\n). Because of this, config files needs to be copied into new directory after each upgrade.\n\n\n\n\nNote: Relative config files locations\n\n\nThe paths listed above work for default installation, but config files are actually loaded from directory relative to binaries. For example, if Manager Master is located in \n/home/leofs/leo_manager_master\n, config files will be picked from \n/home/leofs/leo_manager_master/etc\n.\n\n\n\n\nSame configuration directory for all versions\n\n\nFor LeoFS v1.3.8 or later, when \n/etc/leofs/leofs.conf\n exists and contains \nGLOBAL_CONFIG=yes\n, configuration is loaded from these directories - independent from version or binaries location:\n\n\n\n\n\n\n\n\nNode Type\n\n\nFull Path\n\n\n\n\n\n\n\n\n\n\nManager Master\n\n\n/etc/leofs/leo_manager_0\n\n\n\n\n\n\nManager Slave\n\n\n/etc/leofs/leo_manager_1\n\n\n\n\n\n\nStorage\n\n\n/etc/leofs/leo_storage\n\n\n\n\n\n\nGateway\n\n\n/etc/leofs/leo_gateway\n\n\n\n\n\n\n\n\nIn this case config files don't need to be moved during upgrades. To enable this feature, edit \n/etc/leofs/leofs.conf\n file, setting\n\n\n1\nGLOBAL_CONFIG=yes\n\n\n\n\n\n\nUsage details\n\n\nThe new configuration directories apply to main config files and \nenvironment files\n (leo_manager.conf/.environment, leo_storage.conf/.environment, leo_gateway.conf/.environment). This \ndoes not\n apply to anything else by default, e.g. server_cert.pem/server_key.pem won't be picked from \n/etc/leofs/leo_gateway\n unless explicitly set so in leo_gateway.conf. It does not apply to .schema files as well, but this can be changed by setting \nRUNNER_SCHEMA_DIR\n in .environment file, if required.\n\n\nUsers have an option of either directly editing main config file (e.g.  \n/etc/leofs/leo_gateway/leo_gateway.conf\n), or creating a configuration override files in \n/etc/leofs/leo_gateway/leo_gateway.d\n, e.g. \n/etc/leofs/leo_gateway/leo_gateway.d/local.conf\n. If main config file was changed, all new package upgrades won't touch the changed file, leaving the task of merging new options and changed settings into current configuration to user. It might be a good idea to use configuration override files instead; if main config file was never edited, it will be replaced by newer version during package upgrades, while options that were changed will be loaded from configuration override directory after the main config file.\n\n\nExample minimal configuration, override-style is a file \n/etc/leofs/leo_gateway/leo_gateway.d/20-leo_gateway.conf\n containing the following:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nnodename\n \n=\n \ngateway_0\n@192.168.0.20\n\n\nmanagers\n \n=\n \n[\nmanager_0\n@192.168.0.10\n,\n \nmanager_1\n@192.168.0.11\n]\n\n\n\nprotocol\n \n=\n \ns3\n\n\nhttp\n.\nport\n \n=\n \n8080\n\n\n\n## Increase RAM cache\n\n\ncache\n.\ncache_ram_capacity\n \n=\n \n1073741824\n\n\n\n\n\n\n\nHow to check which configuration directory is used\n\n\nOn launch, nodes report which directory they load config files from (also, the same directory is mentioned in \"Exec\" line of that output). When launching nodes through scripts directly, like \n/usr/local/leofs/current/leo_manager_1/bin/leo_manager start\n this information is available in erlang.log.* files. By default these files are located at \n/usr/local/leofs/current/leo_*/log\n (unless redefined with RUNNER_LOG_DIR= setting in environment config file). Please check which erlang.log.* file has the most recent date (it might be \nnot\n erlang.log.1). Example output:\n\n\n1\n2\n3\n$ grep -h -e Config -e Exec /usr/local/leofs/current/leo_manager_1/log/erlang.log.3\nConfig path: /etc/leofs/leo_manager_1\nExec: /usr/local/leofs/current/leo_manager_1/erts-8.3/bin/erlexec -heart -boot /usr/local/leofs/current/leo_manager_1/releases/1/leo_manager -mode embedded -config /etc/leofs/leo_manager_1/app.config -args_file /etc/leofs/leo_manager_1/vm.args -- console\n\n\n\n\n\n\nThis shows that configuration was loaded from \n/etc/leofs/leo_manager_1\n.\n\n\nWhen launching nodes as \nsystemd services\n, this information is stored in journald and available through journalctl:\n\n\n1\n2\n3\n$ journalctl -b -l -u leofs-manager-slave \n|\n grep -e Config -e Exec\nSep \n29\n \n19\n:45:42 leo-m1.dev leo_manager\n[\n19318\n]\n: Config path: /usr/local/leofs/current/leo_manager_1/etc\nSep \n29\n \n19\n:45:42 leo-m1.dev leo_manager\n[\n19318\n]\n: Exec: /usr/local/leofs/current/leo_manager_1/erts-9.0/bin/erlexec -noinput -boot /usr/local/leofs/current/leo_manager_1/releases/1/leo_manager -mode embedded -config /usr/local/leofs/current/leo_manager_1/etc/app.config -args_file /usr/local/leofs/current/leo_manager_1/etc/vm.args -- console\n\n\n\n\n\n\nThis shows that configuration was loaded from \n/usr/local/leofs/current/leo_manager_1/etc\n and persistent configuration directories in \n/etc/leofs\n are \nnot\n used.\n\n\nManual creation of /etc/leofs hierarchy\n\n\nThis section applies only to non-Linux users and Linux users who are not using official packages.\n\n\nConfiguration in \n/etc/leofs\n can be used even when it's not supplied with official packages; it should work if it's created properly. Here is example how to do it for LeoGateway:\n\n\n\n\nCreate directories: \nmkdir -p /etc/leofs/leo_gateway\n\n\nMake sure that config directories are \nwritable\n by user running LeoFS, e.g.  \nchown leofs:leofs /etc/leofs/leo_gateway\n. This is crucial, as nodes need to create extra files in that directory at launch.\n\n\nCreate copy of default configuration file, e.g. \ncp /usr/local/leofs/%(version)/leo_gateway/etc/leo_gateway.conf /etc/leofs/leo_gateway/\n\n\nOptionally, create conf.d-style directories for changing only parts of configuration. E.g. \nmkdir /etc/leofs/leo_gateway/leo_gateway.d\n\n\nEdit main config file (e.g. \n/etc/leofs/leo_gateway/leo_gateway.conf\n) or create configuration replacement file (e.g. \n/etc/leofs/leo_gateway/leo_gateway.d/local.conf\n). Optionally, create copy / edit environment file, if needed.\n\n\nCreate \n/etc/leofs/leofs.conf\n and write \nGLOBAL_CONFIG=yes\n in there.\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / Environment Configuration\n\n\nFor Administrators / System Operations / Systemd Services", 
            "title": "Persistent Configuration"
        }, 
        {
            "location": "/admin/system_admin/persistent_configuration/#persistent-configuration-location", 
            "text": "", 
            "title": "Persistent Configuration Location"
        }, 
        {
            "location": "/admin/system_admin/persistent_configuration/#node-configuration-in-etcleofs", 
            "text": "Starting from v1.3.8, LeoFS nodes support loading configuration from  /etc/leofs  hierarchy. This means that there are two locations from which nodes can load their configuration (but only one can be enabled at any time). Official Linux packages ship a copy of config files in  /etc/leofs/leo_* , though the feature itself should work on any system. However, it is  disabled  by default for compatibility with older versions.", 
            "title": "Node configuration in /etc/leofs"
        }, 
        {
            "location": "/admin/system_admin/persistent_configuration/#different-configuration-directory-for-each-leofs-version", 
            "text": "Currently, this is default for all installations.  Config files are loaded from these locations when  /etc/leofs/leofs.conf  does not exist, is empty or contains  GLOBAL_CONFIG=no  (default). \"%(version)\" here is LeoFS version like \"1.3.7\":     Node Type  Full Path      Manager Master  /usr/local/leofs/%(version)/leo_manager_0/etc    Manager Slave  /usr/local/leofs/%(version)/leo_manager_1/etc    Storage  /usr/local/leofs/%(version)/leo_storage/etc    Gateway  /usr/local/leofs/%(version)/leo_gateway/etc     For each LeoFS version, these directories will be different (e.g.  /usr/local/leofs/1.3.6/leo_storage/etc  and  /usr/local/leofs/1.3.7/leo_storage/etc ). Because of this, config files needs to be copied into new directory after each upgrade.   Note: Relative config files locations  The paths listed above work for default installation, but config files are actually loaded from directory relative to binaries. For example, if Manager Master is located in  /home/leofs/leo_manager_master , config files will be picked from  /home/leofs/leo_manager_master/etc .", 
            "title": "Different configuration directory for each LeoFS version"
        }, 
        {
            "location": "/admin/system_admin/persistent_configuration/#same-configuration-directory-for-all-versions", 
            "text": "For LeoFS v1.3.8 or later, when  /etc/leofs/leofs.conf  exists and contains  GLOBAL_CONFIG=yes , configuration is loaded from these directories - independent from version or binaries location:     Node Type  Full Path      Manager Master  /etc/leofs/leo_manager_0    Manager Slave  /etc/leofs/leo_manager_1    Storage  /etc/leofs/leo_storage    Gateway  /etc/leofs/leo_gateway     In this case config files don't need to be moved during upgrades. To enable this feature, edit  /etc/leofs/leofs.conf  file, setting  1 GLOBAL_CONFIG=yes", 
            "title": "Same configuration directory for all versions"
        }, 
        {
            "location": "/admin/system_admin/persistent_configuration/#usage-details", 
            "text": "The new configuration directories apply to main config files and  environment files  (leo_manager.conf/.environment, leo_storage.conf/.environment, leo_gateway.conf/.environment). This  does not  apply to anything else by default, e.g. server_cert.pem/server_key.pem won't be picked from  /etc/leofs/leo_gateway  unless explicitly set so in leo_gateway.conf. It does not apply to .schema files as well, but this can be changed by setting  RUNNER_SCHEMA_DIR  in .environment file, if required.  Users have an option of either directly editing main config file (e.g.   /etc/leofs/leo_gateway/leo_gateway.conf ), or creating a configuration override files in  /etc/leofs/leo_gateway/leo_gateway.d , e.g.  /etc/leofs/leo_gateway/leo_gateway.d/local.conf . If main config file was changed, all new package upgrades won't touch the changed file, leaving the task of merging new options and changed settings into current configuration to user. It might be a good idea to use configuration override files instead; if main config file was never edited, it will be replaced by newer version during package upgrades, while options that were changed will be loaded from configuration override directory after the main config file.  Example minimal configuration, override-style is a file  /etc/leofs/leo_gateway/leo_gateway.d/20-leo_gateway.conf  containing the following:  1\n2\n3\n4\n5\n6\n7\n8 nodename   =   gateway_0 @192.168.0.20  managers   =   [ manager_0 @192.168.0.10 ,   manager_1 @192.168.0.11 ]  protocol   =   s3  http . port   =   8080  ## Increase RAM cache  cache . cache_ram_capacity   =   1073741824", 
            "title": "Usage details"
        }, 
        {
            "location": "/admin/system_admin/persistent_configuration/#how-to-check-which-configuration-directory-is-used", 
            "text": "On launch, nodes report which directory they load config files from (also, the same directory is mentioned in \"Exec\" line of that output). When launching nodes through scripts directly, like  /usr/local/leofs/current/leo_manager_1/bin/leo_manager start  this information is available in erlang.log.* files. By default these files are located at  /usr/local/leofs/current/leo_*/log  (unless redefined with RUNNER_LOG_DIR= setting in environment config file). Please check which erlang.log.* file has the most recent date (it might be  not  erlang.log.1). Example output:  1\n2\n3 $ grep -h -e Config -e Exec /usr/local/leofs/current/leo_manager_1/log/erlang.log.3\nConfig path: /etc/leofs/leo_manager_1\nExec: /usr/local/leofs/current/leo_manager_1/erts-8.3/bin/erlexec -heart -boot /usr/local/leofs/current/leo_manager_1/releases/1/leo_manager -mode embedded -config /etc/leofs/leo_manager_1/app.config -args_file /etc/leofs/leo_manager_1/vm.args -- console   This shows that configuration was loaded from  /etc/leofs/leo_manager_1 .  When launching nodes as  systemd services , this information is stored in journald and available through journalctl:  1\n2\n3 $ journalctl -b -l -u leofs-manager-slave  |  grep -e Config -e Exec\nSep  29   19 :45:42 leo-m1.dev leo_manager [ 19318 ] : Config path: /usr/local/leofs/current/leo_manager_1/etc\nSep  29   19 :45:42 leo-m1.dev leo_manager [ 19318 ] : Exec: /usr/local/leofs/current/leo_manager_1/erts-9.0/bin/erlexec -noinput -boot /usr/local/leofs/current/leo_manager_1/releases/1/leo_manager -mode embedded -config /usr/local/leofs/current/leo_manager_1/etc/app.config -args_file /usr/local/leofs/current/leo_manager_1/etc/vm.args -- console   This shows that configuration was loaded from  /usr/local/leofs/current/leo_manager_1/etc  and persistent configuration directories in  /etc/leofs  are  not  used.", 
            "title": "How to check which configuration directory is used"
        }, 
        {
            "location": "/admin/system_admin/persistent_configuration/#manual-creation-of-etcleofs-hierarchy", 
            "text": "This section applies only to non-Linux users and Linux users who are not using official packages.  Configuration in  /etc/leofs  can be used even when it's not supplied with official packages; it should work if it's created properly. Here is example how to do it for LeoGateway:   Create directories:  mkdir -p /etc/leofs/leo_gateway  Make sure that config directories are  writable  by user running LeoFS, e.g.   chown leofs:leofs /etc/leofs/leo_gateway . This is crucial, as nodes need to create extra files in that directory at launch.  Create copy of default configuration file, e.g.  cp /usr/local/leofs/%(version)/leo_gateway/etc/leo_gateway.conf /etc/leofs/leo_gateway/  Optionally, create conf.d-style directories for changing only parts of configuration. E.g.  mkdir /etc/leofs/leo_gateway/leo_gateway.d  Edit main config file (e.g.  /etc/leofs/leo_gateway/leo_gateway.conf ) or create configuration replacement file (e.g.  /etc/leofs/leo_gateway/leo_gateway.d/local.conf ). Optionally, create copy / edit environment file, if needed.  Create  /etc/leofs/leofs.conf  and write  GLOBAL_CONFIG=yes  in there.", 
            "title": "Manual creation of /etc/leofs hierarchy"
        }, 
        {
            "location": "/admin/system_admin/persistent_configuration/#related-links", 
            "text": "For Administrators / Settings / Environment Configuration  For Administrators / System Operations / Systemd Services", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_admin/monitoring/", 
            "text": "System Monitoring\n\n\nSNMPA Setup\n\n\nEach node of LeoStorage, LeoGateway and LeoManager provides a built in SNMP agent which allows to connect external systems, such as \nNagios\n and \nZabbix\n. You can retrieve various statistics of your LeoFS.\n\n\nLeoManager\n\n\nSNMPA Properties\n\n\n\n\n\n\n\n\nItem\n\n\nValue / Range\n\n\n\n\n\n\n\n\n\n\nPort\n\n\n4020..4021, 14020..14021\n\n\n\n\n\n\nBranch\n\n\n1.3.6.1.4.1.35450\n\n\n\n\n\n\nsnmpa_manager_0\n\n\nPort: 4020\n\n\n\n\n\n\nsnmpa_manager_1\n\n\nPort: 4021\n\n\n\n\n\n\nsnmpa_manager_2\n\n\nPort: 14020\n\n\n\n\n\n\nsnmpa_manager_3\n\n\nPort: 14021\n\n\n\n\n\n\n\n\nSNMPA Items of Erlang-VM\n\n\n\n\n\n\n\n\nBranch Number\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nNode name\n\n\n\n\n\n\n1 min average\n\n\n\n\n\n\n\n\n2\n\n\nTotal numeber of processes\n\n\n\n\n\n\n3\n\n\nTotal memory usage\n\n\n\n\n\n\n4\n\n\nSystem memory usage\n\n\n\n\n\n\n5\n\n\nProcesses memory usage\n\n\n\n\n\n\n6\n\n\nETS memory usage\n\n\n\n\n\n\n5 min average\n\n\n\n\n\n\n\n\n7\n\n\nTotal numeber of processes\n\n\n\n\n\n\n8\n\n\nTotal memory usage\n\n\n\n\n\n\n9\n\n\nSystem memory usage\n\n\n\n\n\n\n10\n\n\nProcesses memory usage\n\n\n\n\n\n\n11\n\n\nETS memory usage\n\n\n\n\n\n\nAllocated memmory\n\n\n\n\n\n\n\n\n12\n\n\nUsed/allocated memory for 1 min\n\n\n\n\n\n\n13\n\n\nAllocated memory for 1 min\n\n\n\n\n\n\n14\n\n\nUsed/allocated memory for 5 min\n\n\n\n\n\n\n15\n\n\nAllocated memory for 5 min\n\n\n\n\n\n\n\n\nCheck the configuration with \nsnmpwalk\n command after starting LeoFS\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n$ snmpwalk -v 2c -c public \n127\n.0.0.1:4020 .1.3.6.1.4.1.35450\nSNMPv2-SMI::enterprises.35450.15.1.0 \n=\n STRING: \nmanager_0@127.0.0.1\n\nSNMPv2-SMI::enterprises.35450.15.2.0 \n=\n Gauge32: \n123\n\nSNMPv2-SMI::enterprises.35450.15.3.0 \n=\n Gauge32: \n30289989\n\nSNMPv2-SMI::enterprises.35450.15.4.0 \n=\n Gauge32: \n24256857\n\nSNMPv2-SMI::enterprises.35450.15.5.0 \n=\n Gauge32: \n6033132\n\nSNMPv2-SMI::enterprises.35450.15.6.0 \n=\n Gauge32: \n1914017\n\nSNMPv2-SMI::enterprises.35450.15.7.0 \n=\n Gauge32: \n123\n\nSNMPv2-SMI::enterprises.35450.15.8.0 \n=\n Gauge32: \n30309552\n\nSNMPv2-SMI::enterprises.35450.15.9.0 \n=\n Gauge32: \n24278377\n\nSNMPv2-SMI::enterprises.35450.15.10.0 \n=\n Gauge32: \n6031175\n\nSNMPv2-SMI::enterprises.35450.15.11.0 \n=\n Gauge32: \n1935758\n\nSNMPv2-SMI::enterprises.35450.15.12.0 \n=\n Gauge32: \n75\n\nSNMPv2-SMI::enterprises.35450.15.13.0 \n=\n Gauge32: \n84635402\n\nSNMPv2-SMI::enterprises.35450.15.14.0 \n=\n Gauge32: \n78\n\nSNMPv2-SMI::enterprises.35450.15.15.0 \n=\n Gauge32: \n88735915\n\n\n\n\n\n\n\nLeoStorage\n\n\nSNMPA Properties\n\n\n\n\n\n\n\n\nItem\n\n\nValue / Range\n\n\n\n\n\n\n\n\n\n\nPort\n\n\n4010..4014\n\n\n\n\n\n\nBranch\n\n\n1.3.6.1.4.1.35450\n\n\n\n\n\n\nsnmpa_storage_0\n\n\nPort: 4010\n\n\n\n\n\n\nsnmpa_storage_1\n\n\nPort: 4011\n\n\n\n\n\n\nsnmpa_storage_2\n\n\nPort: 4012\n\n\n\n\n\n\nsnmpa_storage_3\n\n\nPort: 4013\n\n\n\n\n\n\n\n\nSNMPA Items of Erlang-VM\n\n\n\n\n\n\n\n\nBranch Number\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nNode name\n\n\n\n\n\n\n1 min average\n\n\n\n\n\n\n\n\n2\n\n\nTotal numeber of processes\n\n\n\n\n\n\n3\n\n\nTotal memory usage\n\n\n\n\n\n\n4\n\n\nSystem memory usage\n\n\n\n\n\n\n5\n\n\nProcesses memory usage\n\n\n\n\n\n\n6\n\n\nETS memory usage\n\n\n\n\n\n\n5 min average\n\n\n\n\n\n\n\n\n7\n\n\nTotal numeber of processes\n\n\n\n\n\n\n8\n\n\nTotal memory usage\n\n\n\n\n\n\n9\n\n\nSystem memory usage\n\n\n\n\n\n\n10\n\n\nProcesses memory usage\n\n\n\n\n\n\n11\n\n\nETS memory usage\n\n\n\n\n\n\nAllocated memmory\n\n\n\n\n\n\n\n\n31\n\n\nUsed/allocated memory for 1 min\n\n\n\n\n\n\n32\n\n\nAllocated memory for 1 min\n\n\n\n\n\n\n33\n\n\nUsed/allocated memory for 5 min\n\n\n\n\n\n\n34\n\n\nAllocated memory for 5 min\n\n\n\n\n\n\n\n\nSNMPA Items of LeoStorage\n\n\n\n\n\n\n\n\nBranch Number\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nRequest counter for 1 min\n\n\n\n\n\n\n\n\n12\n\n\nTotal number of WRITE requests\n\n\n\n\n\n\n13\n\n\nTotal number of READ requests\n\n\n\n\n\n\n14\n\n\nTotal number of DELETE requests\n\n\n\n\n\n\nRequest counter for 5 min\n\n\n\n\n\n\n\n\n15\n\n\nTotal number of WRITE requests\n\n\n\n\n\n\n16\n\n\nTotal number of READ requests\n\n\n\n\n\n\n17\n\n\nTotal number of DELETE requests\n\n\n\n\n\n\nStored objects related\n\n\n\n\n\n\n\n\n18\n\n\nTotal number of active objects\n\n\n\n\n\n\n19\n\n\nTotal number of objects\n \n(It includes inactive objects which are removed and updated objects)\n\n\n\n\n\n\n20\n\n\nTotal active object size\n\n\n\n\n\n\n21\n\n\nTotal object size\n\n\n\n\n\n\nMQ related\n\n\n\n\n\n\n\n\n22\n\n\nTotal messages of \nobject replication\n\n\n\n\n\n\n23\n\n\nTotal messages of \nsync-vnodes\n\n\n\n\n\n\n24\n\n\nTotal messages of \nrebalance\n\n\n\n\n\n\n41\n\n\nTotal messages of \nrecovery-node\n \n(since \nv1.4.0\n)\n\n\n\n\n\n\n42\n\n\nTotal messages of \ndeletion-directry\n \n(since \nv1.4.0\n)\n\n\n\n\n\n\n43\n\n\nTotal messages of \nasync deletion-directries\n \n(since \nv1.4.0\n)\n\n\n\n\n\n\n44\n\n\nTotal messages of a requet of \ndeletion-directry\n \n(since \nv1.4.0\n)\n\n\n\n\n\n\n45\n\n\nTotal messages of \ncomparison-metadata\n for the multi datacenter replication \n(since \nv1.4.0\n)\n\n\n\n\n\n\n46\n\n\nTotal messages of a request of \nsync-object\n for the multi datacenter replication \n(since \nv1.4.0\n)\n\n\n\n\n\n\nData-compaction related\n\n\n\n\n\n\n\n\n51\n\n\ndata-compaction state\n0: \nidling\n1: \nrunning\n2: \nsuspending\n\n\n\n\n\n\n52\n\n\nStart date time of last data-compaction \n(unixtime)\n\n\n\n\n\n\n53\n\n\nEnd date time of last data-compaction \n(unixtime)\n\n\n\n\n\n\n54\n\n\nTotal number of pending targets\n\n\n\n\n\n\n55\n\n\nTotal number of ongoing targets\n\n\n\n\n\n\n56\n\n\nTotal number of out of targets\n\n\n\n\n\n\n\n\nCheck the configuration with \nsnmpwalk\n command after starting LeoFS\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n$\n \nsnmpwalk\n \n-\nv\n \n2\nc\n \n-\nc\n \npublic\n \n127.0.0.1\n:\n4010\n \n.1.3.6.1.4.1.35450\n\n\niso\n.3.6.1.4.1.35450.56.1.0\n \n=\n \nSTRING\n:\n \nstorage_0@127.0.0.1\n\n\niso\n.3.6.1.4.1.35450.56.2.0\n \n=\n \nGauge32\n:\n \n577\n\n\niso\n.3.6.1.4.1.35450.56.3.0\n \n=\n \nGauge32\n:\n \n47509309\n\n\niso\n.3.6.1.4.1.35450.56.4.0\n \n=\n \nGauge32\n:\n \n27404799\n\n\niso\n.3.6.1.4.1.35450.56.5.0\n \n=\n \nGauge32\n:\n \n20096683\n\n\niso\n.3.6.1.4.1.35450.56.6.0\n \n=\n \nGauge32\n:\n \n5967268\n\n\niso\n.3.6.1.4.1.35450.56.7.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.8.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.9.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.10.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.11.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.12.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.13.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.14.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.15.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.16.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.17.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.18.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.19.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.20.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.21.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.22.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.23.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.24.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.31.0\n \n=\n \nGauge32\n:\n \n63\n\n\niso\n.3.6.1.4.1.35450.56.32.0\n \n=\n \nGauge32\n:\n \n73028949\n\n\niso\n.3.6.1.4.1.35450.56.33.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.34.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.41.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.42.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.43.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.44.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.45.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.46.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.51.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.52.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.53.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.54.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.55.0\n \n=\n \nGauge32\n:\n \n0\n\n\niso\n.3.6.1.4.1.35450.56.56.0\n \n=\n \nGauge32\n:\n \n0\n\n\n\n\n\n\n\nLeoGateway\n\n\nSNMPA Properties\n\n\n\n\n\n\n\n\nItem\n\n\nValue / Range\n\n\n\n\n\n\n\n\n\n\nPort\n\n\n4000..4001\n\n\n\n\n\n\nBranch\n\n\n1.3.6.1.4.1.35450\n\n\n\n\n\n\nsnmpa_gateway_0\n\n\nPort: 4000\n\n\n\n\n\n\nsnmpa_gateway_1\n\n\nPort: 4001\n\n\n\n\n\n\n\n\nSNMPA Items of Erlang-VM\n\n\n\n\n\n\n\n\nBranch Number\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nNode name\n\n\n\n\n\n\n1 min average\n\n\n\n\n\n\n\n\n2\n\n\nTotal numeber of processes\n\n\n\n\n\n\n3\n\n\nTotal memory usage\n\n\n\n\n\n\n4\n\n\nSystem memory usage\n\n\n\n\n\n\n5\n\n\nProcesses memory usage\n\n\n\n\n\n\n6\n\n\nETS memory usage\n\n\n\n\n\n\n5 min average\n\n\n\n\n\n\n\n\n7\n\n\nTotal numeber of processes\n\n\n\n\n\n\n8\n\n\nTotal memory usage\n\n\n\n\n\n\n9\n\n\nSystem memory usage\n\n\n\n\n\n\n10\n\n\nProcesses memory usage\n\n\n\n\n\n\n11\n\n\nETS memory usage\n\n\n\n\n\n\nAllocated memmory\n\n\n\n\n\n\n\n\n31\n\n\nUsed/allocated memory for 1 min\n\n\n\n\n\n\n32\n\n\nAllocated memory for 1 min\n\n\n\n\n\n\n33\n\n\nUsed/allocated memory for 5 min\n\n\n\n\n\n\n34\n\n\nAllocated memory for 5 min\n\n\n\n\n\n\n\n\nSNMPA Items of LeoGateway\n\n\n\n\n\n\n\n\nBranch Number\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nRequest counter for 1 min\n\n\n\n\n\n\n\n\n12\n\n\nTotal number of WRITE requests\n\n\n\n\n\n\n13\n\n\nTotal number of READ requests\n\n\n\n\n\n\n14\n\n\nTotal number of DELETE requests\n\n\n\n\n\n\nRequest counter for 5 min\n\n\n\n\n\n\n\n\n15\n\n\nTotal number of WRITE requests\n\n\n\n\n\n\n16\n\n\nTotal number of READ requests\n\n\n\n\n\n\n17\n\n\nTotal number of DELETE requests\n\n\n\n\n\n\nCache related\n\n\n\n\n\n\n\n\n18\n\n\nTotal counts of cache-hit\n\n\n\n\n\n\n19\n\n\nTotal counts of cache-miss\n\n\n\n\n\n\n20\n\n\nTotal number of cached objects\n\n\n\n\n\n\n21\n\n\nTotal cached object size\n\n\n\n\n\n\n\n\nCheck the configuration with \nsnmpwalk\n command after starting LeoFS\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n$\n \nsnmpwalk\n \n-\nv\n \n2\nc\n \n-\nc\n \npublic\n \n127.0.0.1\n:\n4000\n \n.1.3.6.1.4.1.35450\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.1.0\n \n=\n \nSTRING\n:\n \ngateway_0@127.0.0.1\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.2.0\n \n=\n \nGauge32\n:\n \n279\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.3.0\n \n=\n \nGauge32\n:\n \n45266128\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.4.0\n \n=\n \nGauge32\n:\n \n36653905\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.5.0\n \n=\n \nGauge32\n:\n \n8612223\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.6.0\n \n=\n \nGauge32\n:\n \n2276519\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.7.0\n \n=\n \nGauge32\n:\n \n279\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.8.0\n \n=\n \nGauge32\n:\n \n45157433\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.9.0\n \n=\n \nGauge32\n:\n \n36385227\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.10.0\n \n=\n \nGauge32\n:\n \n8772210\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.11.0\n \n=\n \nGauge32\n:\n \n2261105\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.12.0\n \n=\n \nGauge32\n:\n \n0\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.13.0\n \n=\n \nGauge32\n:\n \n13\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.14.0\n \n=\n \nGauge32\n:\n \n0\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.15.0\n \n=\n \nGauge32\n:\n \n3\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.16.0\n \n=\n \nGauge32\n:\n \n24\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.17.0\n \n=\n \nGauge32\n:\n \n0\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.18.0\n \n=\n \nGauge32\n:\n \n21\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.19.0\n \n=\n \nGauge32\n:\n \n39\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.20.0\n \n=\n \nGauge32\n:\n \n3\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.21.0\n \n=\n \nGauge32\n:\n \n565700\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.31.0\n \n=\n \nGauge32\n:\n \n75\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.32.0\n \n=\n \nGauge32\n:\n \n84635402\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.33.0\n \n=\n \nGauge32\n:\n \n78\n\n\nSNMPv2\n-\nSMI\n::\nenterprises\n.35450.34.34.0\n \n=\n \nGauge32\n:\n \n88735915\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nAdministrators / Settings / LeoManager Settings\n\n\nAdministrators / Settings / LeoStorage Settings\n\n\nAdministrators / Settings / LeoGateway Settings", 
            "title": "System Monitoring"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#system-monitoring", 
            "text": "", 
            "title": "System Monitoring"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#snmpa-setup", 
            "text": "Each node of LeoStorage, LeoGateway and LeoManager provides a built in SNMP agent which allows to connect external systems, such as  Nagios  and  Zabbix . You can retrieve various statistics of your LeoFS.", 
            "title": "SNMPA Setup"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#leomanager", 
            "text": "", 
            "title": "LeoManager"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#snmpa-properties", 
            "text": "Item  Value / Range      Port  4020..4021, 14020..14021    Branch  1.3.6.1.4.1.35450    snmpa_manager_0  Port: 4020    snmpa_manager_1  Port: 4021    snmpa_manager_2  Port: 14020    snmpa_manager_3  Port: 14021", 
            "title": "SNMPA Properties"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#snmpa-items-of-erlang-vm", 
            "text": "Branch Number  Description      1  Node name    1 min average     2  Total numeber of processes    3  Total memory usage    4  System memory usage    5  Processes memory usage    6  ETS memory usage    5 min average     7  Total numeber of processes    8  Total memory usage    9  System memory usage    10  Processes memory usage    11  ETS memory usage    Allocated memmory     12  Used/allocated memory for 1 min    13  Allocated memory for 1 min    14  Used/allocated memory for 5 min    15  Allocated memory for 5 min", 
            "title": "SNMPA Items of Erlang-VM"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#check-the-configuration-with-snmpwalk-command-after-starting-leofs", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 $ snmpwalk -v 2c -c public  127 .0.0.1:4020 .1.3.6.1.4.1.35450\nSNMPv2-SMI::enterprises.35450.15.1.0  =  STRING:  manager_0@127.0.0.1 \nSNMPv2-SMI::enterprises.35450.15.2.0  =  Gauge32:  123 \nSNMPv2-SMI::enterprises.35450.15.3.0  =  Gauge32:  30289989 \nSNMPv2-SMI::enterprises.35450.15.4.0  =  Gauge32:  24256857 \nSNMPv2-SMI::enterprises.35450.15.5.0  =  Gauge32:  6033132 \nSNMPv2-SMI::enterprises.35450.15.6.0  =  Gauge32:  1914017 \nSNMPv2-SMI::enterprises.35450.15.7.0  =  Gauge32:  123 \nSNMPv2-SMI::enterprises.35450.15.8.0  =  Gauge32:  30309552 \nSNMPv2-SMI::enterprises.35450.15.9.0  =  Gauge32:  24278377 \nSNMPv2-SMI::enterprises.35450.15.10.0  =  Gauge32:  6031175 \nSNMPv2-SMI::enterprises.35450.15.11.0  =  Gauge32:  1935758 \nSNMPv2-SMI::enterprises.35450.15.12.0  =  Gauge32:  75 \nSNMPv2-SMI::enterprises.35450.15.13.0  =  Gauge32:  84635402 \nSNMPv2-SMI::enterprises.35450.15.14.0  =  Gauge32:  78 \nSNMPv2-SMI::enterprises.35450.15.15.0  =  Gauge32:  88735915", 
            "title": "Check the configuration with snmpwalk command after starting LeoFS"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#leostorage", 
            "text": "", 
            "title": "LeoStorage"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#snmpa-properties_1", 
            "text": "Item  Value / Range      Port  4010..4014    Branch  1.3.6.1.4.1.35450    snmpa_storage_0  Port: 4010    snmpa_storage_1  Port: 4011    snmpa_storage_2  Port: 4012    snmpa_storage_3  Port: 4013", 
            "title": "SNMPA Properties"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#snmpa-items-of-erlang-vm_1", 
            "text": "Branch Number  Description      1  Node name    1 min average     2  Total numeber of processes    3  Total memory usage    4  System memory usage    5  Processes memory usage    6  ETS memory usage    5 min average     7  Total numeber of processes    8  Total memory usage    9  System memory usage    10  Processes memory usage    11  ETS memory usage    Allocated memmory     31  Used/allocated memory for 1 min    32  Allocated memory for 1 min    33  Used/allocated memory for 5 min    34  Allocated memory for 5 min", 
            "title": "SNMPA Items of Erlang-VM"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#snmpa-items-of-leostorage", 
            "text": "Branch Number  Description      Request counter for 1 min     12  Total number of WRITE requests    13  Total number of READ requests    14  Total number of DELETE requests    Request counter for 5 min     15  Total number of WRITE requests    16  Total number of READ requests    17  Total number of DELETE requests    Stored objects related     18  Total number of active objects    19  Total number of objects   (It includes inactive objects which are removed and updated objects)    20  Total active object size    21  Total object size    MQ related     22  Total messages of  object replication    23  Total messages of  sync-vnodes    24  Total messages of  rebalance    41  Total messages of  recovery-node   (since  v1.4.0 )    42  Total messages of  deletion-directry   (since  v1.4.0 )    43  Total messages of  async deletion-directries   (since  v1.4.0 )    44  Total messages of a requet of  deletion-directry   (since  v1.4.0 )    45  Total messages of  comparison-metadata  for the multi datacenter replication  (since  v1.4.0 )    46  Total messages of a request of  sync-object  for the multi datacenter replication  (since  v1.4.0 )    Data-compaction related     51  data-compaction state 0:  idling 1:  running 2:  suspending    52  Start date time of last data-compaction  (unixtime)    53  End date time of last data-compaction  (unixtime)    54  Total number of pending targets    55  Total number of ongoing targets    56  Total number of out of targets", 
            "title": "SNMPA Items of LeoStorage"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#check-the-configuration-with-snmpwalk-command-after-starting-leofs_1", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41 $   snmpwalk   - v   2 c   - c   public   127.0.0.1 : 4010   .1.3.6.1.4.1.35450  iso .3.6.1.4.1.35450.56.1.0   =   STRING :   storage_0@127.0.0.1  iso .3.6.1.4.1.35450.56.2.0   =   Gauge32 :   577  iso .3.6.1.4.1.35450.56.3.0   =   Gauge32 :   47509309  iso .3.6.1.4.1.35450.56.4.0   =   Gauge32 :   27404799  iso .3.6.1.4.1.35450.56.5.0   =   Gauge32 :   20096683  iso .3.6.1.4.1.35450.56.6.0   =   Gauge32 :   5967268  iso .3.6.1.4.1.35450.56.7.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.8.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.9.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.10.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.11.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.12.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.13.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.14.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.15.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.16.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.17.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.18.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.19.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.20.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.21.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.22.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.23.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.24.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.31.0   =   Gauge32 :   63  iso .3.6.1.4.1.35450.56.32.0   =   Gauge32 :   73028949  iso .3.6.1.4.1.35450.56.33.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.34.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.41.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.42.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.43.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.44.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.45.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.46.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.51.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.52.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.53.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.54.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.55.0   =   Gauge32 :   0  iso .3.6.1.4.1.35450.56.56.0   =   Gauge32 :   0", 
            "title": "Check the configuration with snmpwalk command after starting LeoFS"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#leogateway", 
            "text": "", 
            "title": "LeoGateway"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#snmpa-properties_2", 
            "text": "Item  Value / Range      Port  4000..4001    Branch  1.3.6.1.4.1.35450    snmpa_gateway_0  Port: 4000    snmpa_gateway_1  Port: 4001", 
            "title": "SNMPA Properties"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#snmpa-items-of-erlang-vm_2", 
            "text": "Branch Number  Description      1  Node name    1 min average     2  Total numeber of processes    3  Total memory usage    4  System memory usage    5  Processes memory usage    6  ETS memory usage    5 min average     7  Total numeber of processes    8  Total memory usage    9  System memory usage    10  Processes memory usage    11  ETS memory usage    Allocated memmory     31  Used/allocated memory for 1 min    32  Allocated memory for 1 min    33  Used/allocated memory for 5 min    34  Allocated memory for 5 min", 
            "title": "SNMPA Items of Erlang-VM"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#snmpa-items-of-leogateway", 
            "text": "Branch Number  Description      Request counter for 1 min     12  Total number of WRITE requests    13  Total number of READ requests    14  Total number of DELETE requests    Request counter for 5 min     15  Total number of WRITE requests    16  Total number of READ requests    17  Total number of DELETE requests    Cache related     18  Total counts of cache-hit    19  Total counts of cache-miss    20  Total number of cached objects    21  Total cached object size", 
            "title": "SNMPA Items of LeoGateway"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#check-the-configuration-with-snmpwalk-command-after-starting-leofs_2", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 $   snmpwalk   - v   2 c   - c   public   127.0.0.1 : 4000   .1.3.6.1.4.1.35450  SNMPv2 - SMI :: enterprises .35450.34.1.0   =   STRING :   gateway_0@127.0.0.1  SNMPv2 - SMI :: enterprises .35450.34.2.0   =   Gauge32 :   279  SNMPv2 - SMI :: enterprises .35450.34.3.0   =   Gauge32 :   45266128  SNMPv2 - SMI :: enterprises .35450.34.4.0   =   Gauge32 :   36653905  SNMPv2 - SMI :: enterprises .35450.34.5.0   =   Gauge32 :   8612223  SNMPv2 - SMI :: enterprises .35450.34.6.0   =   Gauge32 :   2276519  SNMPv2 - SMI :: enterprises .35450.34.7.0   =   Gauge32 :   279  SNMPv2 - SMI :: enterprises .35450.34.8.0   =   Gauge32 :   45157433  SNMPv2 - SMI :: enterprises .35450.34.9.0   =   Gauge32 :   36385227  SNMPv2 - SMI :: enterprises .35450.34.10.0   =   Gauge32 :   8772210  SNMPv2 - SMI :: enterprises .35450.34.11.0   =   Gauge32 :   2261105  SNMPv2 - SMI :: enterprises .35450.34.12.0   =   Gauge32 :   0  SNMPv2 - SMI :: enterprises .35450.34.13.0   =   Gauge32 :   13  SNMPv2 - SMI :: enterprises .35450.34.14.0   =   Gauge32 :   0  SNMPv2 - SMI :: enterprises .35450.34.15.0   =   Gauge32 :   3  SNMPv2 - SMI :: enterprises .35450.34.16.0   =   Gauge32 :   24  SNMPv2 - SMI :: enterprises .35450.34.17.0   =   Gauge32 :   0  SNMPv2 - SMI :: enterprises .35450.34.18.0   =   Gauge32 :   21  SNMPv2 - SMI :: enterprises .35450.34.19.0   =   Gauge32 :   39  SNMPv2 - SMI :: enterprises .35450.34.20.0   =   Gauge32 :   3  SNMPv2 - SMI :: enterprises .35450.34.21.0   =   Gauge32 :   565700  SNMPv2 - SMI :: enterprises .35450.34.31.0   =   Gauge32 :   75  SNMPv2 - SMI :: enterprises .35450.34.32.0   =   Gauge32 :   84635402  SNMPv2 - SMI :: enterprises .35450.34.33.0   =   Gauge32 :   78  SNMPv2 - SMI :: enterprises .35450.34.34.0   =   Gauge32 :   88735915", 
            "title": "Check the configuration with snmpwalk command after starting LeoFS"
        }, 
        {
            "location": "/admin/system_admin/monitoring/#related-links", 
            "text": "Administrators / Settings / LeoManager Settings  Administrators / Settings / LeoStorage Settings  Administrators / Settings / LeoGateway Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_admin/migration/", 
            "text": "System Migration\n\n\nUpgrade an older LeoFS version to the latest one\n\n\nOperation Flow\n\n\nIf you would like to migrate a LeoFS system, you can achieve that by following the operation flow.\n\n\n\n\n\n\nSee the large diagram\n\n\n\n\nTakeover and Adjust Confugurations\n\n\nBefore getting started with the migration of a LeoFS system, you need to take over the configuration, then adjust the paths and set the new configurations.\nIt is possible to simplify this by storing node configuration in \n/etc/leofs\n, independent on version. Please refer to \nFor Administrators / System Administration / Persistent Configuration\n for instructions on enabling and using this feature.\n\n\nAdded Or Changed LeoFS' Configurations\n\n\nLeoManager Master\n\n\n[since v1.3.3]\n LeoFS' MDC replication feature was improved. Some configuration are added in \nthe configuration of LeoManager's master\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n## --------------------------------------------------------------------\n\n\n## MANAGER - Multi DataCenter Settings\n\n\n## --------------------------------------------------------------------\n\n\n## A number of replication targets\n\n\nmdc_replication.max_targets\n \n=\n \n2\n\n\n\n## A number of replicas per a datacenter\n\n\n## [note] A local LeoFS sends a stacked object which contains an items of a replication method:\n\n\n##          - [L1_N] A number of replicas\n\n\n##          - [L1_W] A number of replicas needed for a successful WRITE operation\n\n\n##          - [L1_R] A number of replicas needed for a successful READ operation\n\n\n##          - [L1_D] A number of replicas needed for a successful DELETE operation\n\n\n##       A remote cluster of a LeoFS system which receives its object,\n\n\n##       and then replicates it by its contained reoplication method.\n\n\nmdc_replication.num_of_replicas_a_dc\n \n=\n \n1\n\n\n\n## MDC replication / A number of replicas needed for a successful WRITE operation\n\n\nmdc_replication.consistency.write\n \n=\n \n1\n\n\n\n## MDC replication / A number of replicas needed for a successful READ operation\n\n\nmdc_replication.consistency.read\n \n=\n \n1\n\n\n\n## MDC replication / A number of replicas needed for a successful DELETE operation\n\n\nmdc_replication.consistency.delete\n \n=\n \n1\n\n\n\n\n\n\n\nLeoStorage\n\n\n[since v1.3.3]\n Data synchronization configuration are added in \nthe configuration of LeoStorage\n.\n\n\n1\n2\n3\n4\n5\n6\n## Mode of the data synchronization - [none, periodic, writethrough]\n\n\n## - default:none\n\n\nobj_containers.sync_mode\n \n=\n \nnone\n\n\n\n## Interval in ms of the data synchronization - default: 1000ms\n\n\nobj_containers.sync_interval_in_ms\n \n=\n \n1000\n\n\n\n\n\n\n\nChanges in LeoFS' Packages (1.3.8+)\n\n\nStarting from v1.3.8 there are changes in \nthe official Linux packages\n. They should work out of the box for new installations and for new nodes on existing installations. However, one needs to pay special attention during upgrading existing nodes to v1.3.8.\n\n\n\n\nfor all supported distributions: there is now symlink \n/usr/local/leofs/current\n which points to \n/usr/local/leofs/\nversion\n, this allows fixed path to launch scripts, independent of LeoFS version (e.g. \n/usr/local/leofs/current/leo_manager_0/bin/leo_manager\n). Users that created symlink with that name themselves should remove it before installing new package.\n\n\nfor EL6 / EL7 packages: installing multiple versions of LeoFS package is not supported anymore. There can be only one version of \nleofs\n package installed in system; new version should only be installed as upgrade, with \nrpm -U\n / \nrpm -F\n.\n\n\nfor systemd-based distributions (Ubuntu 16.04, EL7) special upgrade steps are needed.\n\n\n\n\nProcedure of installing v1.3.8 upgrade for systemd-based distributions (Ubuntu 16.04 and EL7)\n\n\nWhen upgrading existing node running LeoFS package 1.3.7 or earlier to v1.3.8 or later, please follow these steps:\n\n\n\n\nStop currently running LeoFS node. Backup current config files for that node, if needed.\n\n\nExecute \npgrep -a epmd\n and make sure there is \nno output\n, i.e. no instances of epmd are running. If there are any, kill them with \npkill epmd\n command. Never kill \nepmd\n while LeoFS node is still running, though, because it will render it non-functional until restart.\n\n\nFor EL7, execute \nrpm -q leofs\n and make sure that no more than single version of package is installed.\n\n\nUpgrade to new package version (dpkg -i for Ubuntu 16.04, rpm -U for EL7). After that, follow the usual procedure for LeoFS upgrade between versions.\n\n\n\n\nIf an error was made and epmd wasn't stopped before installation of the new package, an error similar to this will appear during package upgrade:\n\n\n1\nJob for leofs-epmd.socket failed. See \nsystemctl status leofs-epmd.socket\n and \njournalctl -xe\n for details.\n\n\n\n\n\n\nIn that case, users must do the following before trying to start the new version:\n\n\n\n\nAfter making sure that LeoFS nodes aren't running on this system, kill running epmd instance (e.g. \npkill epmd\n)\n\n\nExecute \nsystemctl start leofs-epmd.socket\n - there should be no output if everything was done correctly\n\n\n\n\n(or, alternatively, system reboot will take care of this problem as well).\n\n\nThese special steps are not needed for installing upgrades after v1.3.8.\n\n\nAfter upgrade, users of systemd-based distributions might be interested in switching to new way of launching nodes which offers improvements for system administration and new features like automatic startup/shutdown of LeoFS node on startup or reboot. Please read \nFor Administrators / System Operations / Systemd Services\n for more information.\n\n\nChanges in LeoFS' Packages (1.3.3+)\n\n\nStarting from v1.3.3, all LeoFS nodes are running as non-privileged user \nleofs\n in \nthe official Linux packages\n. It should work out of the box for new installations and for new nodes on existing installations. However, for existing nodes upgrading to v1.3.3 \n(or later)\n from previous versions, the change might be not seamless.\n\n\nExtra Steps\n\n\nRunning LeoFS with Default Paths\n\n\nFor those who have LeoFS configured with:\n\n\n\n\nqueue\n and \nmnesia\n in \n/usr/local/leofs/\nversion\n/leo_*/work\n\n\nlog files\n in \n/usr/local/leofs/\nversion\n/leo_*/log\n\n\nLeoStorage data files in \n/usr/local/leofs/\nversion\n/leo_storage/avs\n\n\n\n\nProcedures\n\n\nMigrate Files and Directories\n\n\nDuring upgrade of node \n(of any type)\n, \nafter\n stopping the old version and copying or moving every files to be moved into the new directories, change the owner with the commands below. It has to be done \nbefore\n launching the\nnew version.\n\n\n1\n2\n3\n4\n# chown -R leofs:leofs /usr/local/leofs/%version/leo_storage/avs\n# chown -R leofs:leofs /usr/local/leofs/%version/leo_gateway/cache\n# chown -R leofs:leofs /usr/local/leofs/%version/leo_*/log\n# chown -R leofs:leofs /usr/local/leofs/%version/leo_*/work\n\n\n\n\n\n\nRemove Unnecessary Directories\n\n\nRemove old temporary directory used by launch scripts. This step is needed because when earlier version was launched with \nroot\n permissions, it creates a set of temporary directories in \n/tmp\n which cannot be re-used by non-privileged user as is, and launch scripts will fail with obscure messages - or with no message at all, except for an error in syslog \n(usually \n/var/log/messages\n)\n.\n\n\n1\n# rm -rf /tmp/usr\n\n\n\n\n\n\nRe-launch the System\n\n\nStart the node through its launch script, as per upgrade flow diagram.\n\n\nRunning LeoFS with customized paths\n\n\nFor those who have LeoFS configured with, for example:\n\n\n\n\nqueue\n and \nmnesia\n in \n/mnt/work\n\n\nlog files\n in \n/var/log/leofs\n\n\n\n\nLeoStorage data files in \n/mnt/avs\n\n\n\n\n\n\nBefore starting new version of a node, execute \nchown -R leofs:leofs \n..\n for all these external directories\n\n\n\n\n\n\nDon't forget to remove temporary directory \n(\nrm -rf /tmp/usr\n)\n as well for the reasons described above.\n\n\n\n\n\n\nThese users might be interested in new features of \nenvironment config files\n, which allow to redefine some environment variables like paths in launch script.\n\n\nRefer \nFor Administrators / Settings / Environment Configuration\n for more information.\n\n\nRunning LeoFS with customized launch scripts\n\n\nFor those who have LeoFS already running as non-privileged user.\n\n\n\n\n\n\nScripts that are provided by packages generally should be enough to run on most configurations without changes. If needed, change user from \nleofs\n to some other in \"environment\" config files \n(e.g. \nRUNNER_USER=localuser\n)\n. Refer to the later section for more details about environment config files.\n\n\n\n\n\n\nPossible pitfall includes ownership of \n/usr/local/leofs/.erlang.cookie\n file, which is set to \nleofs\n during package installation. This should only be a problem when trying to run LeoFS nodes with permissions of some user which is not called \nleofs\n, but has home directory set to \n/usr/local/leofs\n. This is not supported due to technical reasons. Home directory of that user must be set to something else.\n\n\n\n\n\n\nRunning LeoFS in any form and keeping LeoFS running as \nroot\n\n\nFor those who want to keep maximum compatibility with the previous installation.\nIn \nenvironment config file\n, set \nRUNNER_USER\n:\n\n\n1\nRUNNER_USER\n=\nroot\n\n\n\n\n\n\n\nNote that switching this node to run as non-privileged user later will require extra steps to carefully change all permissions. This is not recommended, but possible \n(at very least, in addition to \nchown\n commands from before, permissions of \nleo_*/etc\n and \nleo_*/snmp/*/db\n will have to be changed recursively as well)\n.\n\n\nNote for Developers\n\n\nAs described at the previous section, the default user running LeoFS processes has changed to \nleofs\n so that requires developers to\n\n\n\n\nTweak environment config files to set \nRUNNER_USER\n to the user you have logged in while developing with \nmake release/bootstrap.sh/mdcr.sh\n.\n\n\nRemove all files under \n$PIPE_DIR\n before starting any LeoFS processes.\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / System Administration / Persistent Configuration Location\n\n\nFor Administrators / Settings / Environment Configuration\n\n\nFor Administrators / Settings / LeoManager Settings\n\n\nFor Administrators / Settings / LeoStorage Settings\n\n\nFor Administrators / Settings / LeoGateway Settings\n\n\nFor Administrators / System Operations / Systemd Services", 
            "title": "System Migration"
        }, 
        {
            "location": "/admin/system_admin/migration/#system-migration", 
            "text": "", 
            "title": "System Migration"
        }, 
        {
            "location": "/admin/system_admin/migration/#upgrade-an-older-leofs-version-to-the-latest-one", 
            "text": "", 
            "title": "Upgrade an older LeoFS version to the latest one"
        }, 
        {
            "location": "/admin/system_admin/migration/#operation-flow", 
            "text": "If you would like to migrate a LeoFS system, you can achieve that by following the operation flow.    See the large diagram", 
            "title": "Operation Flow"
        }, 
        {
            "location": "/admin/system_admin/migration/#takeover-and-adjust-confugurations", 
            "text": "Before getting started with the migration of a LeoFS system, you need to take over the configuration, then adjust the paths and set the new configurations.\nIt is possible to simplify this by storing node configuration in  /etc/leofs , independent on version. Please refer to  For Administrators / System Administration / Persistent Configuration  for instructions on enabling and using this feature.", 
            "title": "Takeover and Adjust Confugurations"
        }, 
        {
            "location": "/admin/system_admin/migration/#added-or-changed-leofs-configurations", 
            "text": "", 
            "title": "Added Or Changed LeoFS' Configurations"
        }, 
        {
            "location": "/admin/system_admin/migration/#leomanager-master", 
            "text": "[since v1.3.3]  LeoFS' MDC replication feature was improved. Some configuration are added in  the configuration of LeoManager's master .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 ## --------------------------------------------------------------------  ## MANAGER - Multi DataCenter Settings  ## --------------------------------------------------------------------  ## A number of replication targets  mdc_replication.max_targets   =   2  ## A number of replicas per a datacenter  ## [note] A local LeoFS sends a stacked object which contains an items of a replication method:  ##          - [L1_N] A number of replicas  ##          - [L1_W] A number of replicas needed for a successful WRITE operation  ##          - [L1_R] A number of replicas needed for a successful READ operation  ##          - [L1_D] A number of replicas needed for a successful DELETE operation  ##       A remote cluster of a LeoFS system which receives its object,  ##       and then replicates it by its contained reoplication method.  mdc_replication.num_of_replicas_a_dc   =   1  ## MDC replication / A number of replicas needed for a successful WRITE operation  mdc_replication.consistency.write   =   1  ## MDC replication / A number of replicas needed for a successful READ operation  mdc_replication.consistency.read   =   1  ## MDC replication / A number of replicas needed for a successful DELETE operation  mdc_replication.consistency.delete   =   1", 
            "title": "LeoManager Master"
        }, 
        {
            "location": "/admin/system_admin/migration/#leostorage", 
            "text": "[since v1.3.3]  Data synchronization configuration are added in  the configuration of LeoStorage .  1\n2\n3\n4\n5\n6 ## Mode of the data synchronization - [none, periodic, writethrough]  ## - default:none  obj_containers.sync_mode   =   none  ## Interval in ms of the data synchronization - default: 1000ms  obj_containers.sync_interval_in_ms   =   1000", 
            "title": "LeoStorage"
        }, 
        {
            "location": "/admin/system_admin/migration/#changes-in-leofs-packages-138", 
            "text": "Starting from v1.3.8 there are changes in  the official Linux packages . They should work out of the box for new installations and for new nodes on existing installations. However, one needs to pay special attention during upgrading existing nodes to v1.3.8.   for all supported distributions: there is now symlink  /usr/local/leofs/current  which points to  /usr/local/leofs/ version , this allows fixed path to launch scripts, independent of LeoFS version (e.g.  /usr/local/leofs/current/leo_manager_0/bin/leo_manager ). Users that created symlink with that name themselves should remove it before installing new package.  for EL6 / EL7 packages: installing multiple versions of LeoFS package is not supported anymore. There can be only one version of  leofs  package installed in system; new version should only be installed as upgrade, with  rpm -U  /  rpm -F .  for systemd-based distributions (Ubuntu 16.04, EL7) special upgrade steps are needed.", 
            "title": "Changes in LeoFS' Packages (1.3.8+)"
        }, 
        {
            "location": "/admin/system_admin/migration/#procedure-of-installing-v138-upgrade-for-systemd-based-distributions-ubuntu-1604-and-el7", 
            "text": "When upgrading existing node running LeoFS package 1.3.7 or earlier to v1.3.8 or later, please follow these steps:   Stop currently running LeoFS node. Backup current config files for that node, if needed.  Execute  pgrep -a epmd  and make sure there is  no output , i.e. no instances of epmd are running. If there are any, kill them with  pkill epmd  command. Never kill  epmd  while LeoFS node is still running, though, because it will render it non-functional until restart.  For EL7, execute  rpm -q leofs  and make sure that no more than single version of package is installed.  Upgrade to new package version (dpkg -i for Ubuntu 16.04, rpm -U for EL7). After that, follow the usual procedure for LeoFS upgrade between versions.   If an error was made and epmd wasn't stopped before installation of the new package, an error similar to this will appear during package upgrade:  1 Job for leofs-epmd.socket failed. See  systemctl status leofs-epmd.socket  and  journalctl -xe  for details.   In that case, users must do the following before trying to start the new version:   After making sure that LeoFS nodes aren't running on this system, kill running epmd instance (e.g.  pkill epmd )  Execute  systemctl start leofs-epmd.socket  - there should be no output if everything was done correctly   (or, alternatively, system reboot will take care of this problem as well).  These special steps are not needed for installing upgrades after v1.3.8.  After upgrade, users of systemd-based distributions might be interested in switching to new way of launching nodes which offers improvements for system administration and new features like automatic startup/shutdown of LeoFS node on startup or reboot. Please read  For Administrators / System Operations / Systemd Services  for more information.", 
            "title": "Procedure of installing v1.3.8 upgrade for systemd-based distributions (Ubuntu 16.04 and EL7)"
        }, 
        {
            "location": "/admin/system_admin/migration/#changes-in-leofs-packages-133", 
            "text": "Starting from v1.3.3, all LeoFS nodes are running as non-privileged user  leofs  in  the official Linux packages . It should work out of the box for new installations and for new nodes on existing installations. However, for existing nodes upgrading to v1.3.3  (or later)  from previous versions, the change might be not seamless.", 
            "title": "Changes in LeoFS' Packages (1.3.3+)"
        }, 
        {
            "location": "/admin/system_admin/migration/#extra-steps", 
            "text": "", 
            "title": "Extra Steps"
        }, 
        {
            "location": "/admin/system_admin/migration/#running-leofs-with-default-paths", 
            "text": "For those who have LeoFS configured with:   queue  and  mnesia  in  /usr/local/leofs/ version /leo_*/work  log files  in  /usr/local/leofs/ version /leo_*/log  LeoStorage data files in  /usr/local/leofs/ version /leo_storage/avs", 
            "title": "Running LeoFS with Default Paths"
        }, 
        {
            "location": "/admin/system_admin/migration/#procedures", 
            "text": "", 
            "title": "Procedures"
        }, 
        {
            "location": "/admin/system_admin/migration/#migrate-files-and-directories", 
            "text": "During upgrade of node  (of any type) ,  after  stopping the old version and copying or moving every files to be moved into the new directories, change the owner with the commands below. It has to be done  before  launching the\nnew version.  1\n2\n3\n4 # chown -R leofs:leofs /usr/local/leofs/%version/leo_storage/avs\n# chown -R leofs:leofs /usr/local/leofs/%version/leo_gateway/cache\n# chown -R leofs:leofs /usr/local/leofs/%version/leo_*/log\n# chown -R leofs:leofs /usr/local/leofs/%version/leo_*/work", 
            "title": "Migrate Files and Directories"
        }, 
        {
            "location": "/admin/system_admin/migration/#remove-unnecessary-directories", 
            "text": "Remove old temporary directory used by launch scripts. This step is needed because when earlier version was launched with  root  permissions, it creates a set of temporary directories in  /tmp  which cannot be re-used by non-privileged user as is, and launch scripts will fail with obscure messages - or with no message at all, except for an error in syslog  (usually  /var/log/messages ) .  1 # rm -rf /tmp/usr", 
            "title": "Remove Unnecessary Directories"
        }, 
        {
            "location": "/admin/system_admin/migration/#re-launch-the-system", 
            "text": "Start the node through its launch script, as per upgrade flow diagram.", 
            "title": "Re-launch the System"
        }, 
        {
            "location": "/admin/system_admin/migration/#running-leofs-with-customized-paths", 
            "text": "For those who have LeoFS configured with, for example:   queue  and  mnesia  in  /mnt/work  log files  in  /var/log/leofs   LeoStorage data files in  /mnt/avs    Before starting new version of a node, execute  chown -R leofs:leofs  ..  for all these external directories    Don't forget to remove temporary directory  ( rm -rf /tmp/usr )  as well for the reasons described above.    These users might be interested in new features of  environment config files , which allow to redefine some environment variables like paths in launch script.  Refer  For Administrators / Settings / Environment Configuration  for more information.", 
            "title": "Running LeoFS with customized paths"
        }, 
        {
            "location": "/admin/system_admin/migration/#running-leofs-with-customized-launch-scripts", 
            "text": "For those who have LeoFS already running as non-privileged user.    Scripts that are provided by packages generally should be enough to run on most configurations without changes. If needed, change user from  leofs  to some other in \"environment\" config files  (e.g.  RUNNER_USER=localuser ) . Refer to the later section for more details about environment config files.    Possible pitfall includes ownership of  /usr/local/leofs/.erlang.cookie  file, which is set to  leofs  during package installation. This should only be a problem when trying to run LeoFS nodes with permissions of some user which is not called  leofs , but has home directory set to  /usr/local/leofs . This is not supported due to technical reasons. Home directory of that user must be set to something else.", 
            "title": "Running LeoFS with customized launch scripts"
        }, 
        {
            "location": "/admin/system_admin/migration/#running-leofs-in-any-form-and-keeping-leofs-running-as-root", 
            "text": "For those who want to keep maximum compatibility with the previous installation.\nIn  environment config file , set  RUNNER_USER :  1 RUNNER_USER = root    Note that switching this node to run as non-privileged user later will require extra steps to carefully change all permissions. This is not recommended, but possible  (at very least, in addition to  chown  commands from before, permissions of  leo_*/etc  and  leo_*/snmp/*/db  will have to be changed recursively as well) .", 
            "title": "Running LeoFS in any form and keeping LeoFS running as root"
        }, 
        {
            "location": "/admin/system_admin/migration/#note-for-developers", 
            "text": "As described at the previous section, the default user running LeoFS processes has changed to  leofs  so that requires developers to   Tweak environment config files to set  RUNNER_USER  to the user you have logged in while developing with  make release/bootstrap.sh/mdcr.sh .  Remove all files under  $PIPE_DIR  before starting any LeoFS processes.", 
            "title": "Note for Developers"
        }, 
        {
            "location": "/admin/system_admin/migration/#related-links", 
            "text": "For Administrators / System Administration / Persistent Configuration Location  For Administrators / Settings / Environment Configuration  For Administrators / Settings / LeoManager Settings  For Administrators / Settings / LeoStorage Settings  For Administrators / Settings / LeoGateway Settings  For Administrators / System Operations / Systemd Services", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_admin/integration/", 
            "text": "System Integration\n\n\nCDN Integration\n\n\nHow To Integrate LeoFS With CDN\n\n\nThere is nothing special to do for integrating LeoFS with CDN\n1\n. Since almost CDN service providers take care of a CacheControl Header received from an origin to determine how long a file should be cached on their edge servers, so if you want to modify TTL according to URLs, you can do it by using \nhttp_custom_header.conf\n.\n\n\nHow To Use \nhttp_custom_header.conf\n\n\nAppend the following line to \nleo_gateway.conf\n which contains a LeoGateway's directory. Arrange the \nhttp_custom_header.conf\n into the path specified at \nleo_gateway.conf\n.\n\n\n1\n2\n## HTTP custom header configuration file path\n\n\nhttp.headers_config_file\n \n=\n \n./etc/http_custom_header.conf\n\n\n\n\n\n\n\nHow To Write \nhttp_custom_header.conf\n\n\nThe syntax is a subset of \nNginx\n2\n configuration\n. You can use location contexts to specify TTL and add any headers to the path.\n\n\n1\n2\n3\n4\n5\nlocation bucket/static {\n    expires    12h;\n    add_header Cache-Control public;\n    add_header X-OriginalHeader OriginalValue;\n}\n\n\n\n\n\n\nIn this case, assuming that a CDN service already has been enabled, and there is a file at \nbucket/static/path_to_file\n, if a user browses that file via the CDN. The CDN will receive a response from a LeoFS system with customized Http headers.\n\n\n1\n2\nCache-Control\n:\n \npublic\n,\n \nmax-age\n=\n43200\n;\n\n\nX-OriginalHeader\n:\n \nOriginalValue\n;\n\n\n\n\n\n\n\nUse Cases\n\n\nSpecify TTL by the bucket.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nlocation bucket1 {\n    expires    1h;\n    add_header Cache-Control public;\n}\nlocation bucket2 {\n    expires    1d;\n    add_header Cache-Control public;\n}\nlocation bucket3 {\n    expires    1h30m;\n    add_header Cache-Control private;\n}\n\n\n\n\n\n\nAppendix\n\n\nSyntax for the expire field\n\n\nLeoFS supports a part of measurement units which can be used in Nginx configuration. Following time intervals can be specified.\n\n\n1\n2\n3\n4\ns\n:\n \nseconds\n\n\nm\n:\n \nminutes\n\n\nh\n:\n \nhours\n\n\nd\n:\n \ndays\n\n\n\n\n\n\n\nList of verified CDN services\n\n\nLeoFS Team tested the following CDN services with LeoFS. We recognize that other CDN services also should work.\n\n\n\n\nAmazon CloudFront \u2013 Content Delivery Network (CDN)\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoGateway Settings\n\n\n\n\n\n\n\n\n\n\n\n\nCDN, Content delivery network\n\n\n\n\n\n\nNginx, An HTTP and reverse proxy server", 
            "title": "System Integration"
        }, 
        {
            "location": "/admin/system_admin/integration/#system-integration", 
            "text": "", 
            "title": "System Integration"
        }, 
        {
            "location": "/admin/system_admin/integration/#cdn-integration", 
            "text": "", 
            "title": "CDN Integration"
        }, 
        {
            "location": "/admin/system_admin/integration/#how-to-integrate-leofs-with-cdn", 
            "text": "There is nothing special to do for integrating LeoFS with CDN 1 . Since almost CDN service providers take care of a CacheControl Header received from an origin to determine how long a file should be cached on their edge servers, so if you want to modify TTL according to URLs, you can do it by using  http_custom_header.conf .", 
            "title": "How To Integrate LeoFS With CDN"
        }, 
        {
            "location": "/admin/system_admin/integration/#how-to-use-http_custom_headerconf", 
            "text": "Append the following line to  leo_gateway.conf  which contains a LeoGateway's directory. Arrange the  http_custom_header.conf  into the path specified at  leo_gateway.conf .  1\n2 ## HTTP custom header configuration file path  http.headers_config_file   =   ./etc/http_custom_header.conf", 
            "title": "How To Use http_custom_header.conf"
        }, 
        {
            "location": "/admin/system_admin/integration/#how-to-write-http_custom_headerconf", 
            "text": "The syntax is a subset of  Nginx 2  configuration . You can use location contexts to specify TTL and add any headers to the path.  1\n2\n3\n4\n5 location bucket/static {\n    expires    12h;\n    add_header Cache-Control public;\n    add_header X-OriginalHeader OriginalValue;\n}   In this case, assuming that a CDN service already has been enabled, and there is a file at  bucket/static/path_to_file , if a user browses that file via the CDN. The CDN will receive a response from a LeoFS system with customized Http headers.  1\n2 Cache-Control :   public ,   max-age = 43200 ;  X-OriginalHeader :   OriginalValue ;", 
            "title": "How To Write http_custom_header.conf"
        }, 
        {
            "location": "/admin/system_admin/integration/#use-cases", 
            "text": "Specify TTL by the bucket.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 location bucket1 {\n    expires    1h;\n    add_header Cache-Control public;\n}\nlocation bucket2 {\n    expires    1d;\n    add_header Cache-Control public;\n}\nlocation bucket3 {\n    expires    1h30m;\n    add_header Cache-Control private;\n}", 
            "title": "Use Cases"
        }, 
        {
            "location": "/admin/system_admin/integration/#appendix", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/admin/system_admin/integration/#syntax-for-the-expire-field", 
            "text": "LeoFS supports a part of measurement units which can be used in Nginx configuration. Following time intervals can be specified.  1\n2\n3\n4 s :   seconds  m :   minutes  h :   hours  d :   days", 
            "title": "Syntax for the expire field"
        }, 
        {
            "location": "/admin/system_admin/integration/#list-of-verified-cdn-services", 
            "text": "LeoFS Team tested the following CDN services with LeoFS. We recognize that other CDN services also should work.   Amazon CloudFront \u2013 Content Delivery Network (CDN)", 
            "title": "List of verified CDN services"
        }, 
        {
            "location": "/admin/system_admin/integration/#related-links", 
            "text": "For Administrators / Settings / LeoGateway Settings       CDN, Content delivery network    Nginx, An HTTP and reverse proxy server", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_admin/multiple_nodes/", 
            "text": "Mutiple instances of nodes on the same system\n\n\nDescription and requirements\n\n\nStarting from v1.3.8, there is support for running multiple instances of the same node types on a single system (without use of containers or other type of isolation). This is useful for users who want to do things like\n\n\n\n\nRunning two (or more) gateways on the same system (in the same or different mode, e.g. S3 gateway + REST gateway)\n\n\nRunning two (or more) completely isolated clusters with different settings on the same set of storage servers\n\n\n\n\nThis feature currently works only on Linux. Other prerequisites for activating it are:\n\n\n\n\nNodes must be launched as systemd services. See \nFor Administrators / System Operations / Systemd Services\n for details on enabling this feature.\n\n\nNodes must load configuration from \n/etc/leofs\n. See \nFor Administrators / System Administration / Persistent Configuration Location\n for details on enabling this feature.\n\n\nAll paths and ports\n in config files must be explicitly set for each instance and point to different locations. It's up to user to ensure that there are no conflicting paths.\n\n\n\n\nThis is advanced feature aimed at existing LeoFS users who need to run another cluster or more gateways using the same hardware; it is recommended to have a working cluster where all work and log directories are configured to use non-default locations before enabling this.\n\n\nPreparing more instances of the same node\n\n\nLet's say we have LeoStorage instance running through systemd (\nleofs-storage\n service) and currently it loads configuration from \n/etc/leofs/leo_storage\n. We want to run another instance on the same system; it needs separate name. In this example, we'll call it \"second\". To create such instance:\n\n\n\n\nCreate local copy of systemd unit file with new name\n\n\nEL7\nversion: \ncp /usr/lib/systemd/system/leofs-storage.service /etc/systemd/system/leofs-storage-second.service\n.\n\n\nUbuntu\n16.04\nversion: \ncp /lib/systemd/system/leofs-storage.service /etc/systemd/system/leofs-storage-second.service\n.\n\n\n\n\n\n\nEdit \n/etc/systemd/system/leofs-storage-second.service\n, changing \nEnvironment=NODE_EXTRA_NAME=\n line to \nEnvironment=NODE_EXTRA_NAME=second\n.\n\n\nReload systemd configuration: \nsystemctl daemon-reload\n.\n\n\n\n\nFirst step here depends on distribution because default location of systemd unit file can differ; it can be checked in \"UNIT FILE LOAD PATH\" section of systemd.unit(5) man page.\n\n\nThis instance would try to load configuration from \n/etc/leofs/leo_storage_second\n upon launch. This configuration hierarchy needs to be created as well:\n\n\n\n\nCreate new configuration directory and *.d subdirectory: \nmkdir -p /etc/leofs/leo_storage_second/leo_storage.d\n.\n\n\nChange permissions of configuration directory so it's \nwritable\n by user running LeoFS (\"leofs\", unless changed manually): \nchown leofs:leofs /etc/leofs/leo_storage_second\n.\n\n\nCopy main configuration file (if you plan to change it) or symlink it (if you only plan to use conf.d-style configuration override): \ncp /etc/leofs/leo_storage/leo_storage.conf /etc/leofs/leo_storage_second/\n.\n\n\nChange configuration of new node, ensuring all paths and ports point to different location from first instance of that node.\n\n\nCreate directories (logs/queue/etc) for the new node and set appropriate permissions on them.\n\n\n\n\nList of options that must have different settings\n\n\nAll these options must be explicitly set in config files and have different settings for each instance of the node on the same system. For gateway and storage:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nnodename\n\nsasl.sasl_error_log\nsasl.error_logger_mf_dir\n\nlog.erlang\nlog.app\nlog.member_dir\nlog.ring_dir\n\nqueue_dir\nerlang.crash_dump\nsnmp_conf\n\n\n\n\n\n\nFor storage nodes, additionally:\n\n\n1\n2\n3\nrpc.server.listen_port\nobj_containers.path\nleo_ordning_reda.temp_stacked_dir\n\n\n\n\n\n\nFor gateways running in HTTP mode (S3 / REST), additionally:\n\n\n1\n2\nhttp.port\nhttp.ssl_port\n\n\n\n\n\n\nPlease note that this list might be not complete as new options might be added to config files. When using this feature, \nit's up to user to ensure that all paths above point to different directories\n. Trying to share the same ports or snmp_conf setting between nodes would result in second instance failing to start, but trying to share log or queue directory could lead to disastrous results. \nAlways double-check that all paths are different before using this feature.\n\n\nStarting other node instances\n\n\nAfter second instance is properly configured, it can be started in parallel with the first one with \nsystemctl start leofs-storage-second\n command.\n\n\nIt is possible to check whether nodes actually try to load configuration from different directories (\ndescribed here in more detail\n):\n\n\n1\n2\n3\n4\n5\n6\n7\n# journalctl -l -u leofs-storage | grep -e Exec -e Config\nSep 20 17:41:42 stor01.lan leo_storage[9513]: Config path: /etc/leofs/leo_storage\nSep 20 17:41:42 stor01.lan leo_storage[9513]: Exec: /usr/local/leofs/current/leo_storage/erts-9.0/bin/erlexec -noinput -boot /usr/local/leofs/current/leo_storage/releases/1/leo_storage -mode minimal -config /etc/leofs/leo_storage/app.config -args_file /etc/leofs/leo_storage/vm.args -- console\n\n# journalctl -l -u leofs-storage-second | grep -e Exec -e Config\nSep 20 17:40:43 stor01.lan leo_storage[9065]: Config path: /etc/leofs/leo_storage_second\nSep 20 17:40:43 stor01.lan leo_storage[9065]: Exec: /usr/local/leofs/current/leo_storage/erts-9.0/bin/erlexec -noinput -boot /usr/local/leofs/current/leo_storage/releases/1/leo_storage -mode minimal -config /etc/leofs/leo_storage_second/app.config -args_file /etc/leofs/leo_storage_second/vm.args -- console\n\n\n\n\n\n\n\n\nNote: Multiple LeoManager instances\n\n\nIt is possible to run multiple instances of LeoManager as well, but the resulting configuration and \nleofs-adm\n operations would be more complicated, so it's not recommended (as results of error might be severe). It would be better to use separate VMs or containers for different LeoManagers.\n\n\n\n\n\n\nNote: Interacting with other instances through launch script\n\n\nIt is possible to interact with extra node instances through original launch script, however it requires a bit roundabout way of exporting environment variable as the owner of process. Example: \nsudo -H -u leofs sh -c \nexport NODE_EXTRA_NAME=second; /usr/local/leofs/current/leo_storage/bin/leo_storage remote_console\n\n\n\n\nExample configuration\n\n\nHere is fully working configuration example for two storage nodes belonging to two different clusters and working on the same system. Second instance is called \"second\" here (unit file for it is located at \n/etc/systemd/system/leofs-storage-second.service\n and it contains \nEnvironment=NODE_EXTRA_NAME=second\n setting).\n\n\nDirectories and permissions:\n\n\n1\n2\n3\n# ls -ld /etc/leofs/leo_storage /etc/leofs/leo_storage_second/\ndrwxr-sr-x. 3 leofs leofs 4096 Sep 22 20:47 /etc/leofs/leo_storage\ndrwxr-sr-x. 3 leofs leofs 4096 Sep 20 17:44 /etc/leofs/leo_storage_second/\n\n\n\n\n\n\nMain configuration for second node is symlinked to first - preferred when configuring nodes through configuration override files. This allows automatically getting new default settings with package upgrades (which include new version of \n/etc/leofs/leo_storage/leo_storage.conf\n):\n\n\n1\n2\n# ls -l /etc/leofs/leo_storage_second/leo_storage.conf\nlrwxrwxrwx. 1 root leofs 31 Sep 19 21:09 /etc/leofs/leo_storage_second/leo_storage.conf -\n ../leo_storage/leo_storage.conf\n\n\n\n\n\n\nContents of \n/etc/leofs/leo_storage/leo_storage.d/20-leo_storage.conf\n - configuration for the first node:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nnodename\n \n=\n \nstor01\n@192.168.0.20\n\n\nmanagers\n \n=\n \n[\nmanager_0\n@192.168.0.10\n,\n \nmanager_1\n@192.168.0.11\n]\n\n\n\nobj_containers\n.\npath\n \n=\n \n[\n/\nmnt\n/\navs\n/\nmain\n]\n\n\nobj_containers\n.\nnum_of_containers\n \n=\n \n[\n64\n]\n\n\n\nsasl\n.\nsasl_error_log\n \n=\n \n/\nvar\n/\nlog\n/\nleofs\n/\nleo_storage\n/\nsasl\n/\nsasl\n-\nerror\n.\nlog\n\n\nsasl\n.\nerror_logger_mf_dir\n \n=\n \n/\nvar\n/\nlog\n/\nleofs\n/\nleo_storage\n/\nsasl\n\n\n\nlog\n.\nerlang\n \n=\n \n/\nvar\n/\nlog\n/\nleofs\n/\nleo_storage\n/\nerlang\n\n\nlog\n.\napp\n \n=\n \n/\nvar\n/\nlog\n/\nleofs\n/\nleo_storage\n/\napp\n\n\nlog\n.\nmember_dir\n \n=\n \n/\nvar\n/\nlog\n/\nleofs\n/\nleo_storage\n/\nring\n\n\nlog\n.\nring_dir\n \n=\n \n/\nvar\n/\nlog\n/\nleofs\n/\nleo_storage\n/\nring\n\n\n\nqueue_dir\n  \n=\n \n/\nvar\n/\nleofs\n/\nleo_storage\n/\nwork\n/\nqueue\n\n\nleo_ordning_reda\n.\ntemp_stacked_dir\n \n=\n \n/\nvar\n/\nleofs\n/\nleo_storage\n/\nwork\n/\nord_reda\n/\n\n\nerlang\n.\ncrash_dump\n \n=\n \n/\nvar\n/\nlog\n/\nleofs\n/\nleo_storage\n/\nerl_crash\n.\ndump\n\n\n\nsnmp_conf\n \n=\n \n.\n/\nsnmp\n/\nsnmpa_storage_0\n/\nleo_storage_snmp\n\n\n\n\n\n\n\nContents of \n/etc/leofs/leo_storage_second/leo_storage.d/20-leo_storage.conf\n - configuration for second node\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nnodename\n \n=\n \nstor_second01\n@192.168.0.20\n\n\nmanagers\n \n=\n \n[\nmanager_0\n@192.168.0.15\n,\n \nmanager_1\n@192.168.0.16\n]\n\n\n\nrpc\n.\nserver\n.\nlisten_port\n \n=\n \n13078\n\n\n\nobj_containers\n.\npath\n \n=\n \n[\n/\nmnt\n/\navs\n/\nsecond\n]\n\n\nobj_containers\n.\nnum_of_containers\n \n=\n \n[\n32\n]\n\n\n\nobject_storage\n.\nis_strict_check\n \n=\n \ntrue\n\n\nwatchdog\n.\nerror\n.\nis_enabled\n \n=\n \ntrue\n\n\nautonomic_op\n.\ncompaction\n.\nis_enabled\n \n=\n \ntrue\n\n\n\nsasl\n.\nsasl_error_log\n \n=\n \n/\nvar\n/\nlog\n/\nleofs\n/\nleo_storage_second\n/\nsasl\n/\nsasl\n-\nerror\n.\nlog\n\n\nsasl\n.\nerror_logger_mf_dir\n \n=\n \n/\nvar\n/\nlog\n/\nleofs\n/\nleo_storage_second\n/\nsasl\n\n\n\nlog\n.\nerlang\n \n=\n \n/\nvar\n/\nlog\n/\nleofs\n/\nleo_storage_second\n/\nerlang\n\n\nlog\n.\napp\n \n=\n \n/\nvar\n/\nlog\n/\nleofs\n/\nleo_storage_second\n/\napp\n\n\nlog\n.\nmember_dir\n \n=\n \n/\nvar\n/\nlog\n/\nleofs\n/\nleo_storage_second\n/\nring\n\n\nlog\n.\nring_dir\n \n=\n \n/\nvar\n/\nlog\n/\nleofs\n/\nleo_storage_second\n/\nring\n\n\n\nqueue_dir\n  \n=\n \n/\nvar\n/\nleofs\n/\nleo_storage_second\n/\nwork\n/\nqueue\n\n\nleo_ordning_reda\n.\ntemp_stacked_dir\n \n=\n \n/\nvar\n/\nleofs\n/\nleo_storage_second\n/\nwork\n/\nord_reda\n/\n\n\nerlang\n.\ncrash_dump\n \n=\n \n/\nvar\n/\nlog\n/\nleofs\n/\nleo_storage_second\n/\nerl_crash\n.\ndump\n\n\n\nsnmp_conf\n \n=\n \n.\n/\nsnmp\n/\nsnmpa_storage_1\n/\nleo_storage_snmp\n\n\n\n\n\n\n\nEach node requires own set of directories with appropriate permissions:\n\n\n1\n2\n3\n4\n# mkdir -p /var/leofs/leo_storage/work/queue /var/log/leofs/leo_storage/app /var/leofs/leo_storage/work/ord_reda /var/log/leofs/leo_storage/sasl\n# chown -R leofs:leofs /var/leofs/leo_storage/ /var/log/leofs/\n# mkdir -p /var/leofs/leo_storage_second/work/queue /var/log/leofs/leo_storage_second/app /var/leofs/leo_storage_second/work/ord_reda /var/log/leofs/leo_storage_second/sasl\n# chown -R leofs:leofs /var/leofs/leo_storage_second/\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / System Operations / Systemd Services\n\n\nFor Administrators / System Administration / Persistent Configuration Location", 
            "title": "Multiple Node Instances"
        }, 
        {
            "location": "/admin/system_admin/multiple_nodes/#mutiple-instances-of-nodes-on-the-same-system", 
            "text": "", 
            "title": "Mutiple instances of nodes on the same system"
        }, 
        {
            "location": "/admin/system_admin/multiple_nodes/#description-and-requirements", 
            "text": "Starting from v1.3.8, there is support for running multiple instances of the same node types on a single system (without use of containers or other type of isolation). This is useful for users who want to do things like   Running two (or more) gateways on the same system (in the same or different mode, e.g. S3 gateway + REST gateway)  Running two (or more) completely isolated clusters with different settings on the same set of storage servers   This feature currently works only on Linux. Other prerequisites for activating it are:   Nodes must be launched as systemd services. See  For Administrators / System Operations / Systemd Services  for details on enabling this feature.  Nodes must load configuration from  /etc/leofs . See  For Administrators / System Administration / Persistent Configuration Location  for details on enabling this feature.  All paths and ports  in config files must be explicitly set for each instance and point to different locations. It's up to user to ensure that there are no conflicting paths.   This is advanced feature aimed at existing LeoFS users who need to run another cluster or more gateways using the same hardware; it is recommended to have a working cluster where all work and log directories are configured to use non-default locations before enabling this.", 
            "title": "Description and requirements"
        }, 
        {
            "location": "/admin/system_admin/multiple_nodes/#preparing-more-instances-of-the-same-node", 
            "text": "Let's say we have LeoStorage instance running through systemd ( leofs-storage  service) and currently it loads configuration from  /etc/leofs/leo_storage . We want to run another instance on the same system; it needs separate name. In this example, we'll call it \"second\". To create such instance:   Create local copy of systemd unit file with new name  EL7 version:  cp /usr/lib/systemd/system/leofs-storage.service /etc/systemd/system/leofs-storage-second.service .  Ubuntu 16.04 version:  cp /lib/systemd/system/leofs-storage.service /etc/systemd/system/leofs-storage-second.service .    Edit  /etc/systemd/system/leofs-storage-second.service , changing  Environment=NODE_EXTRA_NAME=  line to  Environment=NODE_EXTRA_NAME=second .  Reload systemd configuration:  systemctl daemon-reload .   First step here depends on distribution because default location of systemd unit file can differ; it can be checked in \"UNIT FILE LOAD PATH\" section of systemd.unit(5) man page.  This instance would try to load configuration from  /etc/leofs/leo_storage_second  upon launch. This configuration hierarchy needs to be created as well:   Create new configuration directory and *.d subdirectory:  mkdir -p /etc/leofs/leo_storage_second/leo_storage.d .  Change permissions of configuration directory so it's  writable  by user running LeoFS (\"leofs\", unless changed manually):  chown leofs:leofs /etc/leofs/leo_storage_second .  Copy main configuration file (if you plan to change it) or symlink it (if you only plan to use conf.d-style configuration override):  cp /etc/leofs/leo_storage/leo_storage.conf /etc/leofs/leo_storage_second/ .  Change configuration of new node, ensuring all paths and ports point to different location from first instance of that node.  Create directories (logs/queue/etc) for the new node and set appropriate permissions on them.", 
            "title": "Preparing more instances of the same node"
        }, 
        {
            "location": "/admin/system_admin/multiple_nodes/#list-of-options-that-must-have-different-settings", 
            "text": "All these options must be explicitly set in config files and have different settings for each instance of the node on the same system. For gateway and storage:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 nodename\n\nsasl.sasl_error_log\nsasl.error_logger_mf_dir\n\nlog.erlang\nlog.app\nlog.member_dir\nlog.ring_dir\n\nqueue_dir\nerlang.crash_dump\nsnmp_conf   For storage nodes, additionally:  1\n2\n3 rpc.server.listen_port\nobj_containers.path\nleo_ordning_reda.temp_stacked_dir   For gateways running in HTTP mode (S3 / REST), additionally:  1\n2 http.port\nhttp.ssl_port   Please note that this list might be not complete as new options might be added to config files. When using this feature,  it's up to user to ensure that all paths above point to different directories . Trying to share the same ports or snmp_conf setting between nodes would result in second instance failing to start, but trying to share log or queue directory could lead to disastrous results.  Always double-check that all paths are different before using this feature.", 
            "title": "List of options that must have different settings"
        }, 
        {
            "location": "/admin/system_admin/multiple_nodes/#starting-other-node-instances", 
            "text": "After second instance is properly configured, it can be started in parallel with the first one with  systemctl start leofs-storage-second  command.  It is possible to check whether nodes actually try to load configuration from different directories ( described here in more detail ):  1\n2\n3\n4\n5\n6\n7 # journalctl -l -u leofs-storage | grep -e Exec -e Config\nSep 20 17:41:42 stor01.lan leo_storage[9513]: Config path: /etc/leofs/leo_storage\nSep 20 17:41:42 stor01.lan leo_storage[9513]: Exec: /usr/local/leofs/current/leo_storage/erts-9.0/bin/erlexec -noinput -boot /usr/local/leofs/current/leo_storage/releases/1/leo_storage -mode minimal -config /etc/leofs/leo_storage/app.config -args_file /etc/leofs/leo_storage/vm.args -- console\n\n# journalctl -l -u leofs-storage-second | grep -e Exec -e Config\nSep 20 17:40:43 stor01.lan leo_storage[9065]: Config path: /etc/leofs/leo_storage_second\nSep 20 17:40:43 stor01.lan leo_storage[9065]: Exec: /usr/local/leofs/current/leo_storage/erts-9.0/bin/erlexec -noinput -boot /usr/local/leofs/current/leo_storage/releases/1/leo_storage -mode minimal -config /etc/leofs/leo_storage_second/app.config -args_file /etc/leofs/leo_storage_second/vm.args -- console    Note: Multiple LeoManager instances  It is possible to run multiple instances of LeoManager as well, but the resulting configuration and  leofs-adm  operations would be more complicated, so it's not recommended (as results of error might be severe). It would be better to use separate VMs or containers for different LeoManagers.    Note: Interacting with other instances through launch script  It is possible to interact with extra node instances through original launch script, however it requires a bit roundabout way of exporting environment variable as the owner of process. Example:  sudo -H -u leofs sh -c  export NODE_EXTRA_NAME=second; /usr/local/leofs/current/leo_storage/bin/leo_storage remote_console", 
            "title": "Starting other node instances"
        }, 
        {
            "location": "/admin/system_admin/multiple_nodes/#example-configuration", 
            "text": "Here is fully working configuration example for two storage nodes belonging to two different clusters and working on the same system. Second instance is called \"second\" here (unit file for it is located at  /etc/systemd/system/leofs-storage-second.service  and it contains  Environment=NODE_EXTRA_NAME=second  setting).  Directories and permissions:  1\n2\n3 # ls -ld /etc/leofs/leo_storage /etc/leofs/leo_storage_second/\ndrwxr-sr-x. 3 leofs leofs 4096 Sep 22 20:47 /etc/leofs/leo_storage\ndrwxr-sr-x. 3 leofs leofs 4096 Sep 20 17:44 /etc/leofs/leo_storage_second/   Main configuration for second node is symlinked to first - preferred when configuring nodes through configuration override files. This allows automatically getting new default settings with package upgrades (which include new version of  /etc/leofs/leo_storage/leo_storage.conf ):  1\n2 # ls -l /etc/leofs/leo_storage_second/leo_storage.conf\nlrwxrwxrwx. 1 root leofs 31 Sep 19 21:09 /etc/leofs/leo_storage_second/leo_storage.conf -  ../leo_storage/leo_storage.conf   Contents of  /etc/leofs/leo_storage/leo_storage.d/20-leo_storage.conf  - configuration for the first node:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 nodename   =   stor01 @192.168.0.20  managers   =   [ manager_0 @192.168.0.10 ,   manager_1 @192.168.0.11 ]  obj_containers . path   =   [ / mnt / avs / main ]  obj_containers . num_of_containers   =   [ 64 ]  sasl . sasl_error_log   =   / var / log / leofs / leo_storage / sasl / sasl - error . log  sasl . error_logger_mf_dir   =   / var / log / leofs / leo_storage / sasl  log . erlang   =   / var / log / leofs / leo_storage / erlang  log . app   =   / var / log / leofs / leo_storage / app  log . member_dir   =   / var / log / leofs / leo_storage / ring  log . ring_dir   =   / var / log / leofs / leo_storage / ring  queue_dir    =   / var / leofs / leo_storage / work / queue  leo_ordning_reda . temp_stacked_dir   =   / var / leofs / leo_storage / work / ord_reda /  erlang . crash_dump   =   / var / log / leofs / leo_storage / erl_crash . dump  snmp_conf   =   . / snmp / snmpa_storage_0 / leo_storage_snmp    Contents of  /etc/leofs/leo_storage_second/leo_storage.d/20-leo_storage.conf  - configuration for second node   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25 nodename   =   stor_second01 @192.168.0.20  managers   =   [ manager_0 @192.168.0.15 ,   manager_1 @192.168.0.16 ]  rpc . server . listen_port   =   13078  obj_containers . path   =   [ / mnt / avs / second ]  obj_containers . num_of_containers   =   [ 32 ]  object_storage . is_strict_check   =   true  watchdog . error . is_enabled   =   true  autonomic_op . compaction . is_enabled   =   true  sasl . sasl_error_log   =   / var / log / leofs / leo_storage_second / sasl / sasl - error . log  sasl . error_logger_mf_dir   =   / var / log / leofs / leo_storage_second / sasl  log . erlang   =   / var / log / leofs / leo_storage_second / erlang  log . app   =   / var / log / leofs / leo_storage_second / app  log . member_dir   =   / var / log / leofs / leo_storage_second / ring  log . ring_dir   =   / var / log / leofs / leo_storage_second / ring  queue_dir    =   / var / leofs / leo_storage_second / work / queue  leo_ordning_reda . temp_stacked_dir   =   / var / leofs / leo_storage_second / work / ord_reda /  erlang . crash_dump   =   / var / log / leofs / leo_storage_second / erl_crash . dump  snmp_conf   =   . / snmp / snmpa_storage_1 / leo_storage_snmp    Each node requires own set of directories with appropriate permissions:  1\n2\n3\n4 # mkdir -p /var/leofs/leo_storage/work/queue /var/log/leofs/leo_storage/app /var/leofs/leo_storage/work/ord_reda /var/log/leofs/leo_storage/sasl\n# chown -R leofs:leofs /var/leofs/leo_storage/ /var/log/leofs/\n# mkdir -p /var/leofs/leo_storage_second/work/queue /var/log/leofs/leo_storage_second/app /var/leofs/leo_storage_second/work/ord_reda /var/log/leofs/leo_storage_second/sasl\n# chown -R leofs:leofs /var/leofs/leo_storage_second/", 
            "title": "Example configuration"
        }, 
        {
            "location": "/admin/system_admin/multiple_nodes/#related-links", 
            "text": "For Administrators / System Operations / Systemd Services  For Administrators / System Administration / Persistent Configuration Location", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/index_of_commands/", 
            "text": "Index of leofs-adm Command Lines\n\n\nleofs-adm\n easily makes administrative operations of LeoFS, the commands of which include as below:\n\n\n\n\nGeneral Commands\n\n\nLeoStorage Cluster Operation\n\n\nLeoStorage MQ Operation\n\n\nRecover Commands\n\n\nData Compaction Commands\n\n\nDisk Usage Commands\n\n\nLeoGateway Operation\n\n\nLeoManager Maintenance Commands\n\n\nS3-API Related Commands\n\n\nMulti Data Center Operation\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGeneral Commands:\n\n\n\n\n\n\n\n\nstatus \n[\nnode\n]\n\n\nRetrieve status of every node \n(default)\n Retrieve status of a specified node\n\n\n\n\n\n\nwhereis \nfile-path\n\n\nRetrieve an assigned object by a file path\n\n\n\n\n\n\nStorage Operation:\n\n\n\n\n\n\n\n\ndetach \nstorage-node\n\n\nRemove a storage node in a LeoFS' storage cluster\nCurrent status: \nrunning\n OR \nstop\n\n\n\n\n\n\nsuspend \nstorage-node\n\n\nSuspend a storage node for maintenance\nThis command does NOT detach a node from a LeoFS' storage cluster\nWhile suspending, it rejects any requests\nCurrent status: \nrunning\n\n\n\n\n\n\nresume \nstorage-node\n\n\nResume a storage node for finished maintenance\nCurrent status: \nsuspended\n OR \nrestarted\n\n\n\n\n\n\nstart\n\n\nStart LeoFS after distributing a RING from LeoFS Manager to LeoFS Storage and LeoFS Gateway\n\n\n\n\n\n\nrebalance\n\n\nCommit detached and attached nodes to join a cluster\nRebalance objects in a cluster based on updated cluster topology\n\n\n\n\n\n\nmq-stats \nstorage-node\n\n\nSee statuses of message queues used in a LeoStorage node\n\n\n\n\n\n\nmq-suspend \nstorage-node\n \nmq-id\n\n\nSuspend a process consuming a message queue\nActive message queues only can be suspended\nWhile suspending, no messages are consumed\n\n\n\n\n\n\nmq-resume \nstorage-node\n \nmq-id\n\n\nResume a process consuming a message queue\n\n\n\n\n\n\nRecover Commands:\n\n\n\n\n\n\n\n\nrecover-file \nfile-path\n\n\nRecover an inconsistent object specified by the file-path of the local cluster\n\n\n\n\n\n\nrecover-disk \nstorage-node\n \n\n\nRecover all inconsistent objects on the specified disk in the specified node of the local cluster\n\n\n\n\n\n\nrecover-node \nstorage-node\n\n\nRecover all inconsistent objects in the specified node of the local cluster\n\n\n\n\n\n\nrecover-ring \nstorage-node\n\n\nRecover \nRING, a routing table\n of the specified node of the local cluster\n\n\n\n\n\n\nrecover-cluster \ncluster-id\n\n\nRecover **all inconsistent objects in the specified remote cluste**r \n(NOT the local cluster)\n in case of using \nthe multi datacenter replication\n\n\n\n\n\n\nCompaction Commands:\n\n\n\n\n\n\n\n\ncompact-start \nnode\n \nnum-of-targets\n [\nnumber-of-compaction-procs\n]\n\n\nRemove unnecessary objects from a specified node\n \nnum-of-targets\n: It controls a number of containers in parallel \n \nnum-of-compaction-procs\n: It controls a number of procs to execute the data compaction in parallel\n\n\n\n\n\n\ncompact-suspend \nstorage-node\n\n\nSuspend a data compaction processing\n\n\n\n\n\n\ncompact-resume \nstorage-node\n\n\nResume a data compaction processing\n\n\n\n\n\n\ncompact-status \nstorage-node\n\n\nSee current compaction status\n Compaction\u2019s status: \nidle\n, \nrunning\n, \nsuspend\n\n\n\n\n\n\ndiagnose-start \nnode\n\n\nDiagnose data of a specified storage node\n\n\n\n\n\n\nDisk Usage Commands:\n\n\n\n\n\n\n\n\ndu \nstorage-node\n\n\nSee current disk usages\n\n\n\n\n\n\ndu detail \nstorage-node\n\n\nSee current disk usages in detail\n\n\n\n\n\n\nGateway Operation:\n\n\n\n\n\n\n\n\npurge-cache \nfile-path\n\n\nRemove a cache from each LeoFS gateway\n\n\n\n\n\n\nremove-gateway \ngateway-node\n\n\nRemove a specified LeoGateway node, which is already stopped\n\n\n\n\n\n\nManager Maintenance:\n\n\n\n\n\n\n\n\nbackup-mnesia \nbackup-filepath\n\n\nCopy LeoFS\u2019s Manager data to a filepath\n\n\n\n\n\n\nrestore-mnesia \nbackup-filepath\n\n\nRestore LeoFS\u2019s Manager data from a backup file\n\n\n\n\n\n\nupdate-managers \nmanager-master\n \nmanager-slave\n\n\nUpdate LeoFS Manager nodes\n Destribute a new LeoManager nodes to LeoStorage and LeoGateway\n\n\n\n\n\n\ndump-ring \nnode\n\n\nDump RING, a routing table to a local disk\n\n\n\n\n\n\nupdate-log-level \ngateway/storage-node\n \nlog-level\n\n\nUpdate log level of a specified node\n log-level: debug, info, warn, error\n\n\n\n\n\n\nupdate-consistency-level \nwrite-quorum\n \nread-quorum\n \ndelete-quorum\n\n\nUpdate current consistency level of \nR-quorum\n, \nW-quorum\n and \nD-quorum\n\n\n\n\n\n\nWatchdog Operation:\n\n\n\n\n\n\n\n\nupdate-property \nnode\n \nproperty-name\n \nproperty-value\n\n\nUpdate watchdog properties of a specifid node, which includes as below:\n- watchdog.cpu_enabled \nboolean\n- watchdog.cpu_raised_error_times \ninteger\n- watchdog.cpu_interval \ninteger\n- watchdog.cpu_threshold_load_avg \nfloat\n- watchdog.cpu_threshold_util \ninteger\n- watchdog.disk_enabled \nboolean\n- watchdog.disk_raised_error_times \ninteger\n- watchdog.disk_interval \ninteger\n- watchdog.disk_threshold_use \ninteger\n- watchdog.disk_threshold_util \ninteger\n- watchdog.disk_threshold_rkb \ninteger\n- watchdog.disk_threshold_wkb \ninteger\n- watchdog.cluster_enabled \nboolean\n- watchdog.cluster_interval \ninteger\n\n\n\n\n\n\nS3-related Commands:\n\n\n\n\n\n\n\n\ncreate-user \nuser-id\n \n[\npassword\n]\n\n\nRegister a new user\n Generate an S3 key pair, \nAccess Key ID\n and \nSecret Access Key\n\n\n\n\n\n\nimport-user [-f] \nuser-id\n \naccess-key-id\n \nsecret-access-key\n\n\nImport a new user with S3 key pair, \nAccess Key ID\n and \nSecret Access Key\n -f option can be used when you face \nLeoFS' Issue #964, import-user with the access-key-id belonging to a deleted user doesn't work\n\n\n\n\n\n\ndelete-user \nuser-id\n\n\nRemove a user\n\n\n\n\n\n\nget-users\n\n\nRetrieve a list of users\n\n\n\n\n\n\nupdate-user-role \nuser-id\n \nrole-id\n\n\nUpdate a user\u2019s role\n Currently, we are supporting two kinds of roles, [1: \nGeneral user\n], [9: \nAdministrator\n]\n\n\n\n\n\n\nadd-endpoint \nendpoint\n\n\nRegister a new S3 Endpoint\n LeoFS\u2019 domains are ruled by \nNaming rule of AWS S3 Bucket\n\n\n\n\n\n\ndelete-endpoint \nendpoint\n\n\nRemove an endpoint\n\n\n\n\n\n\nget-endpoints\n\n\nRetrieve a list of endpoints\n\n\n\n\n\n\nadd-bucket \nbucket\n \naccess-key-id\n\n\nCreate a new bucket\n\n\n\n\n\n\ndelete-bucket \nbucket\n \naccess-key-id\n\n\nRemove a bucket and all files stored in the bucket\n\n\n\n\n\n\nget-bucket \naccess-key-id\n\n\nRetrieve a list of buckets owned by a specified user\n\n\n\n\n\n\nget-buckets\n\n\nRetrieve a list of all buckets registered\n\n\n\n\n\n\nchown-bucket \nbucket\n \naccess-key-id\n\n\nChange an owner of a bucket\n\n\n\n\n\n\nupdate-acl \nbucket\n \naccess-key-id\n \nacl\n\n\nUpdate ACL, Access Control List for a bucket\n- \nprivate (default)\n: No one except an owner has access rights\n- \npublic-read\n: All users have READ access\n- \npublic-read-write\n: All users have READ and WRITE access\n\n\n\n\n\n\ngen-nfs-mnt-key \nbucket\n \naccess-key-id\n \nclient-ip-address\n\n\nGenerate a key for NFS mount\n\n\n\n\n\n\nMulti Data Center Operation:\n\n\n\n\n\n\n\n\njoin-cluster \nremote-manager-master\n \nremote-manager-slave\n\n\nBegin to communicate between a local cluster and a remote cluster\n\n\n\n\n\n\nremove-cluster \nremote-manager-master\n \nremote-manager-slave\n\n\nTerminate to communicate between a local cluster and a remote cluster\n\n\n\n\n\n\ncluster-status\n\n\nSee a current state of cluster(s)", 
            "title": "Index of LeoFS' Commands"
        }, 
        {
            "location": "/admin/index_of_commands/#index-of-leofs-adm-command-lines", 
            "text": "leofs-adm  easily makes administrative operations of LeoFS, the commands of which include as below:   General Commands  LeoStorage Cluster Operation  LeoStorage MQ Operation  Recover Commands  Data Compaction Commands  Disk Usage Commands  LeoGateway Operation  LeoManager Maintenance Commands  S3-API Related Commands  Multi Data Center Operation      Command  Description      General Commands:     status  [ node ]  Retrieve status of every node  (default)  Retrieve status of a specified node    whereis  file-path  Retrieve an assigned object by a file path    Storage Operation:     detach  storage-node  Remove a storage node in a LeoFS' storage cluster Current status:  running  OR  stop    suspend  storage-node  Suspend a storage node for maintenance This command does NOT detach a node from a LeoFS' storage cluster While suspending, it rejects any requests Current status:  running    resume  storage-node  Resume a storage node for finished maintenance Current status:  suspended  OR  restarted    start  Start LeoFS after distributing a RING from LeoFS Manager to LeoFS Storage and LeoFS Gateway    rebalance  Commit detached and attached nodes to join a cluster Rebalance objects in a cluster based on updated cluster topology    mq-stats  storage-node  See statuses of message queues used in a LeoStorage node    mq-suspend  storage-node   mq-id  Suspend a process consuming a message queue Active message queues only can be suspended While suspending, no messages are consumed    mq-resume  storage-node   mq-id  Resume a process consuming a message queue    Recover Commands:     recover-file  file-path  Recover an inconsistent object specified by the file-path of the local cluster    recover-disk  storage-node    Recover all inconsistent objects on the specified disk in the specified node of the local cluster    recover-node  storage-node  Recover all inconsistent objects in the specified node of the local cluster    recover-ring  storage-node  Recover  RING, a routing table  of the specified node of the local cluster    recover-cluster  cluster-id  Recover **all inconsistent objects in the specified remote cluste**r  (NOT the local cluster)  in case of using  the multi datacenter replication    Compaction Commands:     compact-start  node   num-of-targets  [ number-of-compaction-procs ]  Remove unnecessary objects from a specified node   num-of-targets : It controls a number of containers in parallel    num-of-compaction-procs : It controls a number of procs to execute the data compaction in parallel    compact-suspend  storage-node  Suspend a data compaction processing    compact-resume  storage-node  Resume a data compaction processing    compact-status  storage-node  See current compaction status  Compaction\u2019s status:  idle ,  running ,  suspend    diagnose-start  node  Diagnose data of a specified storage node    Disk Usage Commands:     du  storage-node  See current disk usages    du detail  storage-node  See current disk usages in detail    Gateway Operation:     purge-cache  file-path  Remove a cache from each LeoFS gateway    remove-gateway  gateway-node  Remove a specified LeoGateway node, which is already stopped    Manager Maintenance:     backup-mnesia  backup-filepath  Copy LeoFS\u2019s Manager data to a filepath    restore-mnesia  backup-filepath  Restore LeoFS\u2019s Manager data from a backup file    update-managers  manager-master   manager-slave  Update LeoFS Manager nodes  Destribute a new LeoManager nodes to LeoStorage and LeoGateway    dump-ring  node  Dump RING, a routing table to a local disk    update-log-level  gateway/storage-node   log-level  Update log level of a specified node  log-level: debug, info, warn, error    update-consistency-level  write-quorum   read-quorum   delete-quorum  Update current consistency level of  R-quorum ,  W-quorum  and  D-quorum    Watchdog Operation:     update-property  node   property-name   property-value  Update watchdog properties of a specifid node, which includes as below: - watchdog.cpu_enabled  boolean - watchdog.cpu_raised_error_times  integer - watchdog.cpu_interval  integer - watchdog.cpu_threshold_load_avg  float - watchdog.cpu_threshold_util  integer - watchdog.disk_enabled  boolean - watchdog.disk_raised_error_times  integer - watchdog.disk_interval  integer - watchdog.disk_threshold_use  integer - watchdog.disk_threshold_util  integer - watchdog.disk_threshold_rkb  integer - watchdog.disk_threshold_wkb  integer - watchdog.cluster_enabled  boolean - watchdog.cluster_interval  integer    S3-related Commands:     create-user  user-id   [ password ]  Register a new user  Generate an S3 key pair,  Access Key ID  and  Secret Access Key    import-user [-f]  user-id   access-key-id   secret-access-key  Import a new user with S3 key pair,  Access Key ID  and  Secret Access Key  -f option can be used when you face  LeoFS' Issue #964, import-user with the access-key-id belonging to a deleted user doesn't work    delete-user  user-id  Remove a user    get-users  Retrieve a list of users    update-user-role  user-id   role-id  Update a user\u2019s role  Currently, we are supporting two kinds of roles, [1:  General user ], [9:  Administrator ]    add-endpoint  endpoint  Register a new S3 Endpoint  LeoFS\u2019 domains are ruled by  Naming rule of AWS S3 Bucket    delete-endpoint  endpoint  Remove an endpoint    get-endpoints  Retrieve a list of endpoints    add-bucket  bucket   access-key-id  Create a new bucket    delete-bucket  bucket   access-key-id  Remove a bucket and all files stored in the bucket    get-bucket  access-key-id  Retrieve a list of buckets owned by a specified user    get-buckets  Retrieve a list of all buckets registered    chown-bucket  bucket   access-key-id  Change an owner of a bucket    update-acl  bucket   access-key-id   acl  Update ACL, Access Control List for a bucket -  private (default) : No one except an owner has access rights -  public-read : All users have READ access -  public-read-write : All users have READ and WRITE access    gen-nfs-mnt-key  bucket   access-key-id   client-ip-address  Generate a key for NFS mount    Multi Data Center Operation:     join-cluster  remote-manager-master   remote-manager-slave  Begin to communicate between a local cluster and a remote cluster    remove-cluster  remote-manager-master   remote-manager-slave  Terminate to communicate between a local cluster and a remote cluster    cluster-status  See a current state of cluster(s)", 
            "title": "Index of leofs-adm Command Lines"
        }, 
        {
            "location": "/benchmark/README/", 
            "text": "Benchmark\n\n\nSetting up basho_bench\n\n\nInstallation\n\n\n\n\nBasho basho_bench\u2019s repository\n\n\nBasho basho_bench\u2019s documentation\n\n\nUse the following commands to set up basho_bench.\n\n\n\n\n1\n2\n3\n4\n5\n6\n$ git clone git://github.com/basho/basho_bench.git\n$ git clone https://github.com/leo-project/leofs.git\n$ \ncd\n basho_bench\n$ cp -i ../leofs/test/src/*.erl src/\n$ cp -i ../leofs/test/include/*.hrl include/\n$ make all\n\n\n\n\n\n\nPreparations before testing\n\n\nCreate a test bucket\n\n\nAfter starting a LeoFS system, you need to create a bucket before getting started with benchmarks. In this example, the bucket name is \ntest\n. It is owned by the user \n_test_leofs\n that is already registered internally by LeoFS.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n$ leofs-adm add-endpoint \ngateway-ip-address\n\nOK\n\n$ leofs-adm add-bucket \ntest\n \n05236\n\nOK\n\n$ leofs-adm get-buckets\nbucket   \n|\n owner       \n|\n created at\n---------+-------------+---------------------------\n\ntest\n     \n|\n _test_leofs \n|\n \n2013\n-02-27 \n14\n:06:54 +0900\n\n$ leofs-adm update-acl \ntest\n \n05236\n public-read\nOK\n\n\n\n\n\n\nConfiguration file for basho_bench\n\n\nAn Example\n\n\nSome examples are included in LeoFS' repository at \nleo-project / leofs / test / conf\n. If you would like to learn basho_bench's configuration, you can see \nBashoBench's Configuration\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n{\nmode\n,\n      \nmax\n}.\n\n\n{\nduration\n,\n   \n10\n}.\n\n\n{\nconcurrent\n,\n \n50\n}.\n\n\n\n{\ndriver\n,\n \nbasho_bench_driver_leofs\n}.\n\n\n{\ncode_paths\n,\n \n[\ndeps/ibrowse\n]}.\n\n\n\n{\nhttp_raw_ips\n,\n \n[\n${HOST_NAME_OF_LEOFS_GATEWAY}\n]}.\n\n\n{\nhttp_raw_port\n,\n \n8080\n}.\n\n\n{\nhttp_raw_path\n,\n \n/test\n}.\n\n\n%% {http_raw_path, \n/${BUCKET}\n}.\n\n\n\n{\nkey_generator\n,\n   \n{\npartitioned_sequential_int\n,\n \n1000000\n}}.\n\n\n{\nvalue_generator\n,\n \n{\nfixed_bin\n,\n \n16384\n}}.\n \n%% 16KB\n\n\n{\noperations\n,\n \n[{\nput\n,\n1\n}]}.\n               \n%% PUT:100%\n\n\n%%{operations, [{put,1}, {get, 4}]}.   %% PUT:20%, GET:80%\n\n\n\n{\ncheck_integrity\n,\n \nfalse\n}.\n\n\n\n\n\n\n\nDescription\n\n\n\n\n\n\n\n\nKey\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nhttp_raw_ips\n\n\nThe LeoFS Gateway nodes we want to benchmark\n\n\n\n\n\n\nhttp_raw_port\n\n\nThe port used by the LeoFS Gateway nodes\n\n\n\n\n\n\nhttp_raw_path\n\n\nURL path prefix. The first path segment MUST be a BUCKET name\n\n\n\n\n\n\ncheck_integrity (default:false)\n\n\nCheck integrity of registered object - compare an original MD5 with a retrieved object\u2019s MD5 (Only for developers)\n\n\n\n\n\n\n\n\nRunning basho_bench (1)\n\n\nIn this example, LeoFS and \nbasho_bench\n are installed locally. The following commands can be used to run \nbasho_bench\n.\n\n\n1\n2\n3\n### Loading 1M records of size 16KB\n\n\ncd\n basho_bench\n./basho_bench ../leofs/test/conf/leofs_16K_LOAD1M.config\n\n\n\n\n\n\nRunning basho_bench (2)\n\n\nIn this example, LeoFS and \nbasho_bench\n are installed on different hosts.\n\n\nConfigure the endpoint on LeoManager's console\n\n\nAllows basho_bench\u2019s requests to reach \n${\nHOST_NAME_OF_LEOFS_GATEWAY\n}\n.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n$ leofs-adm add-endpoint \nhost-name-of-leofs-gateway\n\nOK\n\n$ leofs-adm get-endpoints\nendpoint                      \n|\n created at\n------------------------------+---------------------------\nlocalhost                     \n|\n \n2013\n-03-01 \n00\n:14:04 +0000\ns3.amazonaws.com              \n|\n \n2013\n-03-01 \n00\n:14:04 +0000\n\n${\nHOST_NAME_OF_LEOFS_GATEWAY\n}\n \n|\n \n2013\n-03-01 \n00\n:14:04 +0000\n\n\n\n\n\n\nEdit the benchmark\u2019s configuration file\n\n\nYou need to modify the values for \nhttp_raw_ips\n and \nhttp_raw_port\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n{\nmode,      max\n}\n.\n\n{\nduration,   \n10\n}\n.\n\n{\nconcurrent, \n50\n}\n.\n\n\n{\ndriver, basho_bench_driver_leofs\n}\n.\n\n{\ncode_paths, \n[\ndeps/ibrowse\n]}\n.\n\n\n{\nhttp_raw_ips, \n[\n${\nHOST_NAME_OF_LEOFS_GATEWAY\n}\n]}\n. %% able to \nset\n plural nodes\n\n{\nhttp_raw_port, \n${\nPORT\n}\n}\n. %% default: \n8080\n\n\n{\nhttp_raw_path, \n/test\n}\n.\n%% \n{\nhttp_raw_path, \n/\n${\nBUCKET\n}\n}\n.\n\n\n{\nkey_generator,   \n{\npartitioned_sequential_int, \n1000000\n}}\n.\n\n{\nvalue_generator, \n{\nfixed_bin, \n16384\n}}\n. %% 16KB\n\n{\noperations, \n[{\nput,1\n}]}\n.               %% PUT:100%\n%%\n{\noperations, \n[{\nput,1\n}\n, \n{\nget, \n4\n}]}\n.   %% PUT:20%, GET:80%\n\n\n{\ncheck_integrity, false\n}\n.\n\n\n\n\n\n\nRunning basho_bench\n\n\n1\n2\n3\n### Loading 1M records each size is 16KB\n\n\ncd\n basho_bench\n./basho_bench ../leofs/test/conf/leofs_16K_LOAD1M.config\n\n\n\n\n\n\nRelated Links\n\n\n\n\nGetting Started / Quick Installation and Setup\n\n\nGetting Started / Building a LeoFS' cluster with Ansible\n\n\nFor Administrators / System Operations / S3-API related Operations", 
            "title": "Benchmark"
        }, 
        {
            "location": "/benchmark/README/#benchmark", 
            "text": "", 
            "title": "Benchmark"
        }, 
        {
            "location": "/benchmark/README/#setting-up-basho_bench", 
            "text": "", 
            "title": "Setting up basho_bench"
        }, 
        {
            "location": "/benchmark/README/#installation", 
            "text": "Basho basho_bench\u2019s repository  Basho basho_bench\u2019s documentation  Use the following commands to set up basho_bench.   1\n2\n3\n4\n5\n6 $ git clone git://github.com/basho/basho_bench.git\n$ git clone https://github.com/leo-project/leofs.git\n$  cd  basho_bench\n$ cp -i ../leofs/test/src/*.erl src/\n$ cp -i ../leofs/test/include/*.hrl include/\n$ make all", 
            "title": "Installation"
        }, 
        {
            "location": "/benchmark/README/#preparations-before-testing", 
            "text": "", 
            "title": "Preparations before testing"
        }, 
        {
            "location": "/benchmark/README/#create-a-test-bucket", 
            "text": "After starting a LeoFS system, you need to create a bucket before getting started with benchmarks. In this example, the bucket name is  test . It is owned by the user  _test_leofs  that is already registered internally by LeoFS.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 $ leofs-adm add-endpoint  gateway-ip-address \nOK\n\n$ leofs-adm add-bucket  test   05236 \nOK\n\n$ leofs-adm get-buckets\nbucket    |  owner        |  created at\n---------+-------------+--------------------------- test       |  _test_leofs  |   2013 -02-27  14 :06:54 +0900\n\n$ leofs-adm update-acl  test   05236  public-read\nOK", 
            "title": "Create a test bucket"
        }, 
        {
            "location": "/benchmark/README/#configuration-file-for-basho_bench", 
            "text": "", 
            "title": "Configuration file for basho_bench"
        }, 
        {
            "location": "/benchmark/README/#an-example", 
            "text": "Some examples are included in LeoFS' repository at  leo-project / leofs / test / conf . If you would like to learn basho_bench's configuration, you can see  BashoBench's Configuration .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 { mode ,        max }.  { duration ,     10 }.  { concurrent ,   50 }.  { driver ,   basho_bench_driver_leofs }.  { code_paths ,   [ deps/ibrowse ]}.  { http_raw_ips ,   [ ${HOST_NAME_OF_LEOFS_GATEWAY} ]}.  { http_raw_port ,   8080 }.  { http_raw_path ,   /test }.  %% {http_raw_path,  /${BUCKET} }.  { key_generator ,     { partitioned_sequential_int ,   1000000 }}.  { value_generator ,   { fixed_bin ,   16384 }}.   %% 16KB  { operations ,   [{ put , 1 }]}.                 %% PUT:100%  %%{operations, [{put,1}, {get, 4}]}.   %% PUT:20%, GET:80%  { check_integrity ,   false }.", 
            "title": "An Example"
        }, 
        {
            "location": "/benchmark/README/#description", 
            "text": "Key  Value      http_raw_ips  The LeoFS Gateway nodes we want to benchmark    http_raw_port  The port used by the LeoFS Gateway nodes    http_raw_path  URL path prefix. The first path segment MUST be a BUCKET name    check_integrity (default:false)  Check integrity of registered object - compare an original MD5 with a retrieved object\u2019s MD5 (Only for developers)", 
            "title": "Description"
        }, 
        {
            "location": "/benchmark/README/#running-basho_bench-1", 
            "text": "In this example, LeoFS and  basho_bench  are installed locally. The following commands can be used to run  basho_bench .  1\n2\n3 ### Loading 1M records of size 16KB  cd  basho_bench\n./basho_bench ../leofs/test/conf/leofs_16K_LOAD1M.config", 
            "title": "Running basho_bench (1)"
        }, 
        {
            "location": "/benchmark/README/#running-basho_bench-2", 
            "text": "In this example, LeoFS and  basho_bench  are installed on different hosts.", 
            "title": "Running basho_bench (2)"
        }, 
        {
            "location": "/benchmark/README/#configure-the-endpoint-on-leomanagers-console", 
            "text": "Allows basho_bench\u2019s requests to reach  ${ HOST_NAME_OF_LEOFS_GATEWAY } .  1\n2\n3\n4\n5\n6\n7\n8\n9 $ leofs-adm add-endpoint  host-name-of-leofs-gateway \nOK\n\n$ leofs-adm get-endpoints\nendpoint                       |  created at\n------------------------------+---------------------------\nlocalhost                      |   2013 -03-01  00 :14:04 +0000\ns3.amazonaws.com               |   2013 -03-01  00 :14:04 +0000 ${ HOST_NAME_OF_LEOFS_GATEWAY }   |   2013 -03-01  00 :14:04 +0000", 
            "title": "Configure the endpoint on LeoManager's console"
        }, 
        {
            "location": "/benchmark/README/#edit-the-benchmarks-configuration-file", 
            "text": "You need to modify the values for  http_raw_ips  and  http_raw_port .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 { mode,      max } . { duration,    10 } . { concurrent,  50 } . { driver, basho_bench_driver_leofs } . { code_paths,  [ deps/ibrowse ]} . { http_raw_ips,  [ ${ HOST_NAME_OF_LEOFS_GATEWAY } ]} . %% able to  set  plural nodes { http_raw_port,  ${ PORT } } . %% default:  8080  { http_raw_path,  /test } .\n%%  { http_raw_path,  / ${ BUCKET } } . { key_generator,    { partitioned_sequential_int,  1000000 }} . { value_generator,  { fixed_bin,  16384 }} . %% 16KB { operations,  [{ put,1 }]} .               %% PUT:100%\n%% { operations,  [{ put,1 } ,  { get,  4 }]} .   %% PUT:20%, GET:80% { check_integrity, false } .", 
            "title": "Edit the benchmark\u2019s configuration file"
        }, 
        {
            "location": "/benchmark/README/#running-basho_bench", 
            "text": "1\n2\n3 ### Loading 1M records each size is 16KB  cd  basho_bench\n./basho_bench ../leofs/test/conf/leofs_16K_LOAD1M.config", 
            "title": "Running basho_bench"
        }, 
        {
            "location": "/benchmark/README/#related-links", 
            "text": "Getting Started / Quick Installation and Setup  Getting Started / Building a LeoFS' cluster with Ansible  For Administrators / System Operations / S3-API related Operations", 
            "title": "Related Links"
        }, 
        {
            "location": "/faq/fundamentals/", 
            "text": "FAQ: LeoFS Fundamentals\n\n\n\n\n\nWhat kind of storage is leofs?\n\n\nLeoFS is a highly scalable, fault-tolerant \nobject_storage\n for the Web. Significantly, LeoFS supports huge amount and various kind unstructured data such as photo, movie, document, log data and so on.\n\n\nOperationally, LeoFS features multi-master replication with automated failover and built-in horizontal scaling via ConsistentHashing.\n\n\n\n\nRelated Links:\n\n\nLeoFS Introduction\n\n\n\n\n\n\n\n\nWhat are typical uses for LeoFS?\n\n\nIf you are searching a storage system that is able to store huge amount and various kind of files such as photo, movie, log data and so on, LeoFS is suitable for that.\n\n\nThis is because LeoFS is a highly available, distributed storage system. Also, LeoFS can be used to store a lot of data efficiently, safely, and inexpensively.\n\n\n\n\nRelated Links:\n\n\nLeoFS Introduction\n\n\n\n\n\n\n\n\nWhat is benefit for LeoFS users?\n\n\nAdvantage\n\n\nLeoFS is supporting the following features:\n\n\n\n\nS3-API Support\n\n\nLeoFS is an Amazon S3 compatible storage system.\n\n\nSwitch to LeoFS to decrease your cost from more expensive public-cloud solutions.\n\n\n\n\n\n\nLarge Object Support\n\n\nLeoFS can handle files with more than GB\n\n\n\n\n\n\nMulti Data Center Replication\n\n\nLeoFS is a highly scalable, fault-tolerant distributed file system without |SPOF|.\n\n\nLeoFS's cluster can be viewed as ONE-HUGE storage. It consists of a set of loosely connected nodes.\n\n\nWe can build a global scale storage system with easy operations\n\n\n\n\n\n\nHigh Performance without |SPOF|\n\n\nAccording to the original cache mechanism and sophisticated innternal architecture, LeoFS keeps high performance regardless of amount and kind of data without |SPOF|.\n\n\n\n\n\n\n\n\nIn near future, LeoFS is going to provide the powerful features with LeoFS v1.4, v1.5 and v2.0\n\n\n\n\nHybrid Storage\n\n\n[v1.0]\n S3-API and REST-API Support\n\n\n[v1.2]\n NFS v3 Support\n\n\n[v1.5]\n LeoFS' Native Client\n\n\n\n\n\n\nReduction of Storage Costs\n\n\n[v1.5]\n Erasure Code\n\n\n[v2.2]\n Data Deduplication\n\n\n\n\n\n\n\n\nFor Business Managers\n\n\n\n\nStoring confidential/sensitive data internally\n\n\nSaving cost to use commodity servers\n\n\nIncreasing service level with speedy response\n\n\nExpanding business globally\n\n\n\n\nFor Administrators\n\n\n\n\nEasy to install with the packages\n\n\nEasy to operate with |LeoCenterDocs|\n\n\n\n\nWhat is architecture of LeoFS?\n\n\nWe've been mainly focusing on \nHigh Availability\n, \nHigh Scalability\n and \nHigh Cost Performance Ratio\n since unstructured data such as images, movies and logs have been exponentially increasing day by day, and we needed to build a cloud storage that can handle all them.\n\n\nLeoFS consists of 3 core components, \nLeoGateway\n, \nLeoStorage\n and \nLeoManager\n. The role of each component is clearly defined.\n\n\n\n\nLeoGateway\n handles http-requests and http-responses from clients when using REST-API OR S3-API. Also, it has the built-in object-cache system.\n\n\nLeoStorage\n handles \nGET\n, \nPUT\n and \nDELETE\n, Also it has replicator and recoverer in order to keep running and consistency.\n\n\nLeoManager\n always monitors Gateway(s) and Storage(s). Manger monitors node-status and RING(logical routing-table) checksum to keep running and consistency.\n\n\nAlso, what we payed attention when we desined LeoFS are the following 3 things:\n\n\n\n\nTo keep always running and No |SPOF|\n\n\nTo keep high-performance, regardless of the kind and amount of data\n\n\nTo provide easy administration, we already provide LeoFS CUI and GUI console.\n\n\n\n\nIs there the roadmap of LeoFS?\n\n\nWe've published LeoFS milestones on \nLeoFS' GitHub\n. We may revise the milestones occasionally because there is a possibility to add new features or change priority of implementation. We'll keep them always updated.\n\n\nWhat language is LeoFS written in?\n\n\nLeoFS depends on \nErlang\n,  and some LeoFS' libraries are implemented by C/C++.\n\n\n\n\nRelated Links:\n\n\nLeoFS Introduction\n\n\nLeoFS Repository on GitHub", 
            "title": "LeoFS Fundamentals"
        }, 
        {
            "location": "/faq/fundamentals/#faq-leofs-fundamentals", 
            "text": "", 
            "title": "FAQ: LeoFS Fundamentals"
        }, 
        {
            "location": "/faq/fundamentals/#what-kind-of-storage-is-leofs", 
            "text": "LeoFS is a highly scalable, fault-tolerant  object_storage  for the Web. Significantly, LeoFS supports huge amount and various kind unstructured data such as photo, movie, document, log data and so on.  Operationally, LeoFS features multi-master replication with automated failover and built-in horizontal scaling via ConsistentHashing.   Related Links:  LeoFS Introduction", 
            "title": "What kind of storage is leofs?"
        }, 
        {
            "location": "/faq/fundamentals/#what-are-typical-uses-for-leofs", 
            "text": "If you are searching a storage system that is able to store huge amount and various kind of files such as photo, movie, log data and so on, LeoFS is suitable for that.  This is because LeoFS is a highly available, distributed storage system. Also, LeoFS can be used to store a lot of data efficiently, safely, and inexpensively.   Related Links:  LeoFS Introduction", 
            "title": "What are typical uses for LeoFS?"
        }, 
        {
            "location": "/faq/fundamentals/#what-is-benefit-for-leofs-users", 
            "text": "", 
            "title": "What is benefit for LeoFS users?"
        }, 
        {
            "location": "/faq/fundamentals/#advantage", 
            "text": "LeoFS is supporting the following features:   S3-API Support  LeoFS is an Amazon S3 compatible storage system.  Switch to LeoFS to decrease your cost from more expensive public-cloud solutions.    Large Object Support  LeoFS can handle files with more than GB    Multi Data Center Replication  LeoFS is a highly scalable, fault-tolerant distributed file system without |SPOF|.  LeoFS's cluster can be viewed as ONE-HUGE storage. It consists of a set of loosely connected nodes.  We can build a global scale storage system with easy operations    High Performance without |SPOF|  According to the original cache mechanism and sophisticated innternal architecture, LeoFS keeps high performance regardless of amount and kind of data without |SPOF|.     In near future, LeoFS is going to provide the powerful features with LeoFS v1.4, v1.5 and v2.0   Hybrid Storage  [v1.0]  S3-API and REST-API Support  [v1.2]  NFS v3 Support  [v1.5]  LeoFS' Native Client    Reduction of Storage Costs  [v1.5]  Erasure Code  [v2.2]  Data Deduplication", 
            "title": "Advantage"
        }, 
        {
            "location": "/faq/fundamentals/#for-business-managers", 
            "text": "Storing confidential/sensitive data internally  Saving cost to use commodity servers  Increasing service level with speedy response  Expanding business globally", 
            "title": "For Business Managers"
        }, 
        {
            "location": "/faq/fundamentals/#for-administrators", 
            "text": "Easy to install with the packages  Easy to operate with |LeoCenterDocs|", 
            "title": "For Administrators"
        }, 
        {
            "location": "/faq/fundamentals/#what-is-architecture-of-leofs", 
            "text": "We've been mainly focusing on  High Availability ,  High Scalability  and  High Cost Performance Ratio  since unstructured data such as images, movies and logs have been exponentially increasing day by day, and we needed to build a cloud storage that can handle all them.  LeoFS consists of 3 core components,  LeoGateway ,  LeoStorage  and  LeoManager . The role of each component is clearly defined.   LeoGateway  handles http-requests and http-responses from clients when using REST-API OR S3-API. Also, it has the built-in object-cache system.  LeoStorage  handles  GET ,  PUT  and  DELETE , Also it has replicator and recoverer in order to keep running and consistency.  LeoManager  always monitors Gateway(s) and Storage(s). Manger monitors node-status and RING(logical routing-table) checksum to keep running and consistency.  Also, what we payed attention when we desined LeoFS are the following 3 things:   To keep always running and No |SPOF|  To keep high-performance, regardless of the kind and amount of data  To provide easy administration, we already provide LeoFS CUI and GUI console.", 
            "title": "What is architecture of LeoFS?"
        }, 
        {
            "location": "/faq/fundamentals/#is-there-the-roadmap-of-leofs", 
            "text": "We've published LeoFS milestones on  LeoFS' GitHub . We may revise the milestones occasionally because there is a possibility to add new features or change priority of implementation. We'll keep them always updated.", 
            "title": "Is there the roadmap of LeoFS?"
        }, 
        {
            "location": "/faq/fundamentals/#what-language-is-leofs-written-in", 
            "text": "LeoFS depends on  Erlang ,  and some LeoFS' libraries are implemented by C/C++.   Related Links:  LeoFS Introduction  LeoFS Repository on GitHub", 
            "title": "What language is LeoFS written in?"
        }, 
        {
            "location": "/faq/limits/", 
            "text": "FAQ: LeoFS Limits\n\n\n\n\n\nFeatures\n\n\n\n\nLeoFS have covered almost major \nAWS S3-API\n but not all APIs.\n\n\nIf you use \nS3's Multi Part Upload API\n, the size of a part of an object must be less than the size of a chunked object in LeoFS.\n\n\nWhen using the multi datacenter replication feature, we have supported up to 2 clusters with \nLeoFS v1.2\n, but we're going to support over 3 clusters replication with \nLeoFS v2.0\n.\n\n\n\n\nWhen you run recover-node while another recover-node is already working in-progress, objects that are not transmitted to the target node yet in the former recover-node are canceled and the latter recover-node will take place. To ensure that every recover-node completes its job, we'd recommend you to run recover-node one-by-one.\n\n\n\n\n\n\nRelated Links:\n\n\n\n\nAmazon S3 REST API Introduction\n\n\nLeoFS' Issue #177, Responds an incorrect MD5 of an large object\n\n\nLeoFS' Issue #338, Implements communication with three and over clusters\n\n\nLeoFS' Issue #880, Recover-node fails to recover all data on storage node\n\n\n\n\n\n\n\n\nOperations\n\n\n\n\n\n\nWhen you upgrade your LeoFS, you can NOT change the metadata storage as |KVS| - \nbitcask\n or \nleveldb\n can be used in LeoFS - used by LeoFS Storage. We recommend users to replace \nbitcask\n with \nleveldb\n by using |leofs_utils/tools/b2l|.\n\n\n\n\n\n\nRelated Links:\n\n\n\n\nConfiguration\n\n\n\n\n\n\n\n\nNFS Support\n\n\n\n\nNFS implemantation with \nLeoFS v1.1\n is a subset of \nNFS v3\n. \nAuthentication\n, and \nOwner/Permission\n management are NOT covered.\n\n\nThe \nls\n command may take too much time when the target directory have lots of child. We're planning to provide better performance with LeoFS v.2.0.\n\n\nIf you use LeoFS with NFS, you should set the size of a chunked object in LeoFS to 1MB (1048576Bytes) - \nlarge_object.chunked_obj_len = 1048576\n \n(leo_gateway.conf)\n, otherwise the efficiency of disk utilization can be decreased.\n\n\n\n\nWhat are the requirements to run LeoFS with NFS?\n\n\n\n\nLeoGateway (NFS Server):\n\n\nWe've supporeted the targets Debian 6, Ubuntu-Server 14.04 LTS or Higher and CentOS 6.5/7.0 as LeoFS does, but should work on most linux platforms. In addition, We've confirmed LeoFS with NFS works properly on the latest FreeBSD and SmartOS by using \nNFS Integration Test Tool\n.\n\n\n\n\n\n\n\n\n\n\n\n\nRelated Links:\n\n\n\n\nConfiguration", 
            "title": "LeoFS Limits"
        }, 
        {
            "location": "/faq/limits/#faq-leofs-limits", 
            "text": "", 
            "title": "FAQ: LeoFS Limits"
        }, 
        {
            "location": "/faq/limits/#features", 
            "text": "LeoFS have covered almost major  AWS S3-API  but not all APIs.  If you use  S3's Multi Part Upload API , the size of a part of an object must be less than the size of a chunked object in LeoFS.  When using the multi datacenter replication feature, we have supported up to 2 clusters with  LeoFS v1.2 , but we're going to support over 3 clusters replication with  LeoFS v2.0 .   When you run recover-node while another recover-node is already working in-progress, objects that are not transmitted to the target node yet in the former recover-node are canceled and the latter recover-node will take place. To ensure that every recover-node completes its job, we'd recommend you to run recover-node one-by-one.    Related Links:   Amazon S3 REST API Introduction  LeoFS' Issue #177, Responds an incorrect MD5 of an large object  LeoFS' Issue #338, Implements communication with three and over clusters  LeoFS' Issue #880, Recover-node fails to recover all data on storage node", 
            "title": "Features"
        }, 
        {
            "location": "/faq/limits/#operations", 
            "text": "When you upgrade your LeoFS, you can NOT change the metadata storage as |KVS| -  bitcask  or  leveldb  can be used in LeoFS - used by LeoFS Storage. We recommend users to replace  bitcask  with  leveldb  by using |leofs_utils/tools/b2l|.    Related Links:   Configuration", 
            "title": "Operations"
        }, 
        {
            "location": "/faq/limits/#nfs-support", 
            "text": "NFS implemantation with  LeoFS v1.1  is a subset of  NFS v3 .  Authentication , and  Owner/Permission  management are NOT covered.  The  ls  command may take too much time when the target directory have lots of child. We're planning to provide better performance with LeoFS v.2.0.  If you use LeoFS with NFS, you should set the size of a chunked object in LeoFS to 1MB (1048576Bytes) -  large_object.chunked_obj_len = 1048576   (leo_gateway.conf) , otherwise the efficiency of disk utilization can be decreased.   What are the requirements to run LeoFS with NFS?   LeoGateway (NFS Server):  We've supporeted the targets Debian 6, Ubuntu-Server 14.04 LTS or Higher and CentOS 6.5/7.0 as LeoFS does, but should work on most linux platforms. In addition, We've confirmed LeoFS with NFS works properly on the latest FreeBSD and SmartOS by using  NFS Integration Test Tool .       Related Links:   Configuration", 
            "title": "NFS Support"
        }, 
        {
            "location": "/faq/administration/", 
            "text": "FAQ: LeoFS Administration\n\n\n\n\n\nWhere can I get the packages for LeoFS?\n\n\nYou can get the packages by platform from \nLeoFS website\n, and LeoFS package for FreeBSD has been published at \nFreeBSD Fresh Ports\n.\n\n\nHow can I run my LeoFS cluster automatically?\n\n\nCurrently after restarting a storage node,  you need to issue the resume command with leofs-adm script manually like this.\n\n\n1\n$ leofs-adm resume storage_0@127.0.0.1\n\n\n\n\n\n\nWe're planing to provide LeoFS's automated operations like \nauto-compaction\n, \nauto-rebalance\n and others with LeoFS v1.2, v1.4 and v2.0. Actually, \nauto-compaction\n which was already supported with v1.2.\n\n\nHow do multiple users login into LeoFS Manager's console at the same time?\n\n\nAnswer 1:\n\n\nActually, there is no login status in LeoFS, but the number of listening tcp connections is limited by \nleo_manger.conf\n \ndefault\n:\n \nconsole\n.\nacceptors\n.\ncui\n \n=\n \n3\n.\nSo while using default settings, 3 connections can be connected to a manager in parallel.\n\n\nAnswer 2:\n\n\nSince \nLeoFS v1.1.0\n, LeoFS have the more powerful alternative option \nleofs-adm\n command.\n\n\nThis command have not only same functionalities with the existing telnet way but also do NOT keep an tcp connection established for long time (connect only when issueing a command). You don't need to worry about the number of tcp connections.\n\n\nThe result of the du command can be different with the actual disk-usage\n\n\nIn order to reduce system resource usage for calculating the result of the \ndu\n command, LeoFS keep that information on memory and when stopping itself, LeoFS save those data into a local file. Then when restarting, LeoFS load those on memory.\n\n\nSo if LeoFS is stopped unintentionally like killing by OOM killer, those data can become inconsistent with actual usage.\n\n\nIf you get into this situation, you can recover those data by issueing the \ncompact-start\n command to the node. after finishing the compaction, the result of the du command will be consitent with actual usage.\n\n\nWhen issueing the recover node command the LeoFS can get into high load\n\n\nSince the \nrecover-node\n command can lead to issue lots of disk I/O and consume network bandwidth, if you face to issue the recover-node to multiple nodes at once, LeoFS can get into high load and become unresponsive. So we recommend you execute the recover-node command to target nodes one by one.\n\n\nIf this solution could not work for you, you're able to control how much recover-node consume system resources by changing \nthe MQ-related parameters in leo_storage.conf\n.\n\n\nWhat should I do when Too many processes errors happen?\n\n\nLeoFS usually try to keep the number of Erlang processes as minimum as possible, but there are some exceptions when doing something asynchronously.\n\n\n\n\nReplicating an object to the non-primary assigned nodes\n\n\nRetrying to replicate an object when the previous attempt failed\n\n\n\n\nGiven that LeoFS suffered from very high load AND there are some nodes downed for some reason, The number of Erlang processes gradually have increased and might have reached the sysmte limit.\n\n\nWe recommend users to set an appropriate value which depends on your workload to the \n+P option\n. Also if the \n+P option\n does NOT work for you, there are some possibilities that some external system resources like disk, network equipments have broken, Please check out the dmesg/syslog on your sysmtem.\n\n\nHow do I set \"a number of containers\" at LeoFS Storage configuration?\n\n\nObjects/files are stored into LeoFS Storage containers which are log-structured files. LeoFS has the \ndata-compaction\n mechanism in order to remove unncessary objects/files from the object-containers.\n\n\nLeoFS's performance is affected by the data-compaction. And also, LeoFS Storage temporally creates a new file of a object-container corresponding to the compaction target container, which means during the data-compaction needs disk space for the new file of object-container(s).\n\n\nIf \nwrite, update and delete operation\n is a lot, we recommend that the number of containers is 32 OR 64 because it's possible to make effect of the data-compaction at a minimum as much as possible.\n\n\nIn conclusiton:\n\n\n\n\nRead operation \n Write operation:\n\n\nA total number of containers\n = 8\n\n\n\n\n\n\nA lot of write, update and delete operation:\n\n\nA total number of containers\n = 32 OR 64 \n(depends on the disk capacity and performance)\n\n\n\n\n\n\n\n\nleo_storage can not start due to \"enif_send_failed on non smp vm\"\n\n\nWhen starting \nleo_storage\n on a single core machine which crashes with an erl_nif error.\n\n\n1\n2\n3\n## Error log\n\nenif_send: \nenv\n==\nNULL on non-SMP VM\nAborted \n(\ncore dumped\n)\n\n\n\n\n\n\n\nIn this case, you have faced with \nLeoFS Issue#320\n. You need to set a Erlang's VM flag, \n-smp\n in your \nleo_storage.conf\n as follows:\n\n\n1\n2\n## leo_storage.conf\n\nerlang.smp \n=\n \nenable\n\n\n\n\n\n\n\nWhy is the speed of rebalance/recover command too slow?\n\n\nYou must hit \nLeoFS Issue#359\n. Since this issue has been fixed with \nLeoFS v1.2.9\n, we'd recommend you upgrading to the v1.2.9 or higher one, and set an appropriate value for your environment to \nmq.num_of_mq_procs\n in your leo_storage.conf.\n\n\nWhy does LeoFS's SNMP I/F give me wrong values(0) instead of correct values?\n\n\nYou must hit \nLeoFS Issue#361\n. Since this issue has been fixed with \nLeoFS v1.2.9\n, we'd recommend you upgrading to the v1.2.9 or higher one.\n\n\nWhen adding a new storage node, that node doesn't appear with \nleofs-adm status\n, Why?\n\n\nIf you changed a WRONG node name before stopping the daemon, As a result, when a new daemon was starting, it failed to detect that the previous was still running and\nstop command did not work too.\n\n\nSince you can notice this kind of mistake in \nerror.log\n with \nLeoFS v1.2.9\n, we'd recommend you upgrading to the v1.2.9 or higher one.\n\n\nWhat should I do when a \ntimeout\n error happen during upload a very large file?\n\n\nThere are two configurations that could affect how often \ntimeout\n could happen in leo_gateway.conf.\n\n\n\n\nhttp.timeout_for_body (in msecs)\n\n\nlarge_object.reading_chunked_obj_len (in bytes)\n\n\n\n\nAs \ntimeout\n during upload could happen when \nhttp.timeout_for_body\n passed during every \nlarge_object.reading_chunked_obj_len\n bytes read, assuming we have \nhttp.timeout_for_body\n set to 1000 and \nlarge_object.reading_chunked_obj_len\n set to 1048576, we could expect any upload to be succeeded in if we ensured at least 8Mbps(1MB/sec) stable network bandwidth.\n\n\nAlso taking the concurrent uploads into account, we can define the expression that enables us to check whether each configuration is valid as below.\n\n\n1\nEXPECTED_BANDWIDTH_IN_BYTES\n \n=\n \nlarge_object.reading_chunked_obj_len / (http.timeout_for_body / 1000) * MAX_CONCURRENT_UPLOADS\n\n\n\n\n\n\n\nFor example, say we have 1Gbps(128MB/sec) stable bandwidth and 128 concurrent uploads at most then we could be ready to 1MB per connection so the below is considered as a valid configuration.\n\n\n1\n2\n3\nhttp.timeout_for_body\n \n=\n \n1000\n\n\nlarge_object.reading_chunked_obj_len\n \n=\n \n1048576\n\n\n# EXPECTED_BANDWIDTH_IN_BYTES = 1048576 / (1000 / 1000) * 128 = 134217728(128MB)\n\n\n\n\n\n\n\nAlso there is one important thing you have to care.\nTo decrease the bandwidth and odds timeout could happen, you have two options.\n\n\n\n\nDecrease \nlarge_object.reading_chunked_obj_len\n\n\nIncrease \nhttp.timeout_for_body\n\n\n\n\nLogically it's same between decreasing the buffer size and increasing the timeout. However there is one benefit using the former one rather than the latter. As the memory allocation request in Erlang VM happen in \nlarge_object.reading_chunked_obj_len\n unit, the larger buffer size we set, the more memory footprint the host running LeoGateway needs at once so that if LeoGateway accepts lots of connections and those try to upload very large files in parallel then the odds OOM could happen increase. That said, decreasing the buffer size should be the first try in terms of safety(less memory usages).\n\n\nIf nodes in a cluster are connected using DNS names, When will DNS lookups happen?\n\n\n\n\nThe short answer is\n\n\nONLY when starting each node\n\n\nRarely when the network gets unstable, and TCP connections between nodes get disconnected and re-connected later (TCP connections between nodes in a cluster keep alive and being used for RPC(remote procedure call) and any Distributed Erlang\n1\n feature, so once connected to others, No DNS lookups happen except abnormal cases as stated above)\n\n\n\n\n\n\nThe long/precious answer is depending on how Distributed Erlang\n1\n is implemented so you would have to become a master of Distributed Erlang\n1\n protocol, however in reality only remembering the short answer could help you in almost cases\n\n\n\n\nGiven that the above answers, You would not need to be afraid of using hostnames instead of ip addresses.\n\n\nWhy doesn't leofs-adm respond to a command like leofs-adm status?\n\n\nThe netcat installed on your env is probably the one distributed through netcat-traditional package and unfortunately that doesn't work for leofs-adm which requires netcat-openbsd instead. if that is the case, installing netcat-openbsd should solve the problem as reported on github issue\n2\n.\n\n\nWhy doesn't leofs-adm import-user work even the user doesn't exist?\n\n\nIf you are using LeoFS \n= v1.3.8 then you probably hit the issue\n3\n. If that is the case, we'd recommend you upgrade to the v1.4.0 or higher one in which version you can use import-user with the force update option (-f) that enables you to import the user with the existing access-key-id.\n\n\n\n\n\n\n\n\n\n\nDistributed Erlang\n\n\n\n\n\n\nLeoFS' Issue #519, Deb (a LeoFS' package for Ubuntu) should require netcat-openbsd instead of netcat\n\n\n\n\n\n\nLeoFS' Issue #964, import-user with the access-key-id belonging to a deleted user doesn't work", 
            "title": "LeoFS Administration"
        }, 
        {
            "location": "/faq/administration/#faq-leofs-administration", 
            "text": "", 
            "title": "FAQ: LeoFS Administration"
        }, 
        {
            "location": "/faq/administration/#where-can-i-get-the-packages-for-leofs", 
            "text": "You can get the packages by platform from  LeoFS website , and LeoFS package for FreeBSD has been published at  FreeBSD Fresh Ports .", 
            "title": "Where can I get the packages for LeoFS?"
        }, 
        {
            "location": "/faq/administration/#how-can-i-run-my-leofs-cluster-automatically", 
            "text": "Currently after restarting a storage node,  you need to issue the resume command with leofs-adm script manually like this.  1 $ leofs-adm resume storage_0@127.0.0.1   We're planing to provide LeoFS's automated operations like  auto-compaction ,  auto-rebalance  and others with LeoFS v1.2, v1.4 and v2.0. Actually,  auto-compaction  which was already supported with v1.2.", 
            "title": "How can I run my LeoFS cluster automatically?"
        }, 
        {
            "location": "/faq/administration/#how-do-multiple-users-login-into-leofs-managers-console-at-the-same-time", 
            "text": "", 
            "title": "How do multiple users login into LeoFS Manager's console at the same time?"
        }, 
        {
            "location": "/faq/administration/#answer-1", 
            "text": "Actually, there is no login status in LeoFS, but the number of listening tcp connections is limited by  leo_manger.conf   default :   console . acceptors . cui   =   3 .\nSo while using default settings, 3 connections can be connected to a manager in parallel.", 
            "title": "Answer 1:"
        }, 
        {
            "location": "/faq/administration/#answer-2", 
            "text": "Since  LeoFS v1.1.0 , LeoFS have the more powerful alternative option  leofs-adm  command.  This command have not only same functionalities with the existing telnet way but also do NOT keep an tcp connection established for long time (connect only when issueing a command). You don't need to worry about the number of tcp connections.", 
            "title": "Answer 2:"
        }, 
        {
            "location": "/faq/administration/#the-result-of-the-du-command-can-be-different-with-the-actual-disk-usage", 
            "text": "In order to reduce system resource usage for calculating the result of the  du  command, LeoFS keep that information on memory and when stopping itself, LeoFS save those data into a local file. Then when restarting, LeoFS load those on memory.  So if LeoFS is stopped unintentionally like killing by OOM killer, those data can become inconsistent with actual usage.  If you get into this situation, you can recover those data by issueing the  compact-start  command to the node. after finishing the compaction, the result of the du command will be consitent with actual usage.", 
            "title": "The result of the du command can be different with the actual disk-usage"
        }, 
        {
            "location": "/faq/administration/#when-issueing-the-recover-node-command-the-leofs-can-get-into-high-load", 
            "text": "Since the  recover-node  command can lead to issue lots of disk I/O and consume network bandwidth, if you face to issue the recover-node to multiple nodes at once, LeoFS can get into high load and become unresponsive. So we recommend you execute the recover-node command to target nodes one by one.  If this solution could not work for you, you're able to control how much recover-node consume system resources by changing  the MQ-related parameters in leo_storage.conf .", 
            "title": "When issueing the recover node command the LeoFS can get into high load"
        }, 
        {
            "location": "/faq/administration/#what-should-i-do-when-too-many-processes-errors-happen", 
            "text": "LeoFS usually try to keep the number of Erlang processes as minimum as possible, but there are some exceptions when doing something asynchronously.   Replicating an object to the non-primary assigned nodes  Retrying to replicate an object when the previous attempt failed   Given that LeoFS suffered from very high load AND there are some nodes downed for some reason, The number of Erlang processes gradually have increased and might have reached the sysmte limit.  We recommend users to set an appropriate value which depends on your workload to the  +P option . Also if the  +P option  does NOT work for you, there are some possibilities that some external system resources like disk, network equipments have broken, Please check out the dmesg/syslog on your sysmtem.", 
            "title": "What should I do when Too many processes errors happen?"
        }, 
        {
            "location": "/faq/administration/#how-do-i-set-a-number-of-containers-at-leofs-storage-configuration", 
            "text": "Objects/files are stored into LeoFS Storage containers which are log-structured files. LeoFS has the  data-compaction  mechanism in order to remove unncessary objects/files from the object-containers.  LeoFS's performance is affected by the data-compaction. And also, LeoFS Storage temporally creates a new file of a object-container corresponding to the compaction target container, which means during the data-compaction needs disk space for the new file of object-container(s).  If  write, update and delete operation  is a lot, we recommend that the number of containers is 32 OR 64 because it's possible to make effect of the data-compaction at a minimum as much as possible.", 
            "title": "How do I set \"a number of containers\" at LeoFS Storage configuration?"
        }, 
        {
            "location": "/faq/administration/#in-conclusiton", 
            "text": "Read operation   Write operation:  A total number of containers  = 8    A lot of write, update and delete operation:  A total number of containers  = 32 OR 64  (depends on the disk capacity and performance)", 
            "title": "In conclusiton:"
        }, 
        {
            "location": "/faq/administration/#leo_storage-can-not-start-due-to-enif_send_failed-on-non-smp-vm", 
            "text": "When starting  leo_storage  on a single core machine which crashes with an erl_nif error.  1\n2\n3 ## Error log \nenif_send:  env == NULL on non-SMP VM\nAborted  ( core dumped )    In this case, you have faced with  LeoFS Issue#320 . You need to set a Erlang's VM flag,  -smp  in your  leo_storage.conf  as follows:  1\n2 ## leo_storage.conf \nerlang.smp  =   enable", 
            "title": "leo_storage can not start due to \"enif_send_failed on non smp vm\""
        }, 
        {
            "location": "/faq/administration/#why-is-the-speed-of-rebalancerecover-command-too-slow", 
            "text": "You must hit  LeoFS Issue#359 . Since this issue has been fixed with  LeoFS v1.2.9 , we'd recommend you upgrading to the v1.2.9 or higher one, and set an appropriate value for your environment to  mq.num_of_mq_procs  in your leo_storage.conf.", 
            "title": "Why is the speed of rebalance/recover command too slow?"
        }, 
        {
            "location": "/faq/administration/#why-does-leofss-snmp-if-give-me-wrong-values0-instead-of-correct-values", 
            "text": "You must hit  LeoFS Issue#361 . Since this issue has been fixed with  LeoFS v1.2.9 , we'd recommend you upgrading to the v1.2.9 or higher one.", 
            "title": "Why does LeoFS's SNMP I/F give me wrong values(0) instead of correct values?"
        }, 
        {
            "location": "/faq/administration/#when-adding-a-new-storage-node-that-node-doesnt-appear-with-leofs-adm-status-why", 
            "text": "If you changed a WRONG node name before stopping the daemon, As a result, when a new daemon was starting, it failed to detect that the previous was still running and\nstop command did not work too.  Since you can notice this kind of mistake in  error.log  with  LeoFS v1.2.9 , we'd recommend you upgrading to the v1.2.9 or higher one.", 
            "title": "When adding a new storage node, that node doesn't appear with leofs-adm status, Why?"
        }, 
        {
            "location": "/faq/administration/#what-should-i-do-when-a-timeout-error-happen-during-upload-a-very-large-file", 
            "text": "There are two configurations that could affect how often  timeout  could happen in leo_gateway.conf.   http.timeout_for_body (in msecs)  large_object.reading_chunked_obj_len (in bytes)   As  timeout  during upload could happen when  http.timeout_for_body  passed during every  large_object.reading_chunked_obj_len  bytes read, assuming we have  http.timeout_for_body  set to 1000 and  large_object.reading_chunked_obj_len  set to 1048576, we could expect any upload to be succeeded in if we ensured at least 8Mbps(1MB/sec) stable network bandwidth.  Also taking the concurrent uploads into account, we can define the expression that enables us to check whether each configuration is valid as below.  1 EXPECTED_BANDWIDTH_IN_BYTES   =   large_object.reading_chunked_obj_len / (http.timeout_for_body / 1000) * MAX_CONCURRENT_UPLOADS    For example, say we have 1Gbps(128MB/sec) stable bandwidth and 128 concurrent uploads at most then we could be ready to 1MB per connection so the below is considered as a valid configuration.  1\n2\n3 http.timeout_for_body   =   1000  large_object.reading_chunked_obj_len   =   1048576  # EXPECTED_BANDWIDTH_IN_BYTES = 1048576 / (1000 / 1000) * 128 = 134217728(128MB)    Also there is one important thing you have to care.\nTo decrease the bandwidth and odds timeout could happen, you have two options.   Decrease  large_object.reading_chunked_obj_len  Increase  http.timeout_for_body   Logically it's same between decreasing the buffer size and increasing the timeout. However there is one benefit using the former one rather than the latter. As the memory allocation request in Erlang VM happen in  large_object.reading_chunked_obj_len  unit, the larger buffer size we set, the more memory footprint the host running LeoGateway needs at once so that if LeoGateway accepts lots of connections and those try to upload very large files in parallel then the odds OOM could happen increase. That said, decreasing the buffer size should be the first try in terms of safety(less memory usages).", 
            "title": "What should I do when a timeout error happen during upload a very large file?"
        }, 
        {
            "location": "/faq/administration/#if-nodes-in-a-cluster-are-connected-using-dns-names-when-will-dns-lookups-happen", 
            "text": "The short answer is  ONLY when starting each node  Rarely when the network gets unstable, and TCP connections between nodes get disconnected and re-connected later (TCP connections between nodes in a cluster keep alive and being used for RPC(remote procedure call) and any Distributed Erlang 1  feature, so once connected to others, No DNS lookups happen except abnormal cases as stated above)    The long/precious answer is depending on how Distributed Erlang 1  is implemented so you would have to become a master of Distributed Erlang 1  protocol, however in reality only remembering the short answer could help you in almost cases   Given that the above answers, You would not need to be afraid of using hostnames instead of ip addresses.", 
            "title": "If nodes in a cluster are connected using DNS names, When will DNS lookups happen?"
        }, 
        {
            "location": "/faq/administration/#why-doesnt-leofs-adm-respond-to-a-command-like-leofs-adm-status", 
            "text": "The netcat installed on your env is probably the one distributed through netcat-traditional package and unfortunately that doesn't work for leofs-adm which requires netcat-openbsd instead. if that is the case, installing netcat-openbsd should solve the problem as reported on github issue 2 .", 
            "title": "Why doesn't leofs-adm respond to a command like leofs-adm status?"
        }, 
        {
            "location": "/faq/administration/#why-doesnt-leofs-adm-import-user-work-even-the-user-doesnt-exist", 
            "text": "If you are using LeoFS  = v1.3.8 then you probably hit the issue 3 . If that is the case, we'd recommend you upgrade to the v1.4.0 or higher one in which version you can use import-user with the force update option (-f) that enables you to import the user with the existing access-key-id.      Distributed Erlang    LeoFS' Issue #519, Deb (a LeoFS' package for Ubuntu) should require netcat-openbsd instead of netcat    LeoFS' Issue #964, import-user with the access-key-id belonging to a deleted user doesn't work", 
            "title": "Why doesn't leofs-adm import-user work even the user doesn't exist?"
        }, 
        {
            "location": "/faq/client/", 
            "text": "FAQ: LeoFS Clients\n\n\n\n\n\nName Resolution\n\n\n\n\nThere are two ways to access LeoGateway from LeoFS' Clients.\n\n\nPath Style Access\n\n\nSub-Domain Style Access\n\n\n\n\n\n\n\n\nIf a LeoFS Client you use adopts the sub-domain style access, then the client need to resolve \nbucket.endpoint\n domain into the IP address of LeoGateway.\n\n\n\n\n\n\nRelated Links:\n\n\n\n\nA Comment on LeoFS' Issue #748, Connecting to bucket causes badarg error\n\n\nFor Administrators / System Operations / S3 / Endpoint", 
            "title": "LeoFS Clients"
        }, 
        {
            "location": "/faq/client/#faq-leofs-clients", 
            "text": "", 
            "title": "FAQ: LeoFS Clients"
        }, 
        {
            "location": "/faq/client/#name-resolution", 
            "text": "There are two ways to access LeoGateway from LeoFS' Clients.  Path Style Access  Sub-Domain Style Access     If a LeoFS Client you use adopts the sub-domain style access, then the client need to resolve  bucket.endpoint  domain into the IP address of LeoGateway.    Related Links:   A Comment on LeoFS' Issue #748, Connecting to bucket causes badarg error  For Administrators / System Operations / S3 / Endpoint", 
            "title": "Name Resolution"
        }, 
        {
            "location": "/production_checklist/README/", 
            "text": "Production Checklist: LeoFS\n\n\n\n\n\nWhat version should we use?\n\n\nUse the latest stable one. With the version \n= v1.3.0, LeoFS had a serious issue that may cause data-lost, so that use at least \n= v1.3.1. Or in case you need to keep running LeoFS with older one for some reason, make sure that \nlarge_object.reading_chunked_obj_len\n \n= \nlarge_object.chunked_obj_len\n in \nleo_gateway.conf\n. This setting prevent LeoFS from suffering \nLeoFS Issue#531\n.\n\n\nThe last part of a large object can be broken with \nreading_chunked_obj_len\n \n \nchunked_obj_len\n in \nleo_gateway.conf\n.", 
            "title": "Production Checklist"
        }, 
        {
            "location": "/production_checklist/README/#production-checklist-leofs", 
            "text": "", 
            "title": "Production Checklist: LeoFS"
        }, 
        {
            "location": "/production_checklist/README/#what-version-should-we-use", 
            "text": "Use the latest stable one. With the version  = v1.3.0, LeoFS had a serious issue that may cause data-lost, so that use at least  = v1.3.1. Or in case you need to keep running LeoFS with older one for some reason, make sure that  large_object.reading_chunked_obj_len   =  large_object.chunked_obj_len  in  leo_gateway.conf . This setting prevent LeoFS from suffering  LeoFS Issue#531 .  The last part of a large object can be broken with  reading_chunked_obj_len     chunked_obj_len  in  leo_gateway.conf .", 
            "title": "What version should we use?"
        }
    ]
}